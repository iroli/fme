{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 0. Базовый парсер заголовков\n",
    "\n",
    "Вытаскивает из latex-кода заголовки статей и их расположение в файлах.\n",
    "\n",
    "Разбивка происходит в полуручном режиме, т.к. нет уверенности в формате заголовков.\n",
    "\n",
    "В тексте ищутся слова, содержащие в своём составе заглавные буквы на русском и английском языках в отношении, большем или равным заданному (по умолчанию 0.51, при меньших значениях количество вхождений значительно возрастает, например за счёт двухбуквенных предлогов). Предполагается, что таким образом удаётся обнаруживать неправильно машиинно распознанный капс. Слова или цепочки слов, состоящие из одного строчного символа включаются в заголовок, если стоят между слов, определённых как часть заголовка. При этом, одиночные заглавные буквы, а также инициалы не воспринимаются как начало заголовка.\n",
    "\n",
    "## Использование\n",
    "- При удовлетворительном определении заголовка нажать `Enter` без дополнительного ввода.\n",
    "- Если предложенное место заголовком не является ввести `\"n\"`\n",
    "- При неправильном определении границ заголовка ввести два корректировочных числа для сдвига левой и правой границы.\n",
    "  - ЗАМЕЧАНИЕ: сдвиг производится попробельно, т.е. двойной пробел будет распознан как слово нулевой длины.\n",
    "  - ЗАМЕЧАНИЕ: границы отображаемого фрагмента текста будут передвинуты автоматически. Длины левой и правой границ в словах задаются в параметрах.\n",
    "  - ПРИМЕРЫ:\n",
    "    - `out: a [B C] d e f` -> `in: 0 2` -> `out: a [B C D E] f`\n",
    "    - `out: a b c [D E] f` -> `in: 2 -1` -> `out: a [B C D] e f`\n",
    "- Также возможен посимвольный сдвиг правой границы в случае \"сращивания\" заголовка статьи и её текста. Ввести одно число, начиная с точки.\n",
    "  - ПРИМЕРЫ:\n",
    "    - `out: a[BC]def` -> `in: .2` -> `out: a[BCDE]f`\n",
    "    - `out: a[BCDE]f` -> `in: .-1` -> `out: a[BCD]ef`\n",
    "\n",
    "В выводе в терминале переносы строк для удобства заменены на `\"$\"`\n",
    "\n",
    "### Прочее\n",
    "- Для определителя капса достуны исключения, которые никогда не будут рассматриваться, как потенциальные начала заголовков, см. опции. По умолчанию: первые 10 римских цифр, \"МэВ\" и \"ГэВ\". Также определитель не реагирует на \"СМ.\", что часто встречается в ссылках сразу после заголовков.\n",
    "- Использовать системный терминал для взаимодействия оказывается удобнее, чем использовать jupyter, поэтому можно скопировать ячейку с кодом в файл `scripter.py` и запускать его.\n",
    "- При положительном определении заголовка файл дополняется немедленно, прервать процесс можно в любой момент, как и продолжить после -- итоговый файл будет дополяться, а не перезаписываться с нуля при новом запуске программы (главное не забыть предварительно удалить из конца файла дубликаты, если вы начинаете с той страницы, на которой закончили в прошлый раз, а не со следующей).\n",
    "- В случае пропуска парсером заголовка его можно добавить вручную двумя способами:\n",
    "  1) Сдвинуть границы заголовка назад, как описано в инструкции выше. Подходит, если была пропущена небольшая (обычно ссылочная) статья, примерно 20 слов, плюс-минус. При этом после ввода заголовка поиск продолжится с __его__ конца, поэтому следующий заголовок \"вместо\" которого был введён пропущенный будет определён заново и пропущен не будет.\n",
    "  2) Воспользоваться ячейкой 1.1. Для этого в сыром tex-файле страницы нужно отыскать заголовок, скопировать его и __в точности__ вставить в разделе параметров, а также указать номер страницы. Скрипт парсера при этом можно не закрывать, последующая нумерация подстроится автоматически."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 0. Базовый парсер заголовков\n",
    "\n",
    "from os import walk\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "\n",
    "############################ VARS ################################\n",
    "PAGES_DIR = \"./matphys/rpages/\"\n",
    "EXIT_DIR = \"./matphys/\"\n",
    "EXIT_FILE = \"FMEv2.xml\"\n",
    "# First and last pages to be parsed\n",
    "START_PAGE = 639\n",
    "END_PAGE = 700\n",
    "# How many words to display before and after a potential title\n",
    "LEAD_WORDS = 5\n",
    "AFT_WORDS = 5\n",
    "# Look in the description\n",
    "CAPS_QUOT = 0.51\n",
    "EXCEPTIONS = ['I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX', 'X', 'МэВ', 'ГэВ']\n",
    "# Symbols excluded in xml have to be converted back\n",
    "XML_EXCLUDES = {'&quot;' : '\"', '&apos;' : \"'\", '&lt;' : '<',\t'&gt;' : '>',\t'&amp;' : '&'}\n",
    "##################################################################\n",
    "\n",
    "\n",
    "\n",
    "class Article:\n",
    "\tstart_title = 0\n",
    "\tend_title = 0\n",
    "\tfilename = ''\n",
    "\n",
    "\n",
    "\n",
    "# Write xml tree to file\n",
    "def prettify_1(elem:ET.Element) -> str:\n",
    "\t# Pretty-printed XML string for the Element.\n",
    "\trough_string = ET.tostring(elem, 'utf-8')\n",
    "\treparsed = minidom.parseString(rough_string)\n",
    "\treturn reparsed.toprettyxml(indent=\"  \")\n",
    "def xml_write_1(root:ET.Element):\n",
    "\twith codecs.open(EXIT_DIR + EXIT_FILE, 'w', 'utf-8') as f:\n",
    "\t\tf.write(prettify_1(root))\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames_raw = next(walk(PAGES_DIR), (None, None, []))[2]  # [] if no file\n",
    "filenames = []\n",
    "for i in range(START_PAGE, END_PAGE + 1):\n",
    "\tfor filename in filenames_raw:\n",
    "\t\tbeginning = \"rp-\" + str(i) + \"_\"\n",
    "\t\tif filename[:len(beginning)] == beginning and filename[-4:] == \".mmd\":\n",
    "\t\t\tfilenames.append(filename)\n",
    "\t\t\t\t\t\t\n",
    "\n",
    "# Check for existing xml\n",
    "filenames_raw = next(walk(EXIT_DIR), (None, None, []))[2]  # [] if no file\n",
    "if not(EXIT_FILE in filenames_raw):\n",
    "\troot = ET.Element('data')\n",
    "\txml_write_1(root)\n",
    "\n",
    "\n",
    "# Convert xml excluded symbols\n",
    "def xml_excluded_convert (text:str) -> str:\n",
    "\tfor key in XML_EXCLUDES.keys():\n",
    "\t\twhile text.find(key) != -1:\n",
    "\t\t\tpos = text.find(key)\n",
    "\t\t\ttext = text[:pos] + XML_EXCLUDES[key] + text[pos+len(key):]\n",
    "\treturn text\n",
    "def remove_xml_spaces_1(elem:ET.Element) -> ET.Element:\n",
    "\telem.tail = None\n",
    "\tif elem.text != None:\n",
    "\t\tis_space = True\n",
    "\t\tfor letter in elem.text:\n",
    "\t\t\tis_space = False if letter != ' ' else is_space\n",
    "\t\telem.text = None if is_space else xml_excluded_convert(elem.text)\n",
    "\tfor subelem in elem:\n",
    "\t\tsubelem = remove_xml_spaces_1(subelem)\n",
    "\treturn elem\n",
    "def parse_xml_1() -> ET.Element:\n",
    "\t# Parse existing xml (string parsing is needed to avoid extra newlines appearing)\n",
    "\texit_string = ''\n",
    "\twith codecs.open(EXIT_DIR + EXIT_FILE, 'r', 'utf-8') as f:\n",
    "\t\tfor i in f.readlines():\n",
    "\t\t\texit_string += i[:-1]\n",
    "\troot = ET.fromstring(exit_string)\n",
    "\t# Remove empty tails and texts\n",
    "\troot = remove_xml_spaces_1(root)\n",
    "\treturn root\n",
    "root = parse_xml_1()\n",
    "num = len(root) + 1\n",
    "\n",
    "\n",
    "# Add article title and metadata to xml tree\n",
    "def add_artice_1(elem:Article) -> int:\n",
    "\t# Update root in case it's been changed\n",
    "\troot = parse_xml_1()\n",
    "\tnum = len(root) + 1\n",
    "\tarticle = ET.SubElement(root, 'article', {'n':str(num)})\n",
    "\ttitle = ET.SubElement(article, 'title')\n",
    "\ttitle.text = file[elem.start_title+1:elem.end_title]\n",
    "\ttitle_meta = ET.SubElement(article, 'title-meta')\n",
    "\ttitle_file = ET.SubElement(title_meta, 'title-file')\n",
    "\ttitle_file.text = elem.filename\n",
    "\ttitle_start = ET.SubElement(title_meta, 'title-start')\n",
    "\ttitle_start.text = str(elem.start_title + 1)\n",
    "\ttitle_end = ET.SubElement(title_meta, 'title-end')\n",
    "\ttitle_end.text = str(elem.end_title)\n",
    "\txml_write_1(root)\n",
    "\treturn num\n",
    "\n",
    "\n",
    "# Count number of alphabetic letters in word\n",
    "def count_letters_1(word:str) -> int:\n",
    "\tnum = 0\n",
    "\tfor letter in word:\n",
    "\t\tnum += 0 if re.match(r\"[A-ZА-Яa-zа-я]\", letter) == None else 1\n",
    "\treturn num\n",
    "\n",
    "# Check if word is written in CAPS\n",
    "def check_caps_1(word:str) -> int:\n",
    "\tnum = 0\n",
    "\tlen_word = 0\n",
    "\twhile len(word) and re.match(r\"[!#$%&'*+-.^_`|~:]\", word[-1]) != None:\n",
    "\t\tword = word[:-1]\n",
    "\twhile len(word) and re.match(r\"[!#$%&'*+-.^_`|~:]\", word[0]) != None:\n",
    "\t\tword = word[1:]\n",
    "\tfor letter in word:\n",
    "\t\t#num += 0 if re.match(r\"[A-ZА-Я0-9]|[!#$%&'*+-.^_`|~:]\", letter) == None else 1\t\t\t\t\t# Too many symbols, math formulas are being detected\n",
    "\t\tlen_word += 1 if re.match(r\"[!#$%&'*+-.^_`|~:]\", letter) == None else 0\n",
    "\t\tnum += 0 if re.match(r\"[A-ZА-Я]\", letter) == None else 1\n",
    "\treturn 0 if len_word == 0 or num / len_word < CAPS_QUOT or word in EXCEPTIONS else num\t\t\t\t# Also exclude common roman numbers\n",
    "\n",
    "# Check for initials like \"I.E.\"\n",
    "def check_initials_1(word:str) -> bool:\n",
    "\tinitials = True\n",
    "\tfor i in range(len(word) - 1):\n",
    "\t\ttype_1 = 0 if re.match(r\"[A-ZА-Яa-zа-я]\", word[i]) == None else 1\n",
    "\t\ttype_2 = 0 if re.match(r\"[A-ZА-Яa-zа-я]\", word[i + 1]) == None else 1\n",
    "\t\tinitials = False if type_1 and type_2 else initials\n",
    "\treturn initials\n",
    "\n",
    "# Check if the word is \"CM.\" which happens often\n",
    "def check_link_1(word:str) -> bool:\n",
    "\tword = word.upper()\n",
    "\t# Convert to cyrillic\n",
    "\tfor i in range(len(word)):\n",
    "\t\tword = (word[:i] + 'С' + word[i+1:]) if word[i] == 'C' else word\n",
    "\t\tword = (word[:i] + 'М' + word[i+1:]) if word[i] == 'M' else word\n",
    "\treturn True if word == 'СМ.' else False\n",
    "\n",
    "\n",
    "# Find next ot prev word boundary (space / newline)\n",
    "def prev_from_1(pos:int, file:str) -> int:\n",
    "\tpos = max(pos, 0)\n",
    "\tprev_space = file.rfind(' ', 0, pos)\n",
    "\tprev_nl = file.rfind('\\n', 0, pos)\n",
    "\tprev_space = -1 if prev_space == -1 else prev_space\n",
    "\tprev_nl = -1 if prev_nl == -1 else prev_nl\n",
    "\treturn max(prev_nl, prev_space)\n",
    "def next_from_1(pos:int, file:str, end_replace = True) -> int:\n",
    "\tnext_space = file.find(' ', pos + 1)\n",
    "\tnext_nl = file.find('\\n', pos + 1)\n",
    "\tif end_replace:\n",
    "\t\tnext_space = len(file) if next_space == -1 else next_space\n",
    "\t\tnext_nl = len(file) if next_nl == -1 else next_nl\n",
    "\treturn max(next_nl, next_space) if next_space == -1 or next_nl == -1 else min(next_nl, next_space)\n",
    "\n",
    "\n",
    "# Main loop\n",
    "for filename in filenames:\n",
    "\tprint()\n",
    "\tprint(\"################################ \" + filename + \" ################################\")\n",
    "\twith codecs.open(PAGES_DIR + filename, 'r', 'utf-8') as f:\n",
    "\t\tfile = f.read()\n",
    "\t\n",
    "\tword_bound_l = -1\n",
    "\tword_bound_r = next_from_1(word_bound_l, file, end_replace=False)\n",
    "\tEOF_reached = False\n",
    "\n",
    "\twhile not EOF_reached:\n",
    "\t\tif word_bound_r == -1:\n",
    "\t\t\tword_bound_r = len(file)\n",
    "\t\t\tEOF_reached = True\n",
    "\n",
    "\n",
    "\t\tif check_caps_1(file[word_bound_l+1:word_bound_r]) < 2 or check_initials_1(file[word_bound_l+1:word_bound_r]) or check_link_1(file[word_bound_l+1:word_bound_r]):\n",
    "\t\t\tword_bound_l = word_bound_r\n",
    "\t\t\tword_bound_r = next_from_1(word_bound_l, file, end_replace=False)\n",
    "\t\t\n",
    "\t\telse: # Possibly found a title\n",
    "\t\t\t# Left border of a title is already known\n",
    "\t\t\tstart_title = word_bound_l\n",
    "\n",
    "\t\t\t# Define right border of a title\n",
    "\t\t\tdefined_end = False\n",
    "\t\t\tend_title = word_bound_r\n",
    "\t\t\twhile not defined_end:\n",
    "\t\t\t\tword_bound_l = word_bound_r\n",
    "\t\t\t\tword_bound_r = next_from_1(word_bound_l, file)\n",
    "\n",
    "\t\t\t\tif word_bound_l == len(file):\n",
    "\t\t\t\t\tdefined_end = True\n",
    "\t\t\t\telif check_link_1(file[word_bound_l+1:word_bound_r]):\n",
    "\t\t\t\t\t# A \"CM.\" link, not a title\n",
    "\t\t\t\t\tpass\n",
    "\t\t\t\telif not check_caps_1(file[word_bound_l+1:word_bound_r]) and count_letters_1(file[word_bound_l+1:word_bound_r]) < 2:\n",
    "\t\t\t\t\tif re.match(r\"[A-ZА-Яa-zа-я]\", file[word_bound_l+1]) != None:\n",
    "\t\t\t\t\t\t# Most possibly belongs to title\n",
    "\t\t\t\t\t\tend_title = word_bound_r\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t# Most possibly NOT belongs to title\n",
    "\t\t\t\t\t\tpass\n",
    "\t\t\t\telif check_caps_1(file[word_bound_l+1:word_bound_r]):\n",
    "\t\t\t\t\tend_title = word_bound_r\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdefined_end = True\n",
    "\n",
    "\t\t\tnext_title = False\n",
    "\t\t\twhile not next_title:\n",
    "\t\t\t\t# Update root in case it's been changed\n",
    "\t\t\t\troot = parse_xml_1()\n",
    "\t\t\t\tnum = len(root) + 1\n",
    "\n",
    "\t\t\t\t# Console output for further user actions\n",
    "\t\t\t\tsegment_start = start_title\n",
    "\t\t\t\tsegment_end = end_title\n",
    "\t\t\t\tfor i in range(LEAD_WORDS):\n",
    "\t\t\t\t\tsegment_start = prev_from_1(segment_start, file)\n",
    "\t\t\t\tfor i in range(AFT_WORDS):\n",
    "\t\t\t\t\tsegment_end = next_from_1(segment_end, file)\n",
    "\t\t\t\t\n",
    "\t\t\t\tout_str = file[segment_start+1:segment_end]\n",
    "\n",
    "\t\t\t\t# Format\n",
    "\t\t\t\tfor i in range(len(out_str)):\n",
    "\t\t\t\t\tout_str = out_str[:i] + ('$' if out_str[i] == '\\n' else out_str[i]) + out_str[i+1:]\n",
    "\t\t\t\tout_str = f\"{num})\\n\" + out_str + '\\n' + ' ' * (start_title - segment_start) + '^' * (end_title - start_title - 1)\n",
    "\t\t\t\t# Check for \"section\" in the string. This is referred to alphabetic tip at the bottom of the page\n",
    "\t\t\t\t\"\"\"if 'section' in out_str or 'title' in out_str:\n",
    "\t\t\t\t\tout_str += '     ############################### Title or section found! ###############################'\"\"\" # Not Used\n",
    "\t\t\t\tprint(out_str)\n",
    "\n",
    "\t\t\t\t# User actions\n",
    "\t\t\t\tresponse = input()\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tif response == '':\n",
    "\t\t\t\t\t\t# Add article\n",
    "\t\t\t\t\t\tarticle = Article()\n",
    "\t\t\t\t\t\tarticle.start_title = start_title\n",
    "\t\t\t\t\t\tarticle.end_title = end_title\n",
    "\t\t\t\t\t\tarticle.filename = filename\n",
    "\t\t\t\t\t\tnum = add_artice_1(article)\n",
    "\t\t\t\t\t\tnext_title = True\n",
    "\t\t\t\t\t\tword_bound_l = end_title\n",
    "\t\t\t\t\t\tword_bound_r = next_from_1(word_bound_l, file, end_replace=False)\n",
    "\t\t\t\t\t\tprint(f'Adding article, n=\"{num}\", title=\"{file[start_title+1:end_title]}\"\\n\\n')\n",
    "\t\t\t\t\telif response == 'n' or response == 'т':\n",
    "\t\t\t\t\t\t# Do not add this one\n",
    "\t\t\t\t\t\tnext_title = True\n",
    "\t\t\t\t\t\tprint(\"Not an article, skipping\\n\\n\")\n",
    "\t\t\t\t\telif response[0] == '.':\n",
    "\t\t\t\t\t\tend_title += int(response[1:])\n",
    "\t\t\t\t\t\tprint(\"Changing title right border\\n\\n\")\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t# Change title borders\n",
    "\t\t\t\t\t\tcorrections = response.split(' ')\n",
    "\t\t\t\t\t\tcorrections[0] = int(corrections[0])\n",
    "\t\t\t\t\t\tcorrections[1] = int(corrections[1])\n",
    "\t\t\t\t\t\tif corrections[0] > 0:\n",
    "\t\t\t\t\t\t\tfor i in range(abs(corrections[0])):\n",
    "\t\t\t\t\t\t\t\tstart_title = prev_from_1(start_title, file)\n",
    "\t\t\t\t\t\tif corrections[0] < 0:\n",
    "\t\t\t\t\t\t\tfor i in range(abs(corrections[0])):\n",
    "\t\t\t\t\t\t\t\tstart_title = next_from_1(start_title, file)\n",
    "\t\t\t\t\t\tif corrections[1] < 0:\n",
    "\t\t\t\t\t\t\tfor i in range(abs(corrections[1])):\n",
    "\t\t\t\t\t\t\t\tend_title = prev_from_1(end_title, file)\n",
    "\t\t\t\t\t\tif corrections[1] > 0:\n",
    "\t\t\t\t\t\t\tfor i in range(abs(corrections[1])):\n",
    "\t\t\t\t\t\t\t\tend_title = next_from_1(end_title, file)\n",
    "\t\t\t\t\t\tprint(\"Changing title borders\\n\\n\")\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tprint(\"########## !!! Failed on input, try again !!! ##########\\n\\n\")\n",
    "\n",
    "\n",
    "# End reached\n",
    "print('###########################################################################################')\n",
    "print('Last requested page processed. Press \"Enter\" to close this window.')\n",
    "response = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Добавление заголовков по одному\n",
    "\n",
    "В разделе параметров указать номер страницы и ТОЧНУЮ формулировку заголовка из сырого latex-текста, а также номер страницы, после чего запустить ячейку.\n",
    "\n",
    "Закрывать скрипт парсера не обязательно, это не вызовет ошибок и его нумерация подстроится автоматически."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1. Добавление заголовков по одному\n",
    "\n",
    "############################ VARS ################################\n",
    "PAGES_DIR = \"./matphys/rpages/\"\n",
    "EXIT_DIR = \"./matphys/\"\n",
    "EXIT_FILE = \"FMEv2.xml\"\n",
    "# Search parameters\n",
    "PAGE = 73\n",
    "TITLE = 'BAPИА山ия -'\n",
    "##################################################################\n",
    "\n",
    "\n",
    "\n",
    "class Article:\n",
    "\tstart_title = 0\n",
    "\tend_title = 0\n",
    "\tfilename = ''\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames_raw = next(walk(PAGES_DIR), (None, None, []))[2]  # [] if no file\n",
    "filenames = []\n",
    "for i in range(PAGE, PAGE + 1):\n",
    "\tfor filename in filenames_raw:\n",
    "\t\tbeginning = \"rp-\" + str(i) + \"_\"\n",
    "\t\tif filename[:len(beginning)] == beginning and filename[-4:] == \".mmd\":\n",
    "\t\t\tfilenames.append(filename)\n",
    "\t\t\t\t\t\t\n",
    "\n",
    "# Check for existing xml\n",
    "filenames_raw = next(walk(EXIT_DIR), (None, None, []))[2]  # [] if no file\n",
    "if not(EXIT_FILE in filenames_raw):\n",
    "\troot = ET.Element('data')\n",
    "\txml_write(root, EXIT_DIR + EXIT_FILE)\n",
    "\n",
    "\n",
    "root = parse_xml(EXIT_DIR + EXIT_FILE)\n",
    "\n",
    "\n",
    "# Add article title and metadata to xml tree\n",
    "def add_artice(elem:Article, root:ET.Element, num:int):\n",
    "\tarticle = ET.SubElement(root, 'article', {'n':str(num)})\n",
    "\ttitle = ET.SubElement(article, 'title')\n",
    "\ttitle.text = file[elem.start_title+1:elem.end_title]\n",
    "\ttitle_meta = ET.SubElement(article, 'title-meta')\n",
    "\ttitle_file = ET.SubElement(title_meta, 'title-file')\n",
    "\ttitle_file.text = elem.filename\n",
    "\ttitle_start = ET.SubElement(title_meta, 'title-start')\n",
    "\ttitle_start.text = str(elem.start_title + 1)\n",
    "\ttitle_end = ET.SubElement(title_meta, 'title-end')\n",
    "\ttitle_end.text = str(elem.end_title)\n",
    "\txml_write(root, EXIT_DIR + EXIT_FILE)\n",
    "\n",
    "# Read requested file\n",
    "with codecs.open(PAGES_DIR + filenames[0], 'r', 'utf-8') as f:\n",
    "\tfile = f.read()\n",
    "\n",
    "# Find titles and add them\n",
    "start_title = 0\n",
    "end_title = 0\n",
    "num = len(root) + 1\n",
    "while file.find(TITLE, end_title) != -1:\n",
    "\tstart_title = file.find(TITLE, start_title)\n",
    "\tend_title = start_title + len(TITLE)\n",
    "\tstart_title -= 1 # Set on space befor the title\n",
    "\n",
    "\tarticle = Article()\n",
    "\tarticle.start_title = max(start_title, 0)\n",
    "\tarticle.end_title = min(end_title, len(file))\n",
    "\tarticle.filename = filenames[0]\n",
    "\tadd_artice(article, root, num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Общий код\n",
    "\n",
    "Ячейка подключает библиотеки и задаёт функции, используемые в ячейках 0.1 (выше), 2. и далее (ниже), для сокращения объёма кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Общий код\n",
    "\n",
    "from os import walk\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import re\n",
    "import codecs\n",
    "from transliterate import translit, get_available_language_codes\n",
    "from random import randint\n",
    "import enchant\n",
    "from enchant.checker import SpellChecker\n",
    "from enchant.tokenize import EmailFilter, URLFilter\n",
    "import difflib\n",
    "\n",
    "\n",
    "# Small dictionaries merger\n",
    "def dict_merge(dict1:dict, dict2:dict) -> dict:\n",
    "\tdict0 = {}\n",
    "\tfor key in dict1.keys():\n",
    "\t\tdict0[key] = dict1[key]\n",
    "\tfor key in dict2.keys():\n",
    "\t\tdict0[key] = dict2[key]\n",
    "\treturn dict0\n",
    "\n",
    "\n",
    "############################ VARS ################################\n",
    "# Symbols and combinations that have to be corrected after OCR\n",
    "COMBINATIONS_CORR_ALPHABET = {\n",
    "\t'A':'А', 'a':'а', 'B':'В', 'b':'Ь', 'C':'С', 'c':'с', 'E':'Е', 'e':'е', 'H':'Н', 'K':'К', 'M':'М', 'O':'О', 'P':'Р', 'p':'р', 'T':'Т', 'X':'Х', 'y':'у', 'x':'х',\n",
    "\t'U' : 'И',\n",
    "\t'u' : 'и',\n",
    "\t'r' : 'г',\n",
    "\t'N' : 'П',\n",
    "\t'n' : 'п',\n",
    "\t'm' : 'т',\n",
    "\t'Y' : 'У',\n",
    "\t#'S' : 'Я',\t\t# Seems irrelevant\n",
    "}\n",
    "COMBINATIONS_CORR_UNICODE = {\n",
    "\t'І' : 'I',\t\t# These two \"I\" are different!\n",
    "\t'ก' : 'п',\n",
    "\t'山' : 'Ц',\n",
    "\t'כ' : 'э',\n",
    "\t'חи' : 'пи',\n",
    "}\n",
    "COMBINATIONS_CORR_OTHER = {\n",
    "\t' -' : '-',\n",
    "\t'- ' : '-',\n",
    "\t'0' : 'О',\n",
    "\t'3' : 'З',\n",
    "\t'6' : 'б',\n",
    "}\n",
    "COMBINATIONS_CORR_GLOBAL = dict_merge(COMBINATIONS_CORR_ALPHABET, dict_merge(COMBINATIONS_CORR_UNICODE, COMBINATIONS_CORR_OTHER))\n",
    "# Symbols excluded in xml have to be converted back\n",
    "XML_EXCLUDES = {\n",
    "\t'&quot;' : '\"',\n",
    "\t'&apos;' : \"'\",\n",
    "\t'&lt;' : '<',\n",
    "\t'&gt;' : '>',\n",
    "\t'&amp;' : '&'\n",
    "}\n",
    "PERSONAL_WORD_LIST = \"./matphys/PWL.txt\"\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Write xml tree to file\n",
    "def prettify(elem:ET.Element) -> str:\n",
    "\t# Pretty-printed XML string for the Element.\n",
    "\trough_string = ET.tostring(elem, 'utf-8')\n",
    "\treparsed = minidom.parseString(rough_string)\n",
    "\treturn reparsed.toprettyxml(indent=\"  \")\n",
    "def xml_write(root:ET.Element, filename:str):\n",
    "\twith codecs.open(filename, 'w', 'utf-8') as f:\n",
    "\t\tf.write(prettify(root))\n",
    "\n",
    "\n",
    "# Convert xml excluded symbols\n",
    "def xml_excluded_convert (text:str) -> str:\n",
    "\tfor key in XML_EXCLUDES.keys():\n",
    "\t\twhile text.find(key) != -1:\n",
    "\t\t\tpos = text.find(key)\n",
    "\t\t\ttext = text[:pos] + XML_EXCLUDES[key] + text[pos+len(key):]\n",
    "\treturn text\n",
    "\n",
    "\n",
    "def remove_xml_spaces(elem:ET.Element, filename:str) -> ET.Element:\n",
    "\telem.tail = None\n",
    "\tif elem.text != None:\n",
    "\t\tis_space = True\n",
    "\t\tfor letter in elem.text:\n",
    "\t\t\tis_space = False if letter != ' ' else is_space\n",
    "\t\tif is_space:\n",
    "\t\t\telem.text = None\n",
    "\t\telse:\n",
    "\t\t\tif elem.tag == 'text':\n",
    "\t\t\t\telem.text = get_texts(filename)[0]\n",
    "\t\t\telif elem.tag == 'text_orig':\n",
    "\t\t\t\telem.text = get_texts(filename)[1]\n",
    "\t\t\telem.text = xml_excluded_convert(elem.text)\n",
    "\tfor subelem in elem:\n",
    "\t\tsubelem = remove_xml_spaces(subelem, filename)\n",
    "\treturn elem\n",
    "def parse_xml(filename:str) -> ET.Element:\n",
    "\t# Parse existing xml (string parsing is needed to avoid extra newlines appearing)\n",
    "\texit_string = ''\n",
    "\twith codecs.open(filename, 'r', 'utf-8') as f:\n",
    "\t\tfor i in f.readlines():\n",
    "\t\t\texit_string += i[:-1]\n",
    "\troot = ET.fromstring(exit_string)\n",
    "\troot = remove_xml_spaces(root, filename)\n",
    "\treturn root\n",
    "\n",
    "\n",
    "# !!!BUG!!! for some reason newlines diappear in texts in parsed xml, so extract article texts manually and replace\n",
    "def get_texts(filename:str) -> str:\n",
    "\twith codecs.open(filename, 'r', 'utf-8') as f:\n",
    "\t\tfile = f.read()\n",
    "\ttext = file[file.find('<text>')+6:file.find('</text>')]\n",
    "\twith codecs.open(filename, 'r', 'utf-8') as f:\n",
    "\t\tfile = f.read()\n",
    "\ttext_orig = file[file.find('<text_orig>')+11:file.find('</text_orig>')]\n",
    "\treturn (text, text_orig)\n",
    "\n",
    "\n",
    "# Get xml tree element wit sertain tag name\n",
    "def get_xml_elem(root:ET.Element, elem_path:str) -> ET.Element:\n",
    "\ttgt = elem_path.split('/')[0]\n",
    "\tfor elem in root:\n",
    "\t\tif elem.tag == tgt:\n",
    "\t\t\tif elem_path.find('/') != -1:\n",
    "\t\t\t\treturn get_xml_elem(elem, elem_path[elem_path.find('/')+1:])\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn elem\n",
    "\treturn None\n",
    "\n",
    "\n",
    "## Titles handling functions\n",
    "# Correct preferred combinations and latin letters\n",
    "def title_handle_latin(title_new: str, COMBINATIONS_CORR: dict) -> str:\n",
    "\tif title_new == None or len(title_new) == 0:\n",
    "\t\treturn title_new\n",
    "\tfor comb in COMBINATIONS_CORR.keys():\n",
    "\t\twhile title_new.find(comb) != -1:\n",
    "\t\t\ttitle_new = title_new[:title_new.find(comb)] + COMBINATIONS_CORR[comb] + title_new[title_new.find(comb) + len(comb):]\n",
    "\treturn title_new\n",
    "# Remove bounding symbols\n",
    "def title_handle_bounding(title_new: str) -> str:\n",
    "\tif title_new == None or len(title_new) == 0:\n",
    "\t\treturn title_new\n",
    "\twhile len(title_new) and re.match(r\"[!#%&'*+-.^_`|~:;]\", title_new[0]) != None:\n",
    "\t\ttitle_new = title_new[1:]\n",
    "\twhile len(title_new) and re.match(r\"[!#%&'*+-.^_`|~:;]\", title_new[-1]) != None:\n",
    "\t\ttitle_new = title_new[:-1]\n",
    "\treturn title_new\n",
    "# Merge single-lettered words\n",
    "def title_handle_merge(title_new: str) -> str:\n",
    "\tif title_new == None or len(title_new) == 0:\n",
    "\t\treturn title_new\n",
    "\ttitle_new = ' ' + title_new + ' '\n",
    "\tfor i in range(len(title_new) - 4):\n",
    "\t\tif (title_new[i] == ' ' or title_new[i] == '№') and title_new[i + 2] == ' ' and title_new[i + 4] == ' ':\n",
    "\t\t\ttitle_new = title_new[:i+2] + '№' + title_new[i+3:]\n",
    "\ti = 0\n",
    "\twhile i < len(title_new):\n",
    "\t\tif title_new[i] == '№':\n",
    "\t\t\ttitle_new = title_new[:i] + title_new[i+1:]\n",
    "\t\t\ti = 0\n",
    "\t\telse:\n",
    "\t\t\ti += 1\n",
    "\twhile title_new[0] == ' ':\n",
    "\t\ttitle_new = title_new[1:]\n",
    "\twhile title_new[-1] == ' ':\n",
    "\t\ttitle_new = title_new[:-1]\n",
    "\treturn title_new\n",
    "# Revert changes for aux formulas in titles\n",
    "def title_handle_formulas(title_new: str, title: str) -> str:\n",
    "\tif title_new == None or len(title_new) == 0:\n",
    "\t\treturn title_new\n",
    "\tpos_old = 0\n",
    "\tpos_new = 0\n",
    "\twhile title.find('$', pos_old) != -1 and title_new.find('$', pos_new) != -1:\n",
    "\t\tpos_old = title.find('$', pos_old) + 1\n",
    "\t\tpos_new = title_new.find('$', pos_new) + 1\n",
    "\t\tpos_old_next = title.find('$', pos_old) if title.find('$', pos_old) != -1 else len(title)\n",
    "\t\tpos_new_next = title_new.find('$', pos_new) if title_new.find('$', pos_new) != -1 else len(title_new)\n",
    "\t\ttitle_new = title_new[:pos_new] + title[pos_old:pos_old_next] + title_new[pos_new_next:]\n",
    "\t\tpos_old = pos_old_next + 1\n",
    "\t\tpos_new = (title_new.find('$', pos_new) if title_new.find('$', pos_new) != -1 else len(title_new)) + 1\n",
    "\twhile title_new[-2:] == '-$' or title_new[-2:] == ',$' or title_new[-2:] == ':$':\n",
    "\t\ttitle_new = title_new[:-2] + '$'\n",
    "\treturn title_new\n",
    "\n",
    "\n",
    "# Position checkers\n",
    "# Checks if given position is between opening and closing scopes\n",
    "def check_in_scopes(text:str, pos:int, scope_open:str, scope_close:str) -> bool:\n",
    "\tif text == None:\n",
    "\t\treturn False\n",
    "\topen_prev = text.rfind(scope_open, 0, pos)\n",
    "\tclose_prev = text.rfind(scope_close, 0, pos)\n",
    "\topen_next = text.find(scope_open, pos)\n",
    "\tclose_next = text.find(scope_close, pos)\n",
    "\tafter_open = True if ((open_prev != -1 and close_prev == -1) or (open_prev > close_prev and open_prev != -1 and close_prev != -1)) else False\n",
    "\tbefore_close = True if ((open_next == -1 and close_next != -1) or (open_next > close_next and open_next != -1 and close_next != -1)) else False\n",
    "\treturn (after_open and before_close)\n",
    "\n",
    "def check_in_uri(text:str, pos:int) -> bool:\n",
    "\treturn check_in_scopes(text, pos, 'URI[[', ']]/URI')\n",
    "\n",
    "def check_in_link(text:str, pos:int) -> bool:\n",
    "\treturn check_in_scopes(text, pos, '![](', ')')\n",
    "\n",
    "def check_in_formula(text:str, pos:int) -> bool:\n",
    "\t# Main formulas\n",
    "\tin_main = check_in_scopes(text, pos, '\\\\[', '\\\\]')\n",
    "\t# Aux formulas\n",
    "\tif text == None:\n",
    "\t\treturn False\n",
    "\tpos_find = 0\n",
    "\tcnt = 0\n",
    "\tfound_before = 0\n",
    "\t# Count dollar symbols and find target position\n",
    "\twhile text.find('$', pos_find) != -1:\n",
    "\t\tpos_find = text.find('$', pos_find)\n",
    "\t\tcnt += 1\n",
    "\t\tif not found_before and pos <= pos_find:\n",
    "\t\t\tfound_before = cnt\n",
    "\t\tpos_find += 1\n",
    "\t# If cnt is not even assume that first one is garbage from title\n",
    "\tin_aux = not ((found_before + cnt) % 2) and found_before > 0\n",
    "\treturn in_main or in_aux\n",
    "\n",
    "\n",
    "# Prepare spellcheckers\n",
    "ru_dict = enchant.DictWithPWL(\"ru_RU\", PERSONAL_WORD_LIST)\n",
    "ru_checker = SpellChecker(ru_dict, filters=[EmailFilter, URLFilter])\n",
    "en_dict = enchant.DictWithPWL(\"en_US\", PERSONAL_WORD_LIST)\n",
    "en_checker = SpellChecker(en_dict, filters=[EmailFilter, URLFilter])\n",
    "def spellcheck_dict_update():\n",
    "\tglobal ru_dict\n",
    "\tglobal ru_checker\n",
    "\tglobal en_dict\n",
    "\tglobal en_checker\n",
    "\tru_dict = enchant.DictWithPWL(\"ru_RU\", PERSONAL_WORD_LIST)\n",
    "\tru_checker = SpellChecker(ru_dict, filters=[EmailFilter, URLFilter])\n",
    "\ten_dict = enchant.DictWithPWL(\"en_US\", PERSONAL_WORD_LIST)\n",
    "\ten_checker = SpellChecker(en_dict, filters=[EmailFilter, URLFilter])\n",
    "\n",
    "# Use PyEnchant spellchecker\n",
    "def do_spellcheck(text: str) -> dict:\n",
    "\tglobal ru_dict\n",
    "\tglobal ru_checker\n",
    "\tglobal en_dict\n",
    "\tglobal en_checker\n",
    "\tdictionaries = [ru_dict, en_dict]\n",
    "\tcheckers = [ru_checker, en_checker]\n",
    "\ttext_suggestions = dict()\n",
    "\n",
    "\t# Spellcheck\n",
    "\tfor i in range(len(checkers)):\n",
    "\t\tchecker = checkers[i]\n",
    "\t\tdictionary = dictionaries[i]\n",
    "\n",
    "\t\tchecker.set_text(text)\n",
    "\t\tfor woi in checker:\n",
    "\t\t\t# Exclude some wois to reduce computation time and output\n",
    "\t\t\tif check_in_uri(text, woi.wordpos) or check_in_link(text, woi.wordpos) or check_in_formula(text, woi.wordpos) or len(woi.word) < 4 or woi.wordpos in text_suggestions.keys() or text[min(woi.wordpos + len(woi.word), len(text)-1)] in ['.']:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Check if word is correct in some other language\n",
    "\t\t\tword_is_correct_in_other_dict = False\n",
    "\t\t\tfor _dictionary in dictionaries:\n",
    "\t\t\t\tword_is_correct_in_other_dict = True if _dictionary.check(woi.word) else word_is_correct_in_other_dict\n",
    "\t\t\tif word_is_correct_in_other_dict:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Generate a suggestion\n",
    "\t\t\tsim = dict()\n",
    "\t\t\tword_suggestions = set(dictionary.suggest(woi.word))\n",
    "\t\t\tfor word in word_suggestions:\n",
    "\t\t\t\tmeasure = difflib.SequenceMatcher(None, woi.word, word).ratio()\n",
    "\t\t\t\tsim[measure] = word\n",
    "\t\t\tsuggest = sim[max(sim.keys())] if len(sim.keys()) else None\n",
    "\t\t\t# Exclude some wois to reduce computation time and output\n",
    "\t\t\tif suggest == None or suggest == woi.word:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\telse:\n",
    "\t\t\t\ttext_suggestions[woi.wordpos] = (woi.word, suggest)\n",
    "\n",
    "\treturn text_suggestions\n",
    "\n",
    "def add_to_pwl(word: str):\n",
    "\twith codecs.open(PERSONAL_WORD_LIST, 'r', 'utf-8') as f:\n",
    "\t\tpwl = f.read()\n",
    "\tif pwl.find(f\"\\n{word}\\n\") == -1:\n",
    "\t\twith codecs.open(PERSONAL_WORD_LIST, 'a', 'utf-8') as f:\n",
    "\t\t\tf.write(f\"{word.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Исправление ошибок в заголовках\n",
    "\n",
    "Состоит из двух частей: \"составитель пар\" и \"подстановщик\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Составитель пар \"оригинальный - исправленный\" для заголовков\n",
    "\n",
    "Формирует xml-список всех заголовков с возможными автоматическими исправлениями (в формате было / стало):\n",
    "1. замена латиницы на агалогичную кириллицу;\n",
    "2. замена задаванных буквосочетаний (см. параметры)\n",
    "3. удаление обрамляющих знаков препинания;\n",
    "4. замена всех букв на заглавные (в том числе это избавляет дальнейшей необходимости исправлять имена);\n",
    "5. слияние разорванных на отдельные буквы слов (если рядом оказываются несколько таких слов, то они оказываются слиты вместе).\n",
    "\n",
    "Этот список необходимо просмотреть и исправить оставшиеся ошибки.\n",
    "\n",
    "Дополнительно, для помощи в поиске орфографических ошибок, формируется строка с изменениями, предложенными спеллчекером. ВНИМАНИЕ: спеллчекер может делать ошибки в именах, специфических терминах и т.п., поэтому следует использовать его результаты лишь для ориентира."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1. Составитель пар \"оригинальный - исправленный\" для заголовков:\n",
    "\n",
    "############################ VARS ################################\n",
    "WORK_DIR = \"./matphys/\"\n",
    "INPUT_FILE = \"FMEv2.xml\"\n",
    "CORRECTION_FILE = \"FMEcorr.xml\"\n",
    "COMBINATIONS_CORR = dict_merge(COMBINATIONS_CORR_GLOBAL, {\n",
    "\t'ХК' : 'Ж',\n",
    "\t'ЬI' : 'Ы',\n",
    "\t'II' : 'Ш',\n",
    "\t'I' : 'П',\n",
    "\t'J' : 'Л',\n",
    "\t'ЛАГРАНХ' : 'ЛАГРАНЖ',\n",
    "\t'ЛАТРАНХ' : 'ЛАГРАНЖ',\n",
    "})\n",
    "SPELLCHECK_ONLY = True # Use if the only thing you need from this script is spellcheck\n",
    "##################################################################\n",
    "\t\t\t\t\t\t\n",
    "\n",
    "# Check for existing xml\n",
    "filenames_raw = next(walk(WORK_DIR), (None, None, []))[2]  # [] if no file\n",
    "if not(INPUT_FILE in filenames_raw):\n",
    "\troot = ET.Element('data')\n",
    "\txml_write(root, WORK_DIR + CORRECTION_FILE)\n",
    "\n",
    "\n",
    "root = parse_xml(WORK_DIR + INPUT_FILE)\n",
    "\n",
    "\n",
    "# Get all the titles into a dict\n",
    "titles_dict = {}\n",
    "pages_dict = {}\n",
    "for article in root:\n",
    "\ttitle = get_xml_elem(article, 'title').text\n",
    "\ttitles_dict[title] = (title, title)\n",
    "\ttitle_file = get_xml_elem(article, 'title-meta/title-file')\n",
    "\tpages_dict[title] = title_file.text[title_file.text.find('-')+1:title_file.text.find('_')]\n",
    "\n",
    "\n",
    "if not SPELLCHECK_ONLY:\n",
    "\t# Correct preferred combinations and latin letters\n",
    "\tfor title in titles_dict.keys():\n",
    "\t\ttitle_new = title_handle_latin(titles_dict[title][0], COMBINATIONS_CORR)\n",
    "\t\ttitles_dict[title] = (title_new, title_new)\n",
    "\n",
    "\t# Remove bounding symbols\n",
    "\tfor title in titles_dict.keys():\n",
    "\t\ttitle_new = title_handle_bounding(titles_dict[title][0])\n",
    "\t\ttitles_dict[title] = (title_new, title_new)\n",
    "\n",
    "\t# CAPS\n",
    "\tfor title in titles_dict.keys():\n",
    "\t\ttitle_new = titles_dict[title][0].upper()\n",
    "\t\ttitles_dict[title] = (title_new, title_new)\n",
    "\n",
    "\t# Merge single-lettered words\n",
    "\tfor title in titles_dict.keys():\n",
    "\t\ttitle_new = title_handle_merge(titles_dict[title][0])\n",
    "\t\ttitles_dict[title] = (title_new, title_new)\n",
    "\n",
    "\t# Revert changes for aux formulas in titles\n",
    "\tfor title in titles_dict.keys():\n",
    "\t\ttitle_new = title_handle_formulas(titles_dict[title][0], title)\n",
    "\t\ttitles_dict[title] = (title_new, title_new)\n",
    "\n",
    "# Try spellcheck on titles\n",
    "spellcheck_dict_update()\n",
    "for title in titles_dict.keys():\n",
    "\ttitle_new = titles_dict[title][0]\n",
    "\ttitle_suggestions = do_spellcheck(title_new)\n",
    "\tfor i in range(len(title_new)):\n",
    "\t\ttitle_new = title_new[:i] + ('_' if title_new[i] not in [' ', '\\n', '\\r'] else title_new[i]) + (title_new[i+1:] if i + 1 <= len(title_new) else '')\n",
    "\tfor pos in sorted(title_suggestions.keys(), reverse=True):\n",
    "\t\ttitle_new = title_new[:pos] + title_suggestions[pos][1] + title_new[pos+len(title_suggestions[pos][0]):]\n",
    "\ttitles_dict[title] = (titles_dict[title][0], title_new)\n",
    "\n",
    "\n",
    "# Write corrections xml\n",
    "root = ET.Element('data')\n",
    "for i in titles_dict.items():\n",
    "\tpair = ET.SubElement(root, 'pair')\n",
    "\ttitle_old = ET.SubElement(pair, 'title_old')\n",
    "\ttitle_old.text = i[0]\n",
    "\ttitle_new = ET.SubElement(pair, 'title_new')\n",
    "\ttitle_new.text = i[1][0]\n",
    "\ttitle_new = ET.SubElement(pair, 'title__sc')\n",
    "\ttitle_new.text = i[1][1]\n",
    "\tpage = ET.SubElement(pair, 'page')\n",
    "\tpage.text = pages_dict[i[0]]\n",
    "xml_write(root, WORK_DIR + CORRECTION_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Подстановщик исправленных заголовков\n",
    "\n",
    "Заменяет все заголовки на исправленные согласно списку пар."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2. Подстановщик исправленных заголовков:\n",
    "\n",
    "############################ VARS ################################\n",
    "WORK_DIR = \"./matphys/\"\n",
    "INPUT_FILE = \"FMEv2.xml\"\n",
    "CORRECTION_FILE = \"FMEcorr.xml\"\n",
    "EXIT_FILE = \"FMEtitles.xml\"\n",
    "##################################################################\n",
    "\n",
    "\n",
    "\n",
    "root = parse_xml(WORK_DIR + CORRECTION_FILE)\n",
    "\n",
    "\n",
    "# Get all the corrections into a dict\n",
    "titles_dict = {}\n",
    "for pair in root:\n",
    "\ttitles_dict[get_xml_elem(pair, 'title_old').text] = get_xml_elem(pair, 'title_new').text\n",
    "\n",
    "\n",
    "root = parse_xml(WORK_DIR + INPUT_FILE)\n",
    "\n",
    "\n",
    "# Replace titles\n",
    "for article in root:\n",
    "\tget_xml_elem(article, 'title').text = titles_dict[get_xml_elem(article, 'title').text]\n",
    "xml_write(root, WORK_DIR + EXIT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Сортировщик / сливщик файлов с заголовками\n",
    "\n",
    "Сортирует статьи в файлах из данного списка в порядке страница-расположение, т.е. (если не сказано иного) в алфавитном порядке и выводит в один выходной файл. Также порядковый номер заменяется uri формата \"http://libmeta.ru/fme/article/1_Kraevaya\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Сортировщик / сливщик файлов с заголовками\n",
    "\n",
    "############################ VARS ################################\n",
    "WORK_DIR = \"./results/\"\n",
    "TITLES_DIR = \"FMEtitles/\"\n",
    "INPUT_FILES = [\"FMEtitles-p5-100.xml\", \"FMEtitles-p101-200.xml\", \"FMEtitles-p201-300.xml\", \"FMEtitles-p301-400.xml\", \"FMEtitles-p301-400-add.xml\",\n",
    "\t\t\t\t\t\t\t \"FMEtitles-p401-500.xml\", \"FMEtitles-p501-600.xml\", \"FMEtitles-p601-692.xml\", \"FMEtitles-p601-692-add.xml\"]\n",
    "EXIT_FILE = \"FMEtitles-merged.xml\"\n",
    "DISABLED = False # Use to prevent accidential URI changes\n",
    "##################################################################\n",
    "\n",
    "\n",
    "\n",
    "class Article:\n",
    "\ttitle = ''\n",
    "\tstart_title = ''\n",
    "\tend_title = ''\n",
    "\tfilename = ''\n",
    "\n",
    "\n",
    "\n",
    "# Add article title and metadata to xml tree\n",
    "def add_artice(elem:Article, root:ET.Element, num:int):\n",
    "\ttranslitted = translit(elem.title[:elem.title.find(' ')], 'ru', True)\n",
    "\twhile translitted.find('/') != -1:\n",
    "\t\ttranslitted = translitted[:translitted.find('/')] + '_' + translitted[translitted.find('/')+1:]\t\t# Prevent slash being counted as subfolder in further\n",
    "\tarticle = ET.SubElement(root, 'article', {'uri':\"http://libmeta.ru/fme/article/\"+str(num)+\"_\" + translitted})\n",
    "\ttitle = ET.SubElement(article, 'title')\n",
    "\ttitle.text = elem.title\n",
    "\ttitle_meta = ET.SubElement(article, 'title-meta')\n",
    "\ttitle_file = ET.SubElement(title_meta, 'title-file')\n",
    "\ttitle_file.text = elem.filename\n",
    "\ttitle_start = ET.SubElement(title_meta, 'title-start')\n",
    "\ttitle_start.text = str(int(elem.start_title) + 1)\n",
    "\ttitle_end = ET.SubElement(title_meta, 'title-end')\n",
    "\ttitle_end.text = elem.end_title\n",
    "\n",
    "\n",
    "# Collect all the articles\n",
    "articles_dict = {}\n",
    "for filename in INPUT_FILES:\n",
    "\troot = parse_xml(WORK_DIR + TITLES_DIR + filename)\n",
    "\tfor article in root:\n",
    "\t\tnum = ()\n",
    "\t\ttitle = get_xml_elem(article, 'title').text\n",
    "\t\telem = get_xml_elem(article, 'title-meta/title-file')\n",
    "\t\tpage = elem.text[elem.text.find('-')+1:elem.text.find('_')]\n",
    "\t\tpos = get_xml_elem(article, 'title-meta/title-start').text\n",
    "\t\tstart = get_xml_elem(article, 'title-meta/title-start').text\n",
    "\t\tend = get_xml_elem(article, 'title-meta/title-end').text\n",
    "\t\tfile = get_xml_elem(article, 'title-meta/title-file').text\n",
    "\t\tnum = (int(page), int(pos))\n",
    "\t\tarticles_dict[num] = {'title':title, 'file':file, 'start':start, 'end':end}\n",
    "\n",
    "\n",
    "# Sort keys and wrtite articles accordingly\n",
    "root = ET.Element('data')\n",
    "nums_list = sorted(list(i for i in articles_dict.keys()))\n",
    "for num in range(len(nums_list)):\n",
    "\tarticle = Article()\n",
    "\tarticle.title = articles_dict[nums_list[num]]['title']\n",
    "\tarticle.start_title = articles_dict[nums_list[num]]['start']\n",
    "\tarticle.end_title = articles_dict[nums_list[num]]['end']\n",
    "\tarticle.filename = articles_dict[nums_list[num]]['file']\n",
    "\tadd_artice(article, root, num + 1)\n",
    "if not DISABLED:\n",
    "\txml_write(root, WORK_DIR + EXIT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Парсер текстов статей\n",
    "\n",
    "По информации из указанного файла с заголовками вытаскивает в сыром виде тексты статей. Каждая статья помещается в свой .xml файл, с заголовком, содержащим номер статьи и первое слово из заголовка транслитом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Парсер текстов статей\n",
    "\n",
    "############################ VARS ################################\n",
    "TITLES_FILE = \"./results/FMEtitles-merged.xml\"\n",
    "PAGES_DIR = \"./matphys/rpages/\"\n",
    "EXIT_DIR = \"./results/FMEarticles/\"\n",
    "COMBINATIONS_CORR = {\n",
    "\t'І' : 'I'\t\t# This teo are different!\n",
    "}\n",
    "##################################################################\n",
    "\n",
    "\n",
    "class Article:\n",
    "\tstart_file = ''\n",
    "\tstart_pos = 0\n",
    "\tend_file = ''\n",
    "\tend_pos = 0\n",
    "\ttext = ''\n",
    "\ttext_orig = ''\n",
    "\turi = ''\n",
    "\ttitle = ''\n",
    "\txml = ''\n",
    "\n",
    "\tdef get_text(self):\n",
    "\t\t# Get filenames\n",
    "\t\tfilenames_raw = next(walk(PAGES_DIR), (None, None, []))[2]  # [] if no file\n",
    "\t\tfilenames = []\n",
    "\t\tfor filename in filenames_raw:\n",
    "\t\t\tif filename[-4:] == \".mmd\":\n",
    "\t\t\t\tfilenames.append(filename)\n",
    "\t\tif self.start_file == self.end_file:\n",
    "\t\t\twith codecs.open(PAGES_DIR + self.start_file, 'r', 'utf-8') as f:\n",
    "\t\t\t\tself.text += f.read()[self.start_pos:self.end_pos]\n",
    "\t\telse:\n",
    "\t\t\twith codecs.open(PAGES_DIR + self.start_file, 'r', 'utf-8') as f:\n",
    "\t\t\t\tself.text += f.read()[self.start_pos:]\n",
    "\t\t\tfor page in range(int(self.start_file[3:self.start_file.find('_')]) + 1, int(self.end_file[3:self.end_file.find('_')])):\n",
    "\t\t\t\tfor filename in filenames:\n",
    "\t\t\t\t\tif int(filename[3:filename.find('_')]) == page:\n",
    "\t\t\t\t\t\tself.text += ' ' # Add a space to prevent word merging\n",
    "\t\t\t\t\t\twith codecs.open(PAGES_DIR + filename, 'r', 'utf-8') as f:\n",
    "\t\t\t\t\t\t\tself.text += f.read()\n",
    "\t\t\tself.text += ' ' # Add a space to prevent word merging\n",
    "\t\t\twith codecs.open(PAGES_DIR + self.end_file, 'r', 'utf-8') as f:\n",
    "\t\t\t\tself.text += f.read()[:self.end_pos]\n",
    "\t\tfor comb in COMBINATIONS_CORR.keys():\n",
    "\t\t\twhile self.text.find(comb) != -1:\n",
    "\t\t\t\tself.text = self.text[:self.text.find(comb)] + COMBINATIONS_CORR[comb] + self.text[self.text.find(comb) + len(comb):]\n",
    "\t\twhile self.text != None and len(self.text) and self.text[0] in [' ', ',', '.', ':', ';', '-', '\\n', '\\r']:\n",
    "\t\t\tself.text = self.text[1:]\n",
    "\t\twhile self.text != None and len(self.text) and self.text[-1] in [' ', '\\n', '\\r']:\n",
    "\t\t\tself.text = self.text[:-1]\n",
    "\t\tself.text_orig = self.text\n",
    "\t\t# Fix several capital symbols per word\n",
    "\t\tword_left = 0\n",
    "\t\tword_right = 0\n",
    "\t\twhile word_left < len(self.text):\n",
    "\t\t\tword_right = min(len(self.text), self.text.find(' ', word_left) if self.text.find(' ', word_left) != -1 else len(self.text))\n",
    "\t\t\tword_right = min(word_right, self.text.find('\\n', word_left) if self.text.find('\\n', word_left) != -1 else len(self.text))\n",
    "\t\t\tword_right = min(word_right, self.text.find('\\r', word_left) if self.text.find('\\r', word_left) != -1 else len(self.text))\n",
    "\t\t\tword_right = min(word_right, self.text.find('-', word_left) if self.text.find('-', word_left) != -1 else len(self.text))\n",
    "\t\t\tword_right = min(word_right, self.text.find('.', word_left) if self.text.find('.', word_left) != -1 else len(self.text))\n",
    "\t\t\tword = self.text[word_left:word_right]\n",
    "\t\t\tif word != None and len(word) > 1 and not check_in_uri(self.text, word_left) and not check_in_formula(self.text, word_left) and not check_in_link(self.text, word_left):\n",
    "\t\t\t\tword = word[0] + word[1:len(word)].lower()\n",
    "\t\t\t\tself.text = self.text[:word_left] + word + self.text[word_right:]\n",
    "\t\t\tword_left = word_right + 1\n",
    "\t\n",
    "\tdef make_xml(self):\n",
    "\t\tself.get_text()\n",
    "\n",
    "\t\tarticle = ET.Element(\"article\", {'uri':self.uri})\n",
    "\t\ttitle = ET.SubElement(article, 'title')\n",
    "\t\ttitle.text = self.title\n",
    "\t\tauthor = ET.SubElement(article, 'authors')\n",
    "\t\ttitle_short = ET.SubElement(article, 'title_short')\n",
    "\t\tpages = ET.SubElement(article, 'pages')\n",
    "\t\tstart = ET.SubElement(pages, 'start')\n",
    "\t\tstart.text = self.start_file[3:self.start_file.find('_', 3)]\n",
    "\t\tend = ET.SubElement(pages, 'end')\n",
    "\t\tend.text = self.end_file[3:self.end_file.find('_', 3)]\n",
    "\t\tliterature = ET.SubElement(article, 'literature')\n",
    "\t\tliterature_orig = ET.SubElement(literature, 'literature_orig')\n",
    "\t\tformulas_remote = ET.SubElement(article, 'formulas_main')\n",
    "\t\tformulas_inline = ET.SubElement(article, 'formulas_aux')\n",
    "\t\trelations = ET.SubElement(article, 'relations')\n",
    "\t\ttext = ET.SubElement(article, 'text')\n",
    "\t\ttext.text = self.text\n",
    "\t\ttext_orig = ET.SubElement(article, 'text_orig')\n",
    "\t\ttext_orig.text = self.text_orig\n",
    "\n",
    "\t\tself.xml = prettify(article)\n",
    "\t\n",
    "\t\n",
    "\n",
    "class Title:\n",
    "\ttext = ''\n",
    "\tfile = ''\n",
    "\tstart_pos = 0\n",
    "\tend_pos = 0\n",
    "\turi = ''\n",
    "\n",
    "\n",
    "def get_title(n:int, root:ET.Element) -> Title:\n",
    "\totitle = Title()\n",
    "\tfor title in root:\n",
    "\t\tif int(title.attrib['uri'][30:title.attrib['uri'].find('_', 30)]) == n:\n",
    "\t\t\totitle.uri = title.attrib['uri']\n",
    "\t\t\totitle.text = get_xml_elem(title, 'title').text\n",
    "\t\t\totitle.file = get_xml_elem(title, 'title-meta/title-file').text\n",
    "\t\t\totitle.start_pos = int(get_xml_elem(title, 'title-meta/title-start').text)\n",
    "\t\t\totitle.end_pos = int(get_xml_elem(title, 'title-meta/title-end').text)\n",
    "\treturn otitle\n",
    "\n",
    "\n",
    "root = parse_xml(TITLES_FILE)\n",
    "\n",
    "# Create articles list\n",
    "articles_list = []\n",
    "title = Title()\n",
    "for i in range(len(root)):\n",
    "\ttitle = get_title(i + 1, root)\n",
    "\tif i:\n",
    "\t\tarticles_list[-1].end_file = title.file\n",
    "\t\tarticles_list[-1].end_pos = max(title.start_pos - 2, 0) # There is a shift for some reason\n",
    "\tarticles_list.append(Article())\n",
    "\tarticles_list[-1].uri = title.uri\n",
    "\tarticles_list[-1].title = title.text\n",
    "\tarticles_list[-1].start_file = title.file\n",
    "\tarticles_list[-1].start_pos = title.end_pos\n",
    "\tarticles_list[-1].end_file = title.file\n",
    "\twith codecs.open(PAGES_DIR + title.file, 'r', 'utf-8') as f:\n",
    "\t\tarticles_list[-1].end_pos = len(f.read())\n",
    "\n",
    "# Parse texts themselves and write\n",
    "for i in range(len(articles_list)):\n",
    "\tarticles_list[i].make_xml()\n",
    "\twith codecs.open(EXIT_DIR + '' + articles_list[i].uri[30:] + '.xml', 'w', 'utf-8') as f:\n",
    "\t\tf.write(articles_list[i].xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Проверка правописания в текстах\n",
    "\n",
    "## 5.1. Сканер\n",
    "\n",
    "Сканирует тексты из указанного диапазона статей и выносит все показавшиеся подозрительными слова в отдельный xml следующего формата:\n",
    "- Статья (имя файла в аттрибутах)\n",
    "  - Слово (позиция в тексте и флаги в аттрибутах)\n",
    "    - Исходный вариант\n",
    "    - Контекстная строка (размер задаётся в разделе параметров скрипта)\n",
    "    - Предложенная замена\n",
    "\n",
    "Предлагается два флага для определения дальнейшей \"судьбы\" слова: \"результат\" (0 -- исходное, 1 -- предложенное) и \"добавление в словарь\" (0 -- не добавлять, 1 -- добавить; применяется к выбранному результату)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1. Проверка правописания в текстах. Сканер.\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "EXIT_DIR = \"./matphys/\"\n",
    "CONTEXT_SIZE = 20\n",
    "START_ARTICLE = 1\n",
    "END_ARTICLE = 1\n",
    "DEFAULT_RESULT_FLAG = '1'\n",
    "DEFAULT_ADD_TO_PWL_FLAG = '0'\n",
    "##################################################################\n",
    "\n",
    "\n",
    "spellcheck_dict_update()\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = next(walk(ARTICLES_DIR), (None, None, []))[2]  # [] if no file\n",
    "#filenames = ['4_ABELEVA.xml']\n",
    "\n",
    "root = ET.Element('data')\n",
    "\n",
    "for filename in filenames:\n",
    "\tarticle_number = int(filename[:filename.find('_')])\n",
    "\tif article_number < START_ARTICLE or article_number > END_ARTICLE:\n",
    "\t\tcontinue\n",
    "\n",
    "\tprint(f'{filename}: found ', end='')\n",
    "\tarticle = parse_xml(ARTICLES_DIR + filename)\n",
    "\ttext = get_xml_elem(article, 'text')\n",
    "\n",
    "\t#add_to_pwl(filename[filename.find('_')+1:filename.find('.xml')])\n",
    "\n",
    "\ttext_suggestions = do_spellcheck(text.text)\n",
    "\tprint(len(text_suggestions.keys()))\n",
    "\tif len(text_suggestions.keys()):\n",
    "\t\tarticle = ET.SubElement(root, 'article', {'filename': filename})\n",
    "\t\tfor pos in text_suggestions.keys():\n",
    "\t\t\t#print(f'{pos}: {text_suggestions[pos][0]} -> {text_suggestions[pos][1]}')\n",
    "\t\t\tword = ET.SubElement(article, 'word', {'pos': str(pos), 'result': DEFAULT_RESULT_FLAG, 'add_to_pwl': DEFAULT_ADD_TO_PWL_FLAG})\n",
    "\t\t\tsource = ET.SubElement(word, 'source')\n",
    "\t\t\tsource.text = text_suggestions[pos][0]\n",
    "\t\t\tcontext = ET.SubElement(word, 'context')\n",
    "\t\t\tcontext_string = text.text[max(0, pos - CONTEXT_SIZE):min(len(text.text), pos + len(text_suggestions[pos][0]) + CONTEXT_SIZE)]\n",
    "\t\t\twhile context_string.find('\\n') != -1:\n",
    "\t\t\t\tcontext_string = context_string[:context_string.find('\\n')] + '\\\\n' + context_string[context_string.find('\\n')+1:]\n",
    "\t\t\twhile context_string.find('\\r') != -1:\n",
    "\t\t\t\tcontext_string = context_string[:context_string.find('\\r')] + '\\\\r' + context_string[context_string.find('\\r')+1:]\n",
    "\t\t\tcontext.text = context_string\n",
    "\t\t\tsuggestion = ET.SubElement(word, 'suggestion')\n",
    "\t\t\tsuggestion.text = text_suggestions[pos][1]\n",
    "\n",
    "\n",
    "with codecs.open(EXIT_DIR + f'FMEspellcheck-a{START_ARTICLE}-{END_ARTICLE}.xml', 'w', 'utf-8') as f:\n",
    "\tf.write(prettify(root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Пополнение словаря\n",
    "\n",
    "Добавляет отмеченные флагом \"добавление в словарь\" слова из всех файлов в директории спеллчека\n",
    "- Учитывается, было ли выбрано оригинальное слово или исправленное флагом \"результат\".\n",
    "- Словарь сортируется по алфавиту при каждом запуске.\n",
    "- Дубликаты удаляются при каждом запуске (символы разного регистра одинаковыми на считаются).\n",
    "- Слова добавленные вручную при запуске не удаляются.\n",
    "\n",
    "Чтобы объединить ваш словарь с другим, скопируйте и вставьте всё содрежимое нового словаря в ваш, после чего запустите скрипт. Дубликаты будут удалены, итоговый словарь будет отсортирован."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2. Проверка правописания в текстах. Сканер.\n",
    "\n",
    "############################ VARS ################################\n",
    "SPELLCHECK_DIR = \"./results/FMEspellcheck/\"\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Read PWL and form word list\n",
    "with codecs.open(PERSONAL_WORD_LIST, 'r', 'utf-8') as f:\n",
    "\tPWL_text = f.read()\n",
    "PWL_list_old = [i.strip() for i in PWL_text.split('\\n')]\n",
    "while '' in PWL_list_old:\n",
    "\tPWL_list_old.remove('')\n",
    "PWL_text = ''\n",
    "\n",
    "# Read all spellcheck outputs and create additions list\n",
    "# Get filenames needed\n",
    "filenames = next(walk(SPELLCHECK_DIR), (None, None, []))[2]  # [] if no file\n",
    "\n",
    "additions = []\n",
    "for filename in filenames:\n",
    "\troot = parse_xml(SPELLCHECK_DIR + filename)\n",
    "\tfor article in root:\n",
    "\t\tif article.tag == \"article\":\n",
    "\t\t\tfor word in article:\n",
    "\t\t\t\tif word.tag == \"word\":\n",
    "\t\t\t\t\tif word.attrib[\"add_to_pwl\"] == '1' and word.attrib[\"result\"] == '1':\n",
    "\t\t\t\t\t\tadditions.append(get_xml_elem(word, 'suggestion').text.strip())\n",
    "\t\t\t\t\telif word.attrib[\"add_to_pwl\"] == '1' and word.attrib[\"result\"] == '0':\n",
    "\t\t\t\t\t\tadditions.append(get_xml_elem(word, 'source').text.strip())\n",
    "\n",
    "# Make new PWL list and sort it\n",
    "PWL_list_new = []\n",
    "for word in PWL_list_old:\n",
    "\tif not word in PWL_list_new:\n",
    "\t\tPWL_list_new.append(word)\n",
    "for word in additions:\n",
    "\tif not word in PWL_list_new:\n",
    "\t\tPWL_list_new.append(word)\n",
    "PWL_list_new.sort()\n",
    "\n",
    "# Write PWL\n",
    "for word in PWL_list_new:\n",
    "\tPWL_text = PWL_text + word + '\\n'\n",
    "with codecs.open(PERSONAL_WORD_LIST, 'w', 'utf-8') as f:\n",
    "\tf.write(PWL_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Подстановка исправленной орфографии\n",
    "\n",
    "Подставляет в исходный текст исправленные слова или оригиналы, в зависимости от установленного флага \"результат\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Парсер авторов статьи\n",
    "\n",
    "Ищет в конце текста статей конструкции типа ` [Xxxx]. [Xxxx]. [Xxxx]` или ` [Xxxx].[Xxxx]. [Xxxx]` и итерпретирует её как автора статьи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Парсинг авторов статьи\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "COMBINATIONS_CORR = dict_merge(COMBINATIONS_CORR_UNICODE, {\n",
    "\t'II' : 'П'\n",
    "})\n",
    "LOCAL_DICT = {'0':'О', '3':'З', '6':'б'}\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = next(walk(ARTICLES_DIR), (None, None, []))[2]  # [] if no file\n",
    "\n",
    "for filename in filenames:\n",
    "\tarticle = parse_xml(ARTICLES_DIR + filename)\n",
    "\ttextelem = get_xml_elem(article, 'text')\n",
    "\ttext = textelem.text\n",
    "\tauthors = get_xml_elem(article, 'authors')\n",
    "\n",
    "\tauth_start = 1\n",
    "\tauth_list = []\n",
    "\twhile auth_start and text != None:\n",
    "\t\t# Find first non-space from the end\n",
    "\t\twhile text[-1] == ' ' or text[-1] == '\\n' or text[-1] == '\\r':\n",
    "\t\t\ttext = text[:-1]\n",
    "\n",
    "\t\tauth_start = 0\n",
    "\t\t# Try recognize\n",
    "\t\tfirst_space = max(text.rfind(' ', 0, len(text)), text.rfind('\\n', 0, len(text)), text.rfind('\\r', 0, len(text)))\n",
    "\t\tsecond_space = max(text.rfind(' ', 0, first_space), text.rfind('\\n', 0, first_space), text.rfind('\\r', 0, first_space))\n",
    "\t\tthird_space = max(text.rfind(' ', 0, second_space), text.rfind('\\n', 0, second_space), text.rfind('\\r', 0, second_space))\n",
    "\t\tif first_space >= 0 and text[first_space-1] == '.' and second_space >= 0:\n",
    "\t\t\tif text.find('.', second_space, first_space-1) != -1: # If there's no space between initials\n",
    "\t\t\t\tthird_space = second_space\n",
    "\t\t\t\tsecond_space = first_space\n",
    "\t\t\tif text[second_space-1] == '.' and third_space >= 0:\n",
    "\t\t\t\t# Check if first letters of each word are capitals\n",
    "\t\t\t\tkeep = text\n",
    "\t\t\t\tfor comb in LOCAL_DICT.keys():\n",
    "\t\t\t\t\twhile text[third_space+1:].find(comb) != -1:\n",
    "\t\t\t\t\t\ttext = text[:third_space+1+text[third_space+1:].find(comb)] + LOCAL_DICT[comb] + text[third_space+2+text[third_space+1:].find(comb):]\n",
    "\t\t\t\tif re.match(r\"[A-ZА-ЯІ]\", text[first_space+1]) != None and re.match(r\"[A-ZА-ЯІ]\", text[second_space+1]) != None and re.match(r\"[A-ZА-ЯІ]\", text[third_space+1]) != None:\n",
    "\t\t\t\t\tauth_start = third_space + 1\n",
    "\t\t\t\ttext = keep\n",
    "\n",
    "\t\tif auth_start: # Suggest that an article cannot consist of author only and therefore auth_start should be > 0\n",
    "\t\t\t#print(article.attrib['uri'], author_text)\n",
    "\t\t\tauthor_text = text[auth_start:]\n",
    "\t\t\tif author_text[author_text.find('.')+1] != ' ': # Add space if there's no one between initials\n",
    "\t\t\t\tauthor_text = author_text[:author_text.find('.')+1] + ' ' + author_text[author_text.find('.')+1:]\n",
    "\t\t\tif author_text[-1] == '.' or author_text[-1] == ',':\n",
    "\t\t\t\tauthor_text = author_text[:-1]\n",
    "\t\t\t# convert wrong symbols\n",
    "\t\t\tfor comb in dict_merge(COMBINATIONS_CORR, LOCAL_DICT).keys():\n",
    "\t\t\t\twhile author_text.find(comb) != -1:\n",
    "\t\t\t\t\tauthor_text = author_text[:author_text.find(comb)] + dict_merge(COMBINATIONS_CORR, LOCAL_DICT)[comb] + author_text[author_text.find(comb) + len(comb):]\n",
    "\t\t\t\n",
    "\t\t\tauth_list.append(author_text)\n",
    "\t\t\ttext = text[:auth_start]\n",
    "\n",
    "\t# add authors, reverse their order to alphabetic\n",
    "\tfor auth in reversed(auth_list):\n",
    "\t\tauthor = ET.SubElement(authors, 'author')\n",
    "\t\tauthor.text = auth\n",
    "\n",
    "\ttextelem.text = text\n",
    "\twith codecs.open(ARTICLES_DIR + filename, 'w', 'utf-8') as f:\n",
    "\t\tf.write(prettify(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Парсер литературы\n",
    "\n",
    "После извлечения авторов статьи в конце за текстом статьи присутствует только строчка литературы, если вообще присутствует. Поэтому ищется и извлекается фрагмент начиная с \"`Лит.:`\". Он разделяется на сегменты по \"`[num]`\", а сегменты на подфрагменты по запятым. Общий вид сегмента полагается следующим: \"`[Авторы (возможно несколько, определяются по наличию инициалов в конце подфрагмента)], Название (возможно содержит запятые), Номер тома (может отсутствовать), [Информация об издании (может частично или полностью отсутствовать)], Год, [Прочее (главы, страницы и прочее, может отсутствовать)];`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Парсинг литературы\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "COMBINATIONS_CORR_LOCAL = dict_merge(dict_merge(COMBINATIONS_CORR_ALPHABET, COMBINATIONS_CORR_UNICODE), {'J':'Л'})\n",
    "##################################################################\n",
    "\n",
    "\n",
    "class Unit:\n",
    "\tauthors = []\n",
    "\ttitle = \"\"\n",
    "\tpublication = \"\"\n",
    "\tyear = \"\"\n",
    "\tother = \"\"\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = next(walk(ARTICLES_DIR), (None, None, []))[2]  # [] if no file\n",
    "\n",
    "for filename in filenames:\n",
    "\tarticle = parse_xml(ARTICLES_DIR + filename)\n",
    "\ttextelem = get_xml_elem(article, 'text')\n",
    "\ttext = textelem.text\n",
    "\tliterature = get_xml_elem(article, 'literature')\n",
    "\tliterature_orig = get_xml_elem(literature, 'literature_orig')\n",
    "\n",
    "\tif textelem.text != None and len(textelem.text):\n",
    "\t\t#Find literature start position and extract if present\n",
    "\t\tfor key in COMBINATIONS_CORR_LOCAL.keys():\n",
    "\t\t\twhile text.find(key) != -1:\n",
    "\t\t\t\ttext = text[:text.find(key)] + COMBINATIONS_CORR_LOCAL[key] + text[text.find(key)+1:]\n",
    "\t\ttext = text.upper()\n",
    "\t\tlit_pos = text.rfind('\\nЛИТ.: ')\n",
    "\t\tlit_pos = text.rfind('\\rЛИТ.: ') if lit_pos == -1 else lit_pos\n",
    "\t\tlit_pos = text.rfind(' ЛИТ.: ') if lit_pos == -1 else lit_pos\n",
    "\t\tif lit_pos != -1:\n",
    "\t\t\tliterature_orig.text = textelem.text[lit_pos:]\n",
    "\t\t\twhile literature_orig.text[0] in [' ', '\\n', '\\r']:\n",
    "\t\t\t\tliterature_orig.text = literature_orig.text[1:]\n",
    "\t\t\ttextelem.text = textelem.text[:lit_pos]\n",
    "\t\t\twhile textelem.text[-1] in [' ', '\\n', '\\r']:\n",
    "\t\t\t\ttextelem.text = textelem.text[:-1]\n",
    "\n",
    "\n",
    "\t\t\t# Parse literature string\n",
    "\t\t\ttext = literature_orig.text\n",
    "\t\t\tunits = []\n",
    "\t\t\tnum = 1\n",
    "\t\t\twhile text.find('['+str(num)+']') != -1:\n",
    "\t\t\t\tunits.append(text[text.find('['+str(num)+']')+len('['+str(num)+']'):(text.find('['+str(num+1)+']') if text.find('['+str(num+1)+']') != -1 else len(text))])\n",
    "\t\t\t\tnum += 1\n",
    "\t\t\tfor unit in units:\n",
    "\t\t\t\tlogical_parts = Unit()\n",
    "\t\t\t\tlogical_parts.authors.clear()\n",
    "\t\t\t\tsubunits = unit.split(',')\n",
    "\t\t\t\twhile '' in subunits:\n",
    "\t\t\t\t\tsubunits.remove('')\n",
    "\t\t\t\tpos_last_auth = -1\n",
    "\t\t\t\tpos_last_title = -1\n",
    "\t\t\t\tpos_thome = -1\n",
    "\t\t\t\tpos_transl = -1\n",
    "\t\t\t\tpos_pub_num = -1\n",
    "\t\t\t\tpos_pub_place = -1\n",
    "\t\t\t\tpos_year = -1\n",
    "\n",
    "\n",
    "\t\t\t\t# Define positions of most common pats of literature string\n",
    "\t\t\t\tfor i in range(len(subunits)):\n",
    "\t\t\t\t\ttext = subunits[i]\n",
    "\t\t\t\t\twhile text[-1] in [' ', '\\n', '\\r', ';']:\n",
    "\t\t\t\t\t\ttext = text[:-1]\n",
    "\t\t\t\t\twhile text[0] in [' ', '\\n', '\\r']:\n",
    "\t\t\t\t\t\ttext = text[1:]\n",
    "\t\t\t\t\tsubunits[i] = text\n",
    "\n",
    "\t\t\t\t\tif pos_last_auth + 1 == i: # Recognize authors\n",
    "\t\t\t\t\t\tkeep = text\n",
    "\t\t\t\t\t\tfor j in range(len(text)):\n",
    "\t\t\t\t\t\t\tif text[j] in COMBINATIONS_CORR_UNICODE:\n",
    "\t\t\t\t\t\t\t\ttext = text[:j] + COMBINATIONS_CORR_UNICODE[text[j]] + text[j+1:]\n",
    "\t\t\t\t\t\tif text[-1] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-2]) != None and text[-3] == ' ' and text[-4] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-5]) != None:\n",
    "\t\t\t\t\t\t\t# \"X. X.\"\n",
    "\t\t\t\t\t\t\tpos_last_auth = i\n",
    "\t\t\t\t\t\t\tpos_initials = -5\n",
    "\t\t\t\t\t\telif text[-1] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-2]) != None and text[-3] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-4]) != None:\n",
    "\t\t\t\t\t\t\t# \"X.X.\"\n",
    "\t\t\t\t\t\t\tpos_last_auth = i\n",
    "\t\t\t\t\t\t\ttext = text[:-2] + ' ' + text[-2:]\n",
    "\t\t\t\t\t\t\tpos_initials = -5\n",
    "\t\t\t\t\t\telif text[-1] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-2]) != None:\n",
    "\t\t\t\t\t\t\t# \"X.\"\n",
    "\t\t\t\t\t\t\tpos_last_auth = i\n",
    "\t\t\t\t\t\t\tpos_initials = -2\n",
    "\t\t\t\t\t\telse: # Title starts\n",
    "\t\t\t\t\t\t\ttext = keep\n",
    "\t\t\t\t\t\t# If correct\n",
    "\t\t\t\t\t\tif pos_last_auth == i:\n",
    "\t\t\t\t\t\t\tsurname = text[:pos_initials]\n",
    "\t\t\t\t\t\t\twhile surname.find(' ') != -1:\n",
    "\t\t\t\t\t\t\t\tsurname = surname[:surname.find(' ')] + surname[surname.find(' ')+1:]\n",
    "\t\t\t\t\t\t\ttext = surname + ' ' + text[pos_initials:]\n",
    "\t\t\t\t\t\t\tj = 1\n",
    "\t\t\t\t\t\t\twhile j < len(text):\n",
    "\t\t\t\t\t\t\t\tif re.match(r\"[А-ЯA-Z]\", text[j]) != None and re.match(r\"[а-яa-z]\", text[j-1]) != None:\n",
    "\t\t\t\t\t\t\t\t\ttext = text[:j] + ' ' + text[j:]\n",
    "\t\t\t\t\t\t\t\t\tj = 1\n",
    "\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\tj += 1\n",
    "\t\t\t\t\t\t\tsubunits[i] = text\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tif pos_thome == -1: # Recognize thome\n",
    "\t\t\t\t\t\t\tkeep = text\n",
    "\t\t\t\t\t\t\tfor j in range(len(text)):\n",
    "\t\t\t\t\t\t\t\tif text[j] in COMBINATIONS_CORR_GLOBAL:\n",
    "\t\t\t\t\t\t\t\t\ttext = text[:j] + COMBINATIONS_CORR_GLOBAL[text[j]] + text[j+1:]\n",
    "\t\t\t\t\t\t\tif text.upper().find('Т.') != -1:\n",
    "\t\t\t\t\t\t\t\tpos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "\t\t\t\t\t\t\t\tpos_thome = i\n",
    "\t\t\t\t\t\t\ttext = keep\n",
    "\t\t\t\t\t\tif pos_transl == -1: # Recognize publication number\n",
    "\t\t\t\t\t\t\tkeep = text\n",
    "\t\t\t\t\t\t\tfor j in range(len(text)):\n",
    "\t\t\t\t\t\t\t\tif text[j] in COMBINATIONS_CORR_GLOBAL:\n",
    "\t\t\t\t\t\t\t\t\ttext = text[:j] + COMBINATIONS_CORR_GLOBAL[text[j]] + text[j+1:]\n",
    "\t\t\t\t\t\t\tif text.upper().find('ПЕР.') != -1:\n",
    "\t\t\t\t\t\t\t\tpos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "\t\t\t\t\t\t\t\tpos_transl = i\n",
    "\t\t\t\t\t\t\ttext = keep\n",
    "\t\t\t\t\t\tif pos_pub_num == -1: # Recognize publication number\n",
    "\t\t\t\t\t\t\tkeep = text\n",
    "\t\t\t\t\t\t\tfor j in range(len(text)):\n",
    "\t\t\t\t\t\t\t\tif text[j] in COMBINATIONS_CORR_GLOBAL:\n",
    "\t\t\t\t\t\t\t\t\ttext = text[:j] + COMBINATIONS_CORR_GLOBAL[text[j]] + text[j+1:]\n",
    "\t\t\t\t\t\t\tif text.upper().find('ИЗД.') != -1:\n",
    "\t\t\t\t\t\t\t\tpos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "\t\t\t\t\t\t\t\tpos_pub_num = i\n",
    "\t\t\t\t\t\t\ttext = keep\n",
    "\t\t\t\t\t\tif pos_pub_place == -1: # Recognize publication place\n",
    "\t\t\t\t\t\t\tkeep = text\n",
    "\t\t\t\t\t\t\tfor j in range(len(text)):\n",
    "\t\t\t\t\t\t\t\tif text[j] in COMBINATIONS_CORR_GLOBAL:\n",
    "\t\t\t\t\t\t\t\t\ttext = text[:j] + COMBINATIONS_CORR_GLOBAL[text[j]] + text[j+1:]\n",
    "\t\t\t\t\t\t\tif text.upper() in ['М.', 'Л.', 'СПБ.', 'М.Л.', 'Л.М.', 'М.СПБ.', 'СПБ.М.']:\n",
    "\t\t\t\t\t\t\t\tpos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "\t\t\t\t\t\t\t\tpos_pub_place = i\n",
    "\t\t\t\t\t\t\ttext = keep\n",
    "\t\t\t\t\t\t# If correct\n",
    "\t\t\t\t\t\tif pos_last_auth != i and (pos_thome == i or pos_pub_num == i or pos_pub_place == i):\n",
    "\t\t\t\t\t\t\tfor j in range(len(text)):\n",
    "\t\t\t\t\t\t\t\tif text[j] in COMBINATIONS_CORR_UNICODE:\n",
    "\t\t\t\t\t\t\t\t\tsubunits[i] = text[:j] + COMBINATIONS_CORR_UNICODE[text[j]] + text[j+1:]\n",
    "\n",
    "\t\t\t\t\t\tif pos_year == -1 and len(text) >= 4: # Recognize year\n",
    "\t\t\t\t\t\t\tnumbers = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\t\t\t\t\t\t\tfor j in range(len(text) - 3):\n",
    "\t\t\t\t\t\t\t\tif text[j] in numbers and text[j+1] in numbers and text[j+2] in numbers and text[j+3] in numbers:\n",
    "\t\t\t\t\t\t\t\t\tpos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "\t\t\t\t\t\t\t\t\tpos_year = i\n",
    "\t\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\t# if correct\n",
    "\t\t\t\t\t\t\tif pos_year == i:\n",
    "\t\t\t\t\t\t\t\tsubunits[i] = text[j:j+4]\n",
    "\n",
    "\n",
    "\t\t\t\t# Extract info from literature string using positions defined above\n",
    "\t\t\t\tfor i in range(len(subunits)):\n",
    "\t\t\t\t\ttext = subunits[i]\n",
    "\t\t\t\t\tif pos_last_auth >= i: # Author\n",
    "\t\t\t\t\t\tlogical_parts.authors.append(text)\n",
    "\t\t\t\t\telif pos_last_auth < i and pos_last_title >= i: # Title\n",
    "\t\t\t\t\t\tlogical_parts.title = logical_parts.title + ('' if len(logical_parts.title) == 0 else ', ') + text\n",
    "\t\t\t\t\telif pos_year == i: # Year\n",
    "\t\t\t\t\t\tlogical_parts.year = logical_parts.year + ('' if len(logical_parts.year) == 0 else ', ') + text\n",
    "\t\t\t\t\telif ((pos_pub_num <= i and pos_pub_num != -1) or (pos_pub_place <= i and pos_pub_place != -1) or (pos_transl <= i and pos_transl != -1) or (pos_thome + 1 <= i and pos_thome != -1)) and pos_year > i: # Publication\n",
    "\t\t\t\t\t\tlogical_parts.publication = logical_parts.publication + ('' if len(logical_parts.publication) == 0 else ', ') + text\n",
    "\t\t\t\t\telse: # Other\n",
    "\t\t\t\t\t\tlogical_parts.other = logical_parts.other + ('' if len(logical_parts.other) == 0 else ', ') + text\n",
    "\n",
    "\n",
    "\t\t\t\t# Debug section\n",
    "\t\t\t\t\"\"\"print('\\n', filename, unit)\n",
    "\t\t\t\tprint('authors:', logical_parts.authors)\n",
    "\t\t\t\tprint('title:', logical_parts.title)\n",
    "\t\t\t\tprint('publication:', logical_parts.publication)\n",
    "\t\t\t\tprint('year:', logical_parts.year)\n",
    "\t\t\t\tprint('other:', logical_parts.other)\n",
    "\t\t\t\tprint(pos_last_auth, pos_last_title, pos_thome, pos_transl, pos_pub_num, pos_pub_place, pos_year)\"\"\"\n",
    "\n",
    "\n",
    "\t\t\t\t# Add literature unit\n",
    "\t\t\t\tunit = ET.SubElement(literature, \"unit\")\n",
    "\t\t\t\tfor auth_str in logical_parts.authors:\n",
    "\t\t\t\t\tauthor = ET.SubElement(unit, \"author\")\n",
    "\t\t\t\t\tauthor.text = auth_str\n",
    "\t\t\t\ttitle = ET.SubElement(unit, \"title\")\n",
    "\t\t\t\ttitle.text = logical_parts.title\n",
    "\t\t\t\tpublication = ET.SubElement(unit, \"publication\")\n",
    "\t\t\t\tpublication.text = logical_parts.publication\n",
    "\t\t\t\tyear = ET.SubElement(unit, \"year\")\n",
    "\t\t\t\tyear.text = logical_parts.year\n",
    "\t\t\t\tother = ET.SubElement(unit, \"other\")\n",
    "\t\t\t\tother.text = logical_parts.other\n",
    "\n",
    "\n",
    "\t\t\t# Write xml\n",
    "\t\t\twith codecs.open(ARTICLES_DIR + filename, 'w', 'utf-8') as f:\n",
    "\t\t\t\tf.write(prettify(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Парсер ссылок типа \"смотри также\"\n",
    "\n",
    "Ищет в тексте ссылки начинающиеся на `\"см. [другие опциональные вводные слова]\"` и пытается найти соответствующие им статьи в энциклопедии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Парсер ссылок типа \"смотри также\"\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "##################################################################\n",
    "\n",
    "\n",
    "class Article:\n",
    "\ttitle = ''\n",
    "\tfile = ''\n",
    "\n",
    "\n",
    "# Find previous space / newline from given position\n",
    "def find_prev_space(text: str, start_pos: int) -> int:\n",
    "\tnewpos_s = text.rfind(' ', 0, start_pos)\n",
    "\tnewpos_s = 0 if newpos_s == -1 else newpos_s\n",
    "\tnewpos_n = text.rfind('\\n', 0, start_pos)\n",
    "\tnewpos_n = 0 if newpos_n == -1 else newpos_n\n",
    "\tnewpos_r = text.rfind('\\r', 0, start_pos)\n",
    "\tnewpos_r = 0 if newpos_r == -1 else newpos_r\n",
    "\treturn max(newpos_s, newpos_n, newpos_r)\n",
    "# Find next space / newline from given position\n",
    "def find_next_space(text: str, start_pos: int) -> int:\n",
    "\tnewpos_s = text.find(' ', start_pos + 1)\n",
    "\tnewpos_s = len(text) if newpos_s == -1 else newpos_s\n",
    "\tnewpos_n = text.find('\\n', start_pos + 1)\n",
    "\tnewpos_n = len(text) if newpos_n == -1 else newpos_n\n",
    "\tnewpos_r = text.find('\\r', start_pos + 1)\n",
    "\tnewpos_r = len(text) if newpos_r == -1 else newpos_r\n",
    "\treturn min(newpos_s, newpos_n, newpos_r)\n",
    "\n",
    "\n",
    "# Try find a matching title to the given one\n",
    "def find_matching_title(seq: str, titles_list: list) -> (bool, bool, int):\n",
    "\tmatch_possible = False\n",
    "\tmatches_list = []\n",
    "\tlist_pos = 0\n",
    "\tseq_list = seq.split(' ')\n",
    "\twhile list_pos < len(titles_list):\n",
    "\t\ttitle = titles_list[list_pos].title.split(' ')\n",
    "\t\tmatch_local = False\n",
    "\t\tif len(title) >= len(seq_list):\n",
    "\t\t\tmatch_local = True\n",
    "\t\t\tfor i in range(len(seq_list)):\n",
    "\t\t\t\tmatch_local = False if seq_list[i] != title[i] else match_local\n",
    "\t\t\tmatch_possible = match_possible or match_local\n",
    "\t\t\tif match_local and len(title) == len(seq_list):\n",
    "\t\t\t\t# Solid match found\n",
    "\t\t\t\treturn (True, True, True, list_pos)\n",
    "\t\tif match_local:\n",
    "\t\t\tmatches_list.append(list_pos)\n",
    "\t\t\n",
    "\t\tlist_pos += 1\n",
    "\n",
    "\t# If only one local match consider possible solid match where title is longer than the link sequence\n",
    "\tif len(matches_list) == 1:\n",
    "\t\treturn (True, True, False, matches_list[0])\n",
    "\n",
    "\t# No solid match found\n",
    "\treturn (match_possible, False, False, -1)\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = next(walk(ARTICLES_DIR), (None, None, []))[2]  # [] if no file\n",
    "\n",
    "# Get all the titles into a list\n",
    "titles_list = []\n",
    "for filename in filenames:\n",
    "\tarticle = parse_xml(ARTICLES_DIR + filename)\n",
    "\ttitle = get_xml_elem(article, 'title').text\n",
    "\tarticle_obj = Article()\n",
    "\tarticle_obj.title = title\n",
    "\tarticle_obj.file = filename\n",
    "\ttitles_list.append(article_obj)\n",
    "\n",
    "### DEBUG\t\t\t\t\t\t\t\t\t\t\t\t###\n",
    "#cnt = 5\t\t\t\t\t\t\t\t\t\t\t\t\t###\n",
    "#filenames = ['3564_JuLA-FARRI.xml']\t###\n",
    "###################################\n",
    "n = 0\n",
    "for filename in filenames:\n",
    "\t### DEBUG\t\t\t\t###\n",
    "\t#cnt -= 1\t\t\t\t###\n",
    "\t#if not cnt:\t\t\t###\n",
    "\t#\tbreak\t\t\t\t\t###\n",
    "\t###################\n",
    "\tarticle = parse_xml(ARTICLES_DIR + filename)\n",
    "\ttextelem = get_xml_elem(article, 'text')\n",
    "\ttext = textelem.text\n",
    "\n",
    "\tif text != None and len(text):\n",
    "\t\t# Move along the text from right to left to allow easier uri insertion\n",
    "\t\tfind_right = len(text)\n",
    "\t\tfind_left = find_prev_space(text, find_right)\n",
    "\n",
    "\t\t# Find link starting word\n",
    "\t\twhile find_left != -1:\n",
    "\t\t\tword = title_handle_latin(text[find_left:find_right].strip(), COMBINATIONS_CORR_GLOBAL).upper()\n",
    "\t\t\tif word == 'СМ.':\n",
    "\t\t\t\tborder_left = find_right\n",
    "\t\t\t\tborder_right = find_right\n",
    "\t\t\t\t_border_right = find_right\n",
    "\t\t\t\tborder_find_allowed = True\n",
    "\t\t\t\tmatch_possible = False\n",
    "\t\t\t\tmatch_single = False\n",
    "\t\t\t\t_match_single = False\n",
    "\t\t\t\tmatch_exact = False\n",
    "\t\t\t\tmatch_pos = -1\n",
    "\t\t\t\t_match_pos = -1\n",
    "\t\t\t\t#border_cnt = 0\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t## DEBUG\n",
    "\t\t\t\twhile border_find_allowed:\n",
    "\t\t\t\t\tborder_right = find_next_space(text, border_right)\n",
    "\t\t\t\t\t#border_cnt += 1\t\t\t\t\t\t\t\t\t\t\t\t\t\t## DEBUG\n",
    "\t\t\t\t\tborder_find_allowed = False if border_right == len(text) else True\n",
    "\t\t\t\t\tevent = title_handle_latin(text[find_right:border_right].strip(), COMBINATIONS_CORR_GLOBAL).upper()\n",
    "\t\t\t\t\tif event in ['В', 'ПРИ']:\n",
    "\t\t\t\t\t\t# Possible starting words continuation\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\tif event in ['ТАКЖЕ', 'В СТ.', 'ПРИ СТ.', 'seeAlso', 'sameAs']:\n",
    "\t\t\t\t\t\t# Confirmed starting words continuation\n",
    "\t\t\t\t\t\tfind_right = border_right\n",
    "\t\t\t\t\t\t#border_cnt = 0\t\t\t\t\t\t\t\t\t\t\t\t\t\t## DEBUG\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\t# Extract word sequence and try find a matching title from list\n",
    "\t\t\t\t\tevent = title_handle_formulas(title_handle_bounding(title_handle_latin(text[border_left:border_right].strip(), COMBINATIONS_CORR_GLOBAL).upper()), text[border_left:border_right].strip())\n",
    "\t\t\t\t\t(match_possible, match_single, match_exact, match_pos) = find_matching_title(event, titles_list)\n",
    "\t\t\t\t\tborder_find_allowed = border_find_allowed and match_possible\n",
    "\t\t\t\t\t#border_find_allowed = border_find_allowed or border_cnt <= 5 ##DEBUG\n",
    "\t\t\t\t\t# Remember if single match\n",
    "\t\t\t\t\tif match_single:\n",
    "\t\t\t\t\t\t_border_right = border_right\n",
    "\t\t\t\t\t\t_match_pos = match_pos\n",
    "\t\t\t\t\t# Consider last single match as exact\n",
    "\t\t\t\t\tif (not match_single and _match_single) or (not border_find_allowed and match_single):\n",
    "\t\t\t\t\t\tborder_right = _border_right\n",
    "\t\t\t\t\t\tmatch_pos = _match_pos\n",
    "\t\t\t\t\t\tmatch_exact = True\n",
    "\t\t\t\t\t_match_single = match_single\n",
    "\t\t\t\t\tprint(event, match_possible, match_single, match_exact, match_pos, titles_list[match_pos].title)\n",
    "\t\t\t\t\t# Process exact match\n",
    "\t\t\t\t\tif match_exact:\n",
    "\t\t\t\t\t\t_match_single = False\n",
    "\t\t\t\t\t\t# Add an inter-link\n",
    "\t\t\t\t\t\tn += 1\n",
    "\t\t\t\t\t\tborder_left += 1 if text[border_left] in [' ', '\\n', '\\r'] else 0\n",
    "\t\t\t\t\t\twhile re.match(r\"[!#%&'*+-.^_`|~:;]\", text[border_right - 1]) != None:\n",
    "\t\t\t\t\t\t\tborder_right -= 1\n",
    "\t\t\t\t\t\turi = 'http://libmeta.ru/fme/relation' + article.attrib['uri'][article.attrib['uri'].rfind('/', 0, article.attrib['uri'].find('_')):article.attrib['uri'].find('_')+1] + str(n) + article.attrib['uri'][article.attrib['uri'].find('_'):]\n",
    "\t\t\t\t\t\trelations = get_xml_elem(article, 'relations')\n",
    "\t\t\t\t\t\trelation = ET.SubElement(relations, 'relation', {'uri':uri})\n",
    "\t\t\t\t\t\trel_text = ET.SubElement(relation, 'rel_text')\n",
    "\t\t\t\t\t\trel_text.text = text[border_left:border_right]\n",
    "\t\t\t\t\t\trel_tgt = ET.SubElement(relation, 'target')\n",
    "\t\t\t\t\t\trelated_article = parse_xml(ARTICLES_DIR + titles_list[match_pos].file)\n",
    "\t\t\t\t\t\trel_tgt.text = related_article.attrib['uri']\n",
    "\t\t\t\t\t\ttext = text[:border_left] + 'URI[[' + uri + ']]/URI' + text[border_right:]\n",
    "\t\t\t\t\t\t# Continue in case of multilink\n",
    "\t\t\t\t\t\tborder_left += len('URI[[' + uri + ']]/URI')\n",
    "\t\t\t\t\t\twhile border_left < len(text) and not text[border_left] in [' ', '\\n', '\\r']:\n",
    "\t\t\t\t\t\t\tborder_left += 1\n",
    "\t\t\t\t\t\tborder_right = border_left\n",
    "\n",
    "\t\t\t\t## DEBUG\n",
    "\t\t\t\t#print(f'\\nFound in {filename}:\\n\t{text[find_left:find_right].strip()} ||| {text[find_right:border_right].strip()} {\"\" if match_exact else \"NO MATCH FOUND\"}')\n",
    "\t\t\t\t#if match_exact:\n",
    "\t\t\t\t#\tprint(f'\tMatch in {titles_list[match_pos].file}, \\\"{titles_list[match_pos].title}\\\"')\n",
    "\n",
    "\t\t\tfind_right = find_left\n",
    "\t\t\tfind_left = find_prev_space(text, find_left) if find_left else -1\n",
    "\n",
    "\t# Write xml\n",
    "\ttextelem.text = text\n",
    "\twith codecs.open(ARTICLES_DIR + filename, 'w', 'utf-8') as f:\n",
    "\t\tf.write(prettify(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Парсер формул\n",
    "\n",
    "Выносит из текстов ранее подготовленных xml-файлов статей сначала выносные, а затем строчные формулы, оставляя на их месте ссылку внутри их математического окружения. \n",
    "\n",
    "Минимальная длина в символах, которой должна обладать строчная формула, настраивается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Парсер формул\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "MIN_INLINE_LEN = 0\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = next(walk(ARTICLES_DIR), (None, None, []))[2]  # [] if no file\n",
    "\n",
    "for filename in filenames:\n",
    "\tarticle = parse_xml(ARTICLES_DIR + filename)\n",
    "\t#print('REMOTES: ' + article.attrib['uri'])\n",
    "\ttext = get_xml_elem(article, 'text')\n",
    "\tformulas_main = get_xml_elem(article, 'formulas_main')\n",
    "\tformulas_aux = get_xml_elem(article, 'formulas_aux')\n",
    "\t\t\t\n",
    "# Get main formulas\n",
    "\tpos_find = 0\n",
    "\tpos_start = 0\n",
    "\tpos_end = 0\n",
    "\tn = 1\n",
    "\twhile text.text != None and text.text.find('\\\\[', pos_find) != -1:\n",
    "\t\tpos_start = text.text.find('\\\\[', pos_find) + 2\n",
    "\t\tpos_end = text.text.find('\\\\]', pos_start)\n",
    "\t\twhile text.text[pos_start] == '\\n':\n",
    "\t\t\tpos_start += 1\n",
    "\t\twhile text.text[pos_end-1] == '\\n':\n",
    "\t\t\tpos_end -= 1\n",
    "\t\tpos_find = pos_start\n",
    "\t\turi = 'http://libmeta.ru/fme/formula/main' + article.attrib['uri'][article.attrib['uri'].rfind('/', 0, article.attrib['uri'].find('_')):article.attrib['uri'].find('_')+1] + str(n) + article.attrib['uri'][article.attrib['uri'].find('_'):]\n",
    "\t\tn += 1\n",
    "\t\tformula = ET.SubElement(formulas_main, 'formula', {'uri':uri})\n",
    "\t\tformula.text = text.text[pos_start:pos_end]\n",
    "\t\ttext.text = text.text[:pos_start] + 'URI[[' + uri + ']]/URI' + text.text[pos_end:]\n",
    "\n",
    "# Get auxilary formulas\n",
    "\tpos_find = 0\n",
    "\tpos_start = 0\n",
    "\tpos_end = 0\n",
    "\tcnt = 0\n",
    "\tn = 1\n",
    "\t# Count dollar symbols\n",
    "\twhile text.text != None and text.text.find('$', pos_find) != -1:\n",
    "\t\tpos_find = text.text.find('$', pos_find) + 1\n",
    "\t\tcnt += 1\n",
    "\t# If cnt is not even assume that first one is garbage from title\n",
    "\tpos_find = 0\n",
    "\tif cnt % 2:\n",
    "\t\tpos_find = text.text.find('$', pos_find)\n",
    "\t\ttext.text = text.text[:pos_find] + '#' + text.text[pos_find+1:]\n",
    "\twhile text.text != None and text.text.find('$', pos_find) != -1:\n",
    "\t\tpos_start = text.text.find('$', pos_find) + 1\n",
    "\t\tpos_end = text.text.find('$', pos_start)\n",
    "\t\tif not check_in_uri(text.text, pos_start) and not check_in_uri(text.text, pos_end):\n",
    "\t\t\twhile text.text[pos_start] == '\\n':\n",
    "\t\t\t\tpos_start += 1\n",
    "\t\t\twhile text.text[pos_end-1] == '\\n':\n",
    "\t\t\t\tpos_end -= 1\n",
    "\t\t\tpos_find = pos_start\n",
    "\t\t\tif pos_end - pos_start >= MIN_INLINE_LEN:\n",
    "\t\t\t\turi = 'http://libmeta.ru/fme/formula/aux' + article.attrib['uri'][article.attrib['uri'].rfind('/', 0, article.attrib['uri'].find('_')):article.attrib['uri'].find('_')+1] + str(n) + article.attrib['uri'][article.attrib['uri'].find('_'):]\n",
    "\t\t\t\tn += 1\n",
    "\t\t\t\tformula = ET.SubElement(formulas_aux, 'formula', {'uri':uri})\n",
    "\t\t\t\tformula.text = text.text[pos_start:pos_end]\n",
    "\t\t\t\ttext.text = text.text[:pos_start] + 'URI[[' + uri + ']]/URI' + text.text[pos_end:]\n",
    "\t\t\tpos_find = text.text.find('$', pos_find) + 1\n",
    "\t\telse:\n",
    "\t\t\tpos_find = pos_end + 1\n",
    "\n",
    "\twith codecs.open(ARTICLES_DIR + filename, 'w', 'utf-8') as f:\n",
    "\t\tf.write(prettify(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Вынос формул\n",
    "\n",
    "Выносит все формулы в отдельный файл с указанием типа для возможной последующей обработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1. Вынос формул\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "EXIT_FILE = \"./results/FMEformulas.xml\"\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = next(walk(ARTICLES_DIR), (None, None, []))[2]  # [] if no file\n",
    "\n",
    "\n",
    "formulas = ET.Element('formulas')\n",
    "\n",
    "for filename in filenames:\n",
    "\troot = parse_xml(ARTICLES_DIR + filename)\n",
    "\tfmain = get_xml_elem(root, 'formulas_main')\n",
    "\tfaux = get_xml_elem(root, 'formulas_aux')\n",
    "\t\n",
    "\tfor formula in fmain:\n",
    "\t\tformulas.append(formula)\n",
    "\tfor formula in faux:\n",
    "\t\tformulas.append(formula)\n",
    "\n",
    "with codecs.open(EXIT_FILE, 'w', 'utf-8') as f:\n",
    "\tf.write(prettify(formulas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Проверка формул\n",
    "\n",
    "Случайным образом выбирает 20 случайных формул (из случайных статей) и ставляет их в математическое окружение Markdown для визуальной проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2. Проверка формул\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "EXIT_FILE = \"./matphys/FMEformulas_check.md\"\n",
    "NUMBER = 20\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = next(walk(ARTICLES_DIR), (None, None, []))[2]  # [] if no file\n",
    "\n",
    "\n",
    "file = ''\n",
    "\n",
    "i = 0\n",
    "while i < NUMBER:\n",
    "\troot = parse_xml(ARTICLES_DIR + filenames[randint(0, len(filenames)-1)])\n",
    "\n",
    "\t# Get all the info from article\n",
    "\tfmain = get_xml_elem(root, 'formulas_main')\n",
    "\tstart = get_xml_elem(root, 'pages/start').text\n",
    "\t\n",
    "\n",
    "\t# if there's no formulas in the article try another one\n",
    "\ttotal_num = 0\n",
    "\tfor formula in fmain:\n",
    "\t\ttotal_num += 1\n",
    "\tif not total_num:\n",
    "\t\tcontinue\n",
    "\ti += 1\n",
    "\t\n",
    "\tnum = randint(0, 100) % total_num\n",
    "\n",
    "\tformula = fmain[num].text\n",
    "\n",
    "\tfile += f'{i}. Статья: {root.attrib[\"uri\"]}, Начало на стр. {start}, формула {num + 1}:\\n$${formula}$$\\n'\n",
    "\n",
    "with codecs.open(EXIT_FILE, 'w', 'utf-8') as f:\n",
    "\tf.write(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
