{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Базовый парсер заголовков\n",
    "\n",
    "Вытаскивает из latex-кода заголовки статей и их расположение в файлах.\n",
    "\n",
    "Разбивка происходит в полуручном режиме, т.к. нет уверенности в формате заголовков.\n",
    "\n",
    "В тексте ищутся слова, содержащие в своём составе заглавные буквы на русском и английском языках в отношении, большем или равным заданному (по умолчанию 0.51, при меньших значениях количество вхождений значительно возрастает, например за счёт двухбуквенных предлогов). Предполагается, что таким образом удаётся обнаруживать неправильно машинно распознанный капс. Слова или цепочки слов, состоящие из одного строчного символа включаются в заголовок, если стоят между слов, определённых как часть заголовка. При этом, одиночные заглавные буквы, а также инициалы не воспринимаются как начало заголовка.\n",
    "\n",
    "## Использование\n",
    "- При удовлетворительном определении заголовка нажать `Enter` без дополнительного ввода.\n",
    "- Если предложенное место заголовком не является ввести `\"n\"`\n",
    "- При неправильном определении границ заголовка ввести два корректировочных числа для сдвига левой и правой границы.\n",
    "  - ЗАМЕЧАНИЕ: сдвиг производится попробельно, т.е. двойной пробел будет распознан как слово нулевой длины.\n",
    "  - ЗАМЕЧАНИЕ: границы отображаемого фрагмента текста будут передвинуты автоматически. Длины левой и правой границ в словах задаются в параметрах.\n",
    "  - ПРИМЕРЫ:\n",
    "    - `out: a [B C] d e f` -> `in: 0 2` -> `out: a [B C D E] f`\n",
    "    - `out: a b c [D E] f` -> `in: 2 -1` -> `out: a [B C D] e f`\n",
    "- Также возможен посимвольный сдвиг правой границы в случае \"сращивания\" заголовка статьи и её текста. Ввести одно число, начиная с точки.\n",
    "  - ПРИМЕРЫ:\n",
    "    - `out: a[BC]def` -> `in: .2` -> `out: a[BCDE]f`\n",
    "    - `out: a[BCDE]f` -> `in: .-1` -> `out: a[BCD]ef`\n",
    "\n",
    "В выводе в терминале переносы строк для удобства заменены на `\"$\"`\n",
    "\n",
    "### Прочее\n",
    "- Для определителя капса доступны исключения, которые никогда не будут рассматриваться, как потенциальные начала заголовков, см. опции. По умолчанию: первые 10 римских цифр, \"МэВ\" и \"ГэВ\". Также определитель не реагирует на \"СМ.\", что часто встречается в ссылках сразу после заголовков.\n",
    "- Использовать системный терминал для взаимодействия оказывается удобнее, чем использовать jupyter, поэтому рекомендуется запускать файл `base_titles_parser.py` из терминала или с использованием Python Launcher (но вы всё ещё можете запустить ячейку ниже).\n",
    "- При положительном определении заголовка файл дополняется немедленно, прервать процесс можно в любой момент, как и продолжить после -- итоговый файл будет дополняться, а не перезаписываться с нуля при новом запуске программы (главное не забыть предварительно удалить из конца файла дубликаты, если вы начинаете с той страницы, на которой закончили в прошлый раз, а не со следующей).\n",
    "- В случае пропуска парсером заголовка его можно добавить вручную двумя способами:\n",
    "  1) Сдвинуть границы заголовка назад, как описано в инструкции выше. Подходит, если была пропущена небольшая (обычно ссылочная) статья, примерно 20 слов, плюс-минус. При этом после ввода заголовка поиск продолжится с __его__ конца, поэтому следующий заголовок \"вместо\" которого был введён пропущенный будет определён заново и пропущен не будет.\n",
    "  2) Воспользоваться ячейкой 1.1. Для этого в сыром tex-файле страницы нужно отыскать заголовок, скопировать его и __в точности__ вставить в разделе параметров, а также указать номер страницы. Скрипт парсера при этом можно не закрывать, последующая нумерация подстроится автоматически."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import base_titles_parser\n",
    "import importlib\n",
    "importlib.reload(base_titles_parser)\n",
    "base_titles_parser.run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1. Добавление заголовков по одному\n",
    "\n",
    "В разделе параметров указать номер страницы и ТОЧНУЮ формулировку заголовка из сырого latex-текста, а также номер страницы, после чего запустить ячейку.\n",
    "\n",
    "Закрывать скрипт парсера не обязательно, это не вызовет ошибок и его нумерация подстроится автоматически."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1.1. Добавление заголовков по одному\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "PAGES_DIR = \"./matphys/rpages/\"\n",
    "EXIT_DIR = \"./results/FMEtitles/\"\n",
    "EXIT_FILE = \"FMEtitles-added-manually.xml\"\n",
    "# Search parameters\n",
    "PAGE = 146\n",
    "TITLE = 'глюоний'\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "class Article:\n",
    "    start_title = 0\n",
    "    end_title = 0\n",
    "    filename = ''\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames_raw = get_filenames(PAGES_DIR)\n",
    "filenames = []\n",
    "for i in range(PAGE, PAGE + 1):\n",
    "    for filename in filenames_raw:\n",
    "        beginning = \"rp-\" + str(i) + \"_\"\n",
    "        if filename[:len(beginning)] == beginning and filename[-4:] == \".mmd\":\n",
    "            filenames.append(filename)\n",
    "\n",
    "\n",
    "# Check for existing xml\n",
    "filenames_raw = get_filenames(EXIT_DIR)\n",
    "if not(EXIT_FILE in filenames_raw):\n",
    "    root = ElementTree.Element('data')\n",
    "    xml_write(root, EXIT_DIR + EXIT_FILE)\n",
    "\n",
    "\n",
    "root = parse_xml(EXIT_DIR + EXIT_FILE)\n",
    "\n",
    "\n",
    "# Add article title and metadata to xml tree\n",
    "def add_article(article_local:Article, etree_root:ElementTree.Element, number:int):\n",
    "    elem_article = ElementTree.SubElement(etree_root, 'article', {'n':str(number)})\n",
    "    elem_title = ElementTree.SubElement(elem_article, 'title')\n",
    "    elem_title.text = file[article_local.start_title + 1:article_local.end_title]\n",
    "    elem_title_meta = ElementTree.SubElement(elem_article, 'title-meta')\n",
    "    elem_title_file = ElementTree.SubElement(elem_title_meta, 'title-file')\n",
    "    elem_title_file.text = article_local.filename\n",
    "    elem_title_start = ElementTree.SubElement(elem_title_meta, 'title-start')\n",
    "    elem_title_start.text = str(article_local.start_title + 1)\n",
    "    elem_title_end = ElementTree.SubElement(elem_title_meta, 'title-end')\n",
    "    elem_title_end.text = str(article_local.end_title)\n",
    "    xml_write(etree_root, EXIT_DIR + EXIT_FILE)\n",
    "\n",
    "# Read requested file\n",
    "with codecs.open(PAGES_DIR + filenames[0], 'r', 'utf-8') as f:\n",
    "    file = f.read()\n",
    "\n",
    "# Find titles and add them\n",
    "start_title = 0\n",
    "end_title = 0\n",
    "num = len(root) + 1\n",
    "while file.find(TITLE, end_title) != -1:\n",
    "    start_title = file.find(TITLE, start_title)\n",
    "    end_title = start_title + len(TITLE)\n",
    "    start_title -= 1 # Set on space before the title\n",
    "\n",
    "    article = Article()\n",
    "    article.start_title = max(start_title, 0)\n",
    "    article.end_title = min(end_title, len(file))\n",
    "    article.filename = filenames[0]\n",
    "    add_article(article, root, num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Исправление ошибок в заголовках\n",
    "\n",
    "Состоит из двух частей: \"составитель пар\" и \"подстановщик\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1. Составитель пар \"оригинальный - исправленный\" для заголовков\n",
    "\n",
    "Формирует xml-список всех заголовков с возможными автоматическими исправлениями (в формате было / стало):\n",
    "1. замена латиницы на аналогичную кириллицу;\n",
    "2. замена заданных буквосочетаний (см. параметры)\n",
    "3. удаление обрамляющих знаков препинания;\n",
    "4. замена всех букв на заглавные (в том числе это избавляет дальнейшей необходимости исправлять имена);\n",
    "5. слияние разорванных на отдельные буквы слов (если рядом оказываются несколько таких слов, то они оказываются слиты вместе).\n",
    "\n",
    "Этот список необходимо просмотреть и исправить оставшиеся ошибки.\n",
    "\n",
    "Дополнительно, для помощи в поиске орфографических ошибок, формируется строка с изменениями, предложенными спеллчекером. ВНИМАНИЕ: спеллчекер может делать ошибки в именах, специфических терминах и т.п., поэтому следует использовать его результаты лишь для ориентира."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2.1. Составитель пар \"оригинальный - исправленный\" для заголовков:\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "WORK_DIR = \"./matphys/\"\n",
    "INPUT_FILE = \"FMEv2.xml\"\n",
    "CORRECTION_FILE = \"FMEcorr.xml\"\n",
    "COMBINATIONS_CORR = dict_merge(COMBINATIONS_CORR_GLOBAL, {\n",
    "    'ХК' : 'Ж',\n",
    "    'ЬI' : 'Ы',\n",
    "    'II' : 'Ш',\n",
    "    'I' : 'П',\n",
    "    'J' : 'Л',\n",
    "    'ЛАГРАНХ' : 'ЛАГРАНЖ',\n",
    "    'ЛАТРАНХ' : 'ЛАГРАНЖ',\n",
    "})\n",
    "SPELLCHECK_ONLY = True # Use if the only thing you need from this script is spellcheck\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Check for existing xml\n",
    "filenames_raw = get_filenames(WORK_DIR)\n",
    "if not(INPUT_FILE in filenames_raw):\n",
    "    root = ElementTree.Element('data')\n",
    "    xml_write(root, WORK_DIR + CORRECTION_FILE)\n",
    "\n",
    "\n",
    "root = parse_xml(WORK_DIR + INPUT_FILE)\n",
    "\n",
    "\n",
    "# Get all the titles into a dict\n",
    "titles_dict = {}\n",
    "pages_dict = {}\n",
    "for article in root:\n",
    "    title = get_xml_elem(article, 'title').text\n",
    "    titles_dict[title] = (title, title)\n",
    "    title_file = get_xml_elem(article, 'title-meta/title-file')\n",
    "    pages_dict[title] = title_file.text[title_file.text.find('-')+1:title_file.text.find('_')]\n",
    "\n",
    "\n",
    "if not SPELLCHECK_ONLY:\n",
    "    # Correct preferred combinations and latin letters\n",
    "    for title in titles_dict.keys():\n",
    "        title_new = title_handle_latin(titles_dict[title][0], COMBINATIONS_CORR)\n",
    "        titles_dict[title] = (title_new, title_new)\n",
    "\n",
    "    # Remove bounding symbols\n",
    "    for title in titles_dict.keys():\n",
    "        title_new = title_handle_bounding(titles_dict[title][0])\n",
    "        titles_dict[title] = (title_new, title_new)\n",
    "\n",
    "    # CAPS\n",
    "    for title in titles_dict.keys():\n",
    "        title_new = titles_dict[title][0].upper()\n",
    "        titles_dict[title] = (title_new, title_new)\n",
    "\n",
    "    # Merge single-lettered words\n",
    "    for title in titles_dict.keys():\n",
    "        title_new = title_handle_merge(titles_dict[title][0])\n",
    "        titles_dict[title] = (title_new, title_new)\n",
    "\n",
    "    # Revert changes for aux formulas in titles\n",
    "    for title in titles_dict.keys():\n",
    "        title_new = title_handle_formulas(titles_dict[title][0], title)\n",
    "        titles_dict[title] = (title_new, title_new)\n",
    "\n",
    "# Try spellcheck on titles\n",
    "spellcheck_dict_update()\n",
    "for title in titles_dict.keys():\n",
    "    title_new = titles_dict[title][0]\n",
    "    title_suggestions = do_spellcheck(title_new)\n",
    "    for i in range(len(title_new)):\n",
    "        title_new = title_new[:i] + ('_' if title_new[i] not in [' ', '\\n', '\\r'] else title_new[i]) + (title_new[i+1:] if i + 1 <= len(title_new) else '')\n",
    "    for pos in sorted(title_suggestions.keys(), reverse=True):\n",
    "        title_new = title_new[:pos] + title_suggestions[pos][1] + title_new[pos+len(title_suggestions[pos][0]):]\n",
    "    titles_dict[title] = (titles_dict[title][0], title_new)\n",
    "\n",
    "\n",
    "# Write corrections xml\n",
    "root = ElementTree.Element('data')\n",
    "for i in titles_dict.items():\n",
    "    pair = ElementTree.SubElement(root, 'pair')\n",
    "    title_old = ElementTree.SubElement(pair, 'title_old')\n",
    "    title_old.text = i[0]\n",
    "    title_new = ElementTree.SubElement(pair, 'title_new')\n",
    "    title_new.text = i[1][0]\n",
    "    title_new = ElementTree.SubElement(pair, 'title__sc')\n",
    "    title_new.text = i[1][1]\n",
    "    page = ElementTree.SubElement(pair, 'page')\n",
    "    page.text = pages_dict[i[0]]\n",
    "xml_write(root, WORK_DIR + CORRECTION_FILE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2. Подстановщик исправленных заголовков\n",
    "\n",
    "Заменяет все заголовки на исправленные согласно списку пар."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2.2. Подстановщик исправленных заголовков:\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "WORK_DIR = \"./matphys/\"\n",
    "INPUT_FILE = \"FMEv2.xml\"\n",
    "CORRECTION_FILE = \"FMEcorr.xml\"\n",
    "EXIT_FILE = \"FMEtitles.xml\"\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "root = parse_xml(WORK_DIR + CORRECTION_FILE)\n",
    "\n",
    "\n",
    "# Get all the corrections into a dict\n",
    "titles_dict = {}\n",
    "for pair in root:\n",
    "    titles_dict[get_xml_elem(pair, 'title_old').text] = get_xml_elem(pair, 'title_new').text\n",
    "\n",
    "\n",
    "root = parse_xml(WORK_DIR + INPUT_FILE)\n",
    "\n",
    "\n",
    "# Replace titles\n",
    "for article in root:\n",
    "    get_xml_elem(article, 'title').text = titles_dict[get_xml_elem(article, 'title').text]\n",
    "xml_write(root, WORK_DIR + EXIT_FILE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Сортировщик / сливщик файлов с заголовками\n",
    "\n",
    "Сортирует статьи в файлах из основного списка в порядке страница-расположение, т.е. (если не сказано иного) в алфавитном порядке и выводит в один выходной файл. Также порядковый номер заменяется uri формата \"http://libmeta.ru/fme/article/1_Kraevaya\". (Созданные uri кешируются по номеру страницы и позиции заголовка в тексте и при последующих запусках остаются неизменными, если включен `URI_SAFER`).\n",
    "\n",
    "Также в конец выходного файл добавляются заголовки из \"ручного\" файла, в том же формате, но без сортировки, что позволяет добавлять случайно забытые статьи без изменения uri и имён файлов всех остальных статей."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing main input files...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3ae9842afb946e98b1417069429ed00"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing \"manual\" file...\n",
      "Writing main articles...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3584 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92a2b8b9bbc645658ffd1fc0545aa939"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f135d0c2eff46799773c989aea8b4eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Сортировщик / сливщик файлов с заголовками\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "WORK_DIR = \"./results/\"\n",
    "TITLES_DIR = \"FMEtitles/\"\n",
    "INPUT_FILES = [\"FMEtitles-p5-100.xml\", \"FMEtitles-p101-200.xml\", \"FMEtitles-p201-300.xml\", \"FMEtitles-p301-400.xml\", \"FMEtitles-p301-400-add.xml\",\n",
    "               \"FMEtitles-p401-500.xml\", \"FMEtitles-p501-600.xml\", \"FMEtitles-p601-692.xml\", \"FMEtitles-p601-692-add.xml\"]\n",
    "MANUALLY_ADDED_FILE = \"FMEtitles-added-manually.xml\"\n",
    "URI_CACHE = \"FMEtitles-uri-cache.xml\"\n",
    "EXIT_FILE = \"FMEtitles-merged.xml\"\n",
    "# Uri safer prevents already existing uri from being changed. Set to False ONLY IF you need to update an existing uris.\n",
    "URI_SAFER = True\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "class Article:\n",
    "    title = ''\n",
    "    start_title = ''\n",
    "    end_title = ''\n",
    "    filename = ''\n",
    "\n",
    "\n",
    "\n",
    "# Try to get uri from the cache for title with given page and pos\n",
    "def get_uri(title_page:str, title_pos:str) -> str:\n",
    "    global cache_root\n",
    "    for elem_uri in cache_root:\n",
    "        if elem_uri.tag == 'uri' and elem_uri.attrib['page'] == title_page and elem_uri.attrib['pos'] == title_pos:\n",
    "            return elem_uri.text\n",
    "    return ''\n",
    "# Cache given uri\n",
    "def cache_uri(title_page:str, title_pos:str, uri_str:str):\n",
    "    global cache_root\n",
    "    elem_uri = ElementTree.SubElement(cache_root, 'uri', {'page':title_page, 'pos':title_pos})\n",
    "    elem_uri.text = uri_str\n",
    "    xml_write(cache_root, WORK_DIR + TITLES_DIR + URI_CACHE)\n",
    "\n",
    "\n",
    "# Add article title and metadata to xml tree\n",
    "def add_article(article_local:Article, etree_root:ElementTree.Element, number:int):\n",
    "    page_str = article_local.filename[article_local.filename.find('-') + 1: article_local.filename.find('_')]\n",
    "    uri_cached = get_uri(page_str, article_local.start_title)\n",
    "    translitted = translit(article_local.title[:article_local.title.find(' ')], 'ru', True)\n",
    "    while translitted.find('/') != -1:\n",
    "        translitted = translitted[:translitted.find('/')] + '_' + translitted[translitted.find('/')+1:]\t\t# Prevent slash being counted as subfolder in further\n",
    "    uri_str = URI_PREFIX + \"article/\" + str(number) + \"_\" + translitted\n",
    "    if URI_SAFER and uri_cached != '':\n",
    "        uri_str = uri_cached\n",
    "    else:\n",
    "        cache_uri(page_str, article_local.start_title, uri_str)\n",
    "    elem_article = ElementTree.SubElement(etree_root, 'article', {'uri':uri_str})\n",
    "    elem_title = ElementTree.SubElement(elem_article, 'title')\n",
    "    elem_title.text = article_local.title\n",
    "    elem_title_meta = ElementTree.SubElement(elem_article, 'title-meta')\n",
    "    elem_title_file = ElementTree.SubElement(elem_title_meta, 'title-file')\n",
    "    elem_title_file.text = article_local.filename\n",
    "    elem_title_start = ElementTree.SubElement(elem_title_meta, 'title-start')\n",
    "    elem_title_start.text = str(int(article_local.start_title) + 1)\n",
    "    elem_title_end = ElementTree.SubElement(elem_title_meta, 'title-end')\n",
    "    elem_title_end.text = article_local.end_title\n",
    "\n",
    "\n",
    "# Check for existing uri list\n",
    "filenames_raw = get_filenames(WORK_DIR + TITLES_DIR)\n",
    "if not(URI_CACHE in filenames_raw):\n",
    "    root = ElementTree.Element('data')\n",
    "    xml_write(root, WORK_DIR + TITLES_DIR + URI_CACHE)\n",
    "cache_root = parse_xml(WORK_DIR + TITLES_DIR + URI_CACHE)\n",
    "\n",
    "\n",
    "# Collect all the articles\n",
    "print(\"Parsing main input files...\")\n",
    "articles_dict = {}\n",
    "for filename in tqdm(INPUT_FILES):\n",
    "    root = parse_xml(WORK_DIR + TITLES_DIR + filename)\n",
    "    for article in root:\n",
    "        title = get_xml_elem(article, 'title').text\n",
    "        elem = get_xml_elem(article, 'title-meta/title-file')\n",
    "        page = elem.text[elem.text.find('-')+1:elem.text.find('_')]\n",
    "        pos = get_xml_elem(article, 'title-meta/title-start').text\n",
    "        start = get_xml_elem(article, 'title-meta/title-start').text\n",
    "        end = get_xml_elem(article, 'title-meta/title-end').text\n",
    "        file = get_xml_elem(article, 'title-meta/title-file').text\n",
    "        num = (int(page), int(pos))\n",
    "        articles_dict[num] = {'title':title, 'file':file, 'start':start, 'end':end}\n",
    "\n",
    "# Same for manually added articles\n",
    "print(\"Parsing \\\"manual\\\" file...\")\n",
    "articles_dict_man = {}\n",
    "nums_list_man = []\n",
    "root = parse_xml(WORK_DIR + TITLES_DIR + MANUALLY_ADDED_FILE)\n",
    "for article in root:\n",
    "    title = get_xml_elem(article, 'title').text\n",
    "    elem = get_xml_elem(article, 'title-meta/title-file')\n",
    "    page = elem.text[elem.text.find('-')+1:elem.text.find('_')]\n",
    "    pos = get_xml_elem(article, 'title-meta/title-start').text\n",
    "    start = get_xml_elem(article, 'title-meta/title-start').text\n",
    "    end = get_xml_elem(article, 'title-meta/title-end').text\n",
    "    file = get_xml_elem(article, 'title-meta/title-file').text\n",
    "    num = (int(page), int(pos))\n",
    "    articles_dict_man[num] = {'title':title, 'file':file, 'start':start, 'end':end}\n",
    "    nums_list_man.append(num)\n",
    "\n",
    "\n",
    "# Sort keys and write articles accordingly\n",
    "root = ElementTree.Element('data')\n",
    "nums_list = sorted(list(i for i in articles_dict.keys()))\n",
    "print(\"Writing main articles...\")\n",
    "for num in tqdm(range(len(nums_list))):\n",
    "    article = Article()\n",
    "    article.title = articles_dict[nums_list[num]]['title']\n",
    "    article.start_title = articles_dict[nums_list[num]]['start']\n",
    "    article.end_title = articles_dict[nums_list[num]]['end']\n",
    "    article.filename = articles_dict[nums_list[num]]['file']\n",
    "    add_article(article, root, num + 1)\n",
    "for num in tqdm(range(len(nums_list_man))):\n",
    "    article = Article()\n",
    "    article.title = articles_dict_man[nums_list_man[num]]['title']\n",
    "    article.start_title = articles_dict_man[nums_list_man[num]]['start']\n",
    "    article.end_title = articles_dict_man[nums_list_man[num]]['end']\n",
    "    article.filename = articles_dict_man[nums_list_man[num]]['file']\n",
    "    add_article(article, root, num + 1 + len(nums_list))\n",
    "xml_write(root, WORK_DIR + EXIT_FILE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Парсер текстов статей\n",
    "\n",
    "По информации из указанного файла с заголовками вытаскивает в сыром виде тексты статей. Каждая статья помещается в свой .xml файл, с именем, содержащим номер статьи и первое слово из заголовка транслитом."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting articles info...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91c3499b602540c8bb0da232463a87bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing articles...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ee79d6bcb614b7aa32babbfe6ca6d82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Парсер текстов статей\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "TITLES_FILE = \"./results/FMEtitles-merged.xml\"\n",
    "PAGES_DIR = \"./matphys/rpages/\"\n",
    "EXIT_DIR = \"./results/FMEarticles/\"\n",
    "COMBINATIONS_CORR = {\n",
    "    'І' : 'I'\t\t# This teo are different!\n",
    "}\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Article:\n",
    "    start_file = ''\n",
    "    start_pos = 0\n",
    "    end_file = ''\n",
    "    end_pos = 0\n",
    "    text = ''\n",
    "    text_orig = ''\n",
    "    uri = ''\n",
    "    num = ''\n",
    "    title = ''\n",
    "    xml = ''\n",
    "\n",
    "    def get_text(self):\n",
    "        # Get filenames\n",
    "        filenames_raw_local = get_filenames(PAGES_DIR)\n",
    "        filenames_local = []\n",
    "        for filename_local in filenames_raw_local:\n",
    "            if filename_local[-4:] == \".mmd\":\n",
    "                filenames_local.append(filename_local)\n",
    "        if self.start_file == self.end_file:\n",
    "            with codecs.open(PAGES_DIR + self.start_file, 'r', 'utf-8') as f_in:\n",
    "                self.text += f_in.read()[self.start_pos:self.end_pos]\n",
    "        else:\n",
    "            with codecs.open(PAGES_DIR + self.start_file, 'r', 'utf-8') as f_in:\n",
    "                self.text += f_in.read()[self.start_pos:]\n",
    "            for page_local in range(int(self.start_file[3:self.start_file.find('_')]) + 1, int(self.end_file[3:self.end_file.find('_')])):\n",
    "                for filename_local in filenames_local:\n",
    "                    if int(filename_local[3:filename_local.find('_')]) == page_local:\n",
    "                        self.text += ' ' # Add a space to prevent word merging\n",
    "                        with codecs.open(PAGES_DIR + filename_local, 'r', 'utf-8') as f_in:\n",
    "                            self.text += f_in.read()\n",
    "            self.text += ' ' # Add a space to prevent word merging\n",
    "            with codecs.open(PAGES_DIR + self.end_file, 'r', 'utf-8') as f_in:\n",
    "                self.text += f_in.read()[:self.end_pos]\n",
    "        for comb_local in COMBINATIONS_CORR.keys():\n",
    "            while self.text.find(comb_local) != -1:\n",
    "                self.text = self.text[:self.text.find(comb_local)] + COMBINATIONS_CORR[comb_local] + self.text[self.text.find(comb_local) + len(comb_local):]\n",
    "        while self.text is not None and len(self.text) and self.text[0] in [' ', ',', '.', ':', ';', '-', '\\n', '\\r']:\n",
    "            self.text = self.text[1:]\n",
    "        while self.text is not None and len(self.text) and self.text[-1] in [' ', '\\n', '\\r']:\n",
    "            self.text = self.text[:-1]\n",
    "        self.text_orig = self.text\n",
    "        # Fix several capital symbols per word\n",
    "        word_left = 0\n",
    "        while word_left < len(self.text):\n",
    "            word_right = min(len(self.text), self.text.find(' ', word_left) if self.text.find(' ', word_left) != -1 else len(self.text))\n",
    "            word_right = min(word_right, self.text.find('\\n', word_left) if self.text.find('\\n', word_left) != -1 else len(self.text))\n",
    "            word_right = min(word_right, self.text.find('\\r', word_left) if self.text.find('\\r', word_left) != -1 else len(self.text))\n",
    "            word_right = min(word_right, self.text.find('-', word_left) if self.text.find('-', word_left) != -1 else len(self.text))\n",
    "            word_right = min(word_right, self.text.find('.', word_left) if self.text.find('.', word_left) != -1 else len(self.text))\n",
    "            word_str = self.text[word_left:word_right]\n",
    "            if word_str is not None and len(word_str) > 1 and not check_in_uri(self.text, word_left) and not check_in_formula(self.text, word_left) and not check_in_link(self.text, word_left):\n",
    "                word_str = word_str[0] + word_str[1:len(word_str)].lower()\n",
    "                self.text = self.text[:word_left] + word_str + self.text[word_right:]\n",
    "            word_left = word_right + 1\n",
    "\n",
    "    def make_xml(self):\n",
    "        self.get_text()\n",
    "\n",
    "        elem_article = ElementTree.Element(\"article\", {'uri':self.uri, 'alphabetic_pos':self.num})\n",
    "        elem_title = ElementTree.SubElement(elem_article, 'title')\n",
    "        elem_title.text = self.title\n",
    "        elem_author = ElementTree.SubElement(elem_article, 'authors')\n",
    "        elem_author.text = None\n",
    "        elem_title_short = ElementTree.SubElement(elem_article, 'title_short')\n",
    "        elem_title_short.text = None\n",
    "        elem_pages = ElementTree.SubElement(elem_article, 'pages')\n",
    "        elem_start = ElementTree.SubElement(elem_pages, 'start')\n",
    "        elem_start.text = self.start_file[3:self.start_file.find('_', 3)]\n",
    "        elem_end = ElementTree.SubElement(elem_pages, 'end')\n",
    "        elem_end.text = self.end_file[3:self.end_file.find('_', 3)]\n",
    "        elem_literature = ElementTree.SubElement(elem_article, 'literature')\n",
    "        elem_literature_orig = ElementTree.SubElement(elem_literature, 'literature_orig')\n",
    "        elem_literature_orig.text = None\n",
    "        elem_formulas_remote = ElementTree.SubElement(elem_article, 'formulas_main')\n",
    "        elem_formulas_remote.text = None\n",
    "        elem_formulas_inline = ElementTree.SubElement(elem_article, 'formulas_aux')\n",
    "        elem_formulas_inline.text = None\n",
    "        elem_relations = ElementTree.SubElement(elem_article, 'relations')\n",
    "        elem_relations.text = None\n",
    "        elem_text = ElementTree.SubElement(elem_article, 'text')\n",
    "        elem_text.text = self.text\n",
    "        elem_text_orig = ElementTree.SubElement(elem_article, 'text_orig')\n",
    "        elem_text_orig.text = self.text_orig\n",
    "\n",
    "        self.xml = prettify(elem_article)\n",
    "\n",
    "\n",
    "\n",
    "class Title:\n",
    "    text = ''\n",
    "    file = ''\n",
    "    start_pos = 0\n",
    "    end_pos = 0\n",
    "    uri = ''\n",
    "\n",
    "\n",
    "def get_titles_dict(etree_root:ElementTree.Element) -> dict:\n",
    "    titles_dict_local = {}\n",
    "    for elem_title in etree_root:\n",
    "        elem_uri = elem_title.attrib['uri']\n",
    "        elem_text = get_xml_elem(elem_title, 'title').text\n",
    "        elem_file = get_xml_elem(elem_title, 'title-meta/title-file').text\n",
    "        elem_page = int(elem_file[elem_file.find('-') + 1 : elem_file.find('_')])\n",
    "        elem_start_pos = int(get_xml_elem(elem_title, 'title-meta/title-start').text)\n",
    "        elem_end_pos = int(get_xml_elem(elem_title, 'title-meta/title-end').text)\n",
    "        titles_dict_local[(elem_page, elem_start_pos)] = Title()\n",
    "        titles_dict_local[(elem_page, elem_start_pos)].uri = elem_uri\n",
    "        titles_dict_local[(elem_page, elem_start_pos)].text = elem_text\n",
    "        titles_dict_local[(elem_page, elem_start_pos)].file = elem_file\n",
    "        titles_dict_local[(elem_page, elem_start_pos)].start_pos = elem_start_pos\n",
    "        titles_dict_local[(elem_page, elem_start_pos)].end_pos = elem_end_pos\n",
    "    return titles_dict_local\n",
    "\n",
    "\n",
    "def get_title(number:int, dict_with_titles:dict) -> Title:\n",
    "    out_title = Title()\n",
    "    titles_dict_keys = sorted(dict_with_titles.keys())\n",
    "    for p in range(len(titles_dict_keys)):\n",
    "        if p == number:\n",
    "            out_title = dict_with_titles[titles_dict_keys[p]]\n",
    "    return out_title\n",
    "\n",
    "\n",
    "root = parse_xml(TITLES_FILE)\n",
    "\n",
    "# Create articles list\n",
    "articles_list = []\n",
    "title = Title()\n",
    "titles_dict = get_titles_dict(root)\n",
    "print(\"Getting articles info...\")\n",
    "for i in tqdm(range(len(root))):\n",
    "    title = get_title(i, titles_dict)\n",
    "    if i:\n",
    "        articles_list[-1].end_file = title.file\n",
    "        articles_list[-1].end_pos = max(title.start_pos - 2, 0) # There is a shift for some reason\n",
    "    articles_list.append(Article())\n",
    "    articles_list[-1].uri = title.uri\n",
    "    articles_list[-1].num = str(i + 1)\n",
    "    articles_list[-1].title = title.text\n",
    "    articles_list[-1].start_file = title.file\n",
    "    articles_list[-1].start_pos = title.end_pos\n",
    "    articles_list[-1].end_file = title.file\n",
    "    with codecs.open(PAGES_DIR + title.file, 'r', 'utf-8') as f:\n",
    "        articles_list[-1].end_pos = len(f.read())\n",
    "\n",
    "# Parse texts themselves and write\n",
    "print(\"Parsing articles...\")\n",
    "for i in tqdm(range(len(articles_list))):\n",
    "    articles_list[i].make_xml()\n",
    "    with codecs.open(EXIT_DIR + '' + articles_list[i].uri[len(URI_PREFIX) + 8:] + '.xml', 'w', 'utf-8') as f:\n",
    "        f.write(articles_list[i].xml)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Проверка правописания в текстах\n",
    "\n",
    "## 5.1. Сканер\n",
    "\n",
    "Сканирует тексты из указанного диапазона статей и выносит все показавшиеся подозрительными слова в отдельный xml следующего формата:\n",
    "- Статья (имя файла в аттрибутах)\n",
    "  - Слово (позиция в тексте и флаги в аттрибутах)\n",
    "    - Исходный вариант\n",
    "    - Контекстная строка (размер задаётся в разделе параметров скрипта)\n",
    "    - Предложенная замена\n",
    "\n",
    "Предлагается два флага для определения дальнейшей \"судьбы\" слова: \"результат\" (0 -- исходное, 1 -- предложенное) и \"добавление в словарь\" (0 -- не добавлять, 1 -- добавить как есть, 2 -- перевести в нижний регистр и добавить (для первого слова в предложении), 3 -- сделать первую букву заглавной и добавить (для имён, случайно распознанных без заглавной); применяется к выбранному результату)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46e11ab5db3b4802a68155d6ff6718aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases found: 55\n"
     ]
    }
   ],
   "source": [
    "# 5.1. Проверка правописания в текстах. Сканер.\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "EXIT_DIR = \"./matphys/\"\n",
    "CONTEXT_SIZE = 20\n",
    "START_ARTICLE = 3551\n",
    "END_ARTICLE = 3586\n",
    "# Flags for usual cases\n",
    "DEFAULT_RESULT_FLAG = '1'\n",
    "DEFAULT_ADD_TO_PWL_FLAG = '0'\n",
    "# Flags is name is detected\n",
    "\"\"\"NAME_RESULT_FLAG = '0'\n",
    "NAME_ADD_TO_PWL_FLAG = '1'\"\"\"\n",
    "# Cases that have to be overriden\n",
    "OVERRIDE_FORCE_CYRILLIC = {\n",
    "    'Ссср' : 'СССР',\n",
    "    'Усср' : 'УССР',\n",
    "    'Церн' : 'ЦЕРН'\n",
    "}\n",
    "OVERRIDE_AS_IS = {\n",
    "}\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "spellcheck_dict_update()\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "#filenames = ['4_ABELEVA.xml']\n",
    "\n",
    "root = ElementTree.Element('data')\n",
    "\n",
    "total_wois = 0\n",
    "for filename in tqdm(filenames):\n",
    "    article_number = int(filename[:filename.find('_')])\n",
    "    if article_number < START_ARTICLE or article_number > END_ARTICLE:\n",
    "        continue\n",
    "\n",
    "    #print(f'{filename}: found ', end='')\n",
    "    article = parse_xml(ARTICLES_DIR + filename)\n",
    "    text = get_xml_elem(article, 'text')\n",
    "    if text.text is None:\n",
    "        continue\n",
    "    elif not len(text.text):\n",
    "        continue\n",
    "\n",
    "    #add_to_pwl(filename[filename.find('_')+1:filename.find('.xml')])\n",
    "\n",
    "    text_suggestions = do_spellcheck(text.text)\n",
    "    #print(len(text_suggestions.keys()))\n",
    "    total_wois += len(text_suggestions.keys())\n",
    "    if len(text_suggestions.keys()):\n",
    "        article = ElementTree.SubElement(root, 'article', {'filename': filename})\n",
    "        for pos in text_suggestions.keys():\n",
    "            #print(f'{pos}: {text_suggestions[pos][0]} -> {text_suggestions[pos][1]}')\n",
    "            local_result_flag = DEFAULT_RESULT_FLAG\n",
    "            local_add_to_pwl_flag = DEFAULT_ADD_TO_PWL_FLAG\n",
    "            # Process possible name case\n",
    "            '''if len(text_suggestions[pos][0]) >= 2 and len(text_suggestions[pos][1]) >= 2:\n",
    "                is_name_orig = re.match(r\"[А-ЯA-Z]\", text_suggestions[pos][0][0]) is not None and re.match(r\"[а-яa-z]\", text_suggestions[pos][0][1]) is not None\n",
    "                is_name_sugg = re.match(r\"[А-ЯA-Z]\", text_suggestions[pos][1][0]) is not None and re.match(r\"[а-яa-z]\", text_suggestions[pos][1][1]) is not None\n",
    "                if is_name_orig and is_name_sugg:\n",
    "                    local_result_flag = NAME_RESULT_FLAG\n",
    "                    local_add_to_pwl_flag = NAME_ADD_TO_PWL_FLAG'''\n",
    "            # Override specific cases\n",
    "            suggestion_text = text_suggestions[pos][1]\n",
    "            if title_handle_latin(text_suggestions[pos][0], COMBINATIONS_CORR_GLOBAL) in OVERRIDE_FORCE_CYRILLIC.keys():\n",
    "                suggestion_text = OVERRIDE_FORCE_CYRILLIC[title_handle_latin(text_suggestions[pos][0], COMBINATIONS_CORR_GLOBAL)]\n",
    "                local_result_flag = DEFAULT_RESULT_FLAG\n",
    "                local_add_to_pwl_flag = DEFAULT_ADD_TO_PWL_FLAG\n",
    "            if text_suggestions[pos][0] in OVERRIDE_AS_IS.keys():\n",
    "                suggestion_text = OVERRIDE_AS_IS[text_suggestions[pos][0]]\n",
    "                local_result_flag = DEFAULT_RESULT_FLAG\n",
    "                local_add_to_pwl_flag = DEFAULT_ADD_TO_PWL_FLAG\n",
    "            word = ElementTree.SubElement(article, 'word', {'pos': str(pos), 'result': local_result_flag, 'add_to_pwl': local_add_to_pwl_flag})\n",
    "            source = ElementTree.SubElement(word, 'source')\n",
    "            source.text = text_suggestions[pos][0]\n",
    "            context = ElementTree.SubElement(word, 'context')\n",
    "            context_string = text.text[max(0, pos - CONTEXT_SIZE):min(len(text.text), pos + len(text_suggestions[pos][0]) + CONTEXT_SIZE)]\n",
    "            while context_string.find('\\n') != -1:\n",
    "                context_string = context_string[:context_string.find('\\n')] + '\\\\n' + context_string[context_string.find('\\n')+1:]\n",
    "            while context_string.find('\\r') != -1:\n",
    "                context_string = context_string[:context_string.find('\\r')] + '\\\\r' + context_string[context_string.find('\\r')+1:]\n",
    "            context.text = context_string\n",
    "            suggestion = ElementTree.SubElement(word, 'suggestion')\n",
    "            suggestion.text = suggestion_text\n",
    "\n",
    "print(\"Cases found:\", total_wois)\n",
    "\n",
    "\n",
    "with codecs.open(EXIT_DIR + f'FMEspellcheck-a{START_ARTICLE}-{END_ARTICLE}.xml', 'w', 'utf-8') as f:\n",
    "    f.write(prettify(root))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2. Пополнение словаря\n",
    "\n",
    "Добавляет отмеченные флагом \"добавление в словарь\" слова из всех файлов в директории спеллчека\n",
    "- Учитывается, было ли выбрано оригинальное слово или исправленное флагом \"результат\".\n",
    "- Словарь сортируется по алфавиту при каждом запуске.\n",
    "- Дубликаты удаляются при каждом запуске (символы разного регистра одинаковыми не считаются).\n",
    "- Слова добавленные вручную при запуске не удаляются.\n",
    "\n",
    "Чтобы объединить ваш словарь с другим, скопируйте и вставьте всё содержимое нового словаря в ваш, после чего запустите скрипт. Дубликаты будут удалены, итоговый словарь будет отсортирован."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for PWL additions...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/69 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87234d241b594af4adec99c466ea0ec5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PWL...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8406 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d982f91d521459890c5b6fdc37d20ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing PWL...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4508 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e78b653d5a1458ea56945122ee12fb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5.2. Проверка правописания в текстах. Пополнение словаря.\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "SPELLCHECK_DIR = \"./results/FMEspellcheck/\"\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Read PWL and form word list\n",
    "with codecs.open(PERSONAL_WORD_LIST, 'r', 'utf-8') as f:\n",
    "    PWL_text = f.read()\n",
    "additions = [i.strip() for i in PWL_text.split('\\n')]\n",
    "while '' in additions:\n",
    "    additions.remove('')\n",
    "PWL_text = ''\n",
    "\n",
    "# Read all spellcheck outputs and create additions list\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(SPELLCHECK_DIR)\n",
    "\n",
    "print(\"Scanning for PWL additions...\")\n",
    "for filename in tqdm(filenames):\n",
    "    root = parse_xml(SPELLCHECK_DIR + filename)\n",
    "    for article in root:\n",
    "        if article.tag == \"article\":\n",
    "            for word in article:\n",
    "                if word.tag == \"word\" and word.attrib[\"add_to_pwl\"] != '0':\n",
    "                    word_text = get_xml_elem(word, 'suggestion').text.strip() if word.attrib[\"result\"] == '1' else get_xml_elem(word, 'source').text.strip()\n",
    "                    # Check (and correct) that the word has no latin and cyrillic letters at the same time\n",
    "                    if word_text is not None and len(word_text):\n",
    "                        exist_from_comb = False\n",
    "                        exist_rus = False\n",
    "                        for i in range(len(word_text)):\n",
    "                            exist_from_comb = True if word_text[i] in COMBINATIONS_CORR_GLOBAL.keys() else exist_from_comb\n",
    "                            exist_rus = True if re.match(r\"[А-Яа-я]\", word_text[i]) is not None else exist_rus\n",
    "                        if exist_from_comb and exist_rus:\n",
    "                            for i in range(len(word_text)):\n",
    "                                word_text = word_text[:i] + (COMBINATIONS_CORR_GLOBAL[word_text[i]] if word_text[i] in COMBINATIONS_CORR_GLOBAL.keys() else word_text[i]) + (word_text[i+1:] if (i + 1) <= len(word_text) else '')\n",
    "                    if word.attrib[\"add_to_pwl\"] == '2':\n",
    "                        additions.append(word_text.lower())\n",
    "                    elif word.attrib[\"add_to_pwl\"] == '3' and len(word_text):\n",
    "                        additions.append(word_text[0].upper() + word_text[1:] if len(word_text) > 1 else '')\n",
    "                    else:\n",
    "                        additions.append(word_text)\n",
    "\n",
    "# Make new PWL list and sort it\n",
    "print(\"Processing PWL...\")\n",
    "PWL_list_new = []\n",
    "for word in tqdm(additions):\n",
    "    # Append word to the list if not present yet\n",
    "    if word is not None and len(word) and not word in PWL_list_new:\n",
    "        PWL_list_new.append(word)\n",
    "PWL_list_new.sort()\n",
    "\n",
    "# Write PWL\n",
    "print(\"Writing PWL...\")\n",
    "for word in tqdm(PWL_list_new):\n",
    "    PWL_text = PWL_text + word + '\\n'\n",
    "with codecs.open(PERSONAL_WORD_LIST, 'w', 'utf-8') as f:\n",
    "    f.write(PWL_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3. Подстановка исправленной орфографии\n",
    "\n",
    "Подставляет в исходный текст исправленные слова или оригиналы, в зависимости от установленного флага \"результат\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/27 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "816f5238a831451ca442f56ed83183bc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5.3. Проверка правописания в текстах. Подстановка исправленной орфографии.\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "SPELLCHECK_DIR = \"./results/FMEspellcheck/\"\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(SPELLCHECK_DIR)\n",
    "\n",
    "for filename_data in tqdm(filenames):\n",
    "    # Parse articles corrections file\n",
    "    root_data = parse_xml(SPELLCHECK_DIR + filename_data)\n",
    "    for article in root_data:\n",
    "        if article.tag == 'article':\n",
    "            filename_article = article.attrib['filename']\n",
    "            root_article = parse_xml(ARTICLES_DIR + filename_article)\n",
    "            text = get_xml_elem(root_article, 'text')\n",
    "            # Parse corrections in one article\n",
    "            words = []\n",
    "            for word in article:\n",
    "                if word.tag == 'word':\n",
    "                    pos = int(word.attrib['pos'])\n",
    "                    len_src = len(get_xml_elem(word, 'source').text)\n",
    "                    word_text = get_xml_elem(word, 'suggestion').text if word.attrib['result'] == '1' else get_xml_elem(word, 'source').text\n",
    "                    if word.attrib['add_to_pwl'] == '3' and len(word_text):\n",
    "                        word_text = word_text[0].upper() + word_text[1:]\n",
    "                    words.append((pos, word_text, len_src))\n",
    "            words.sort(reverse=True)\n",
    "            # Apply corrections\n",
    "            for word in words:\n",
    "                pos = word[0]\n",
    "                len_src = word[2]\n",
    "                word_text = word[1]\n",
    "                text.text = text.text[:pos] + word_text + (text.text[pos+len_src:] if pos+len_src <= len(text.text) else '')\n",
    "            # Write corrected article xml\n",
    "            with codecs.open(ARTICLES_DIR + filename_article, 'w', 'utf-8') as f:\n",
    "                f.write(prettify(root_article))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Парсер авторов статьи\n",
    "\n",
    "Ищет в конце текста статей конструкции типа ` [Xxxx]. [Xxxx]. [Xxxx]` или ` [Xxxx].[Xxxx]. [Xxxx]` и интерпретирует её как автора статьи."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eff302f4cf41475f87aef6772b29e4b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors found in total: 1287\n"
     ]
    }
   ],
   "source": [
    "# 6. Парсинг авторов статьи\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "COMBINATIONS_CORR = dict_merge(COMBINATIONS_CORR_UNICODE, {\n",
    "    'II' : 'П'\n",
    "})\n",
    "LOCAL_DICT = {'0':'О', '3':'З', '6':'б'}\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "n = 0\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    article = parse_xml(ARTICLES_DIR + filename)\n",
    "    textelem = get_xml_elem(article, 'text')\n",
    "    text = textelem.text\n",
    "    authors = get_xml_elem(article, 'authors')\n",
    "\n",
    "    auth_start = 1\n",
    "    auth_list = []\n",
    "    while auth_start and text is not None:\n",
    "        # Find first non-space from the end\n",
    "        while text[-1] == ' ' or text[-1] == '\\n' or text[-1] == '\\r':\n",
    "            text = text[:-1]\n",
    "\n",
    "        auth_start = 0\n",
    "        # Try recognize\n",
    "        first_space = max(text.rfind(' ', 0, len(text)), text.rfind('\\n', 0, len(text)), text.rfind('\\r', 0, len(text)))\n",
    "        second_space = max(text.rfind(' ', 0, first_space), text.rfind('\\n', 0, first_space), text.rfind('\\r', 0, first_space))\n",
    "        third_space = max(text.rfind(' ', 0, second_space), text.rfind('\\n', 0, second_space), text.rfind('\\r', 0, second_space))\n",
    "        if first_space >= 0 and text[first_space-1] == '.' and second_space >= 0:\n",
    "            if text.find('.', second_space, first_space-1) != -1: # If there's no space between initials\n",
    "                third_space = second_space\n",
    "                second_space = first_space\n",
    "            if text[second_space-1] == '.' and third_space >= 0:\n",
    "                # Check if first letters of each word are capitals\n",
    "                keep = text\n",
    "                for comb in LOCAL_DICT.keys():\n",
    "                    while text[third_space+1:].find(comb) != -1:\n",
    "                        text = text[:third_space+1+text[third_space+1:].find(comb)] + LOCAL_DICT[comb] + text[third_space+2+text[third_space+1:].find(comb):]\n",
    "                if re.match(r\"[A-ZА-ЯІ]\", text[first_space+1]) is not None and re.match(r\"[A-ZА-ЯІ]\", text[second_space+1]) is not None and re.match(r\"[A-ZА-ЯІ]\", text[third_space+1]) is not None:\n",
    "                    auth_start = third_space + 1\n",
    "                text = keep\n",
    "\n",
    "        if auth_start: # Suggest that an article cannot consist of author only and therefore auth_start should be > 0\n",
    "            #print(article.attrib['uri'], author_text)\n",
    "            author_text = text[auth_start:]\n",
    "            if author_text[author_text.find('.')+1] != ' ': # Add space if there's no one between initials\n",
    "                author_text = author_text[:author_text.find('.')+1] + ' ' + author_text[author_text.find('.')+1:]\n",
    "            if author_text[-1] == '.' or author_text[-1] == ',':\n",
    "                author_text = author_text[:-1]\n",
    "            # convert wrong symbols\n",
    "            for comb in dict_merge(COMBINATIONS_CORR, LOCAL_DICT).keys():\n",
    "                while author_text.find(comb) != -1:\n",
    "                    author_text = author_text[:author_text.find(comb)] + dict_merge(COMBINATIONS_CORR, LOCAL_DICT)[comb] + author_text[author_text.find(comb) + len(comb):]\n",
    "\n",
    "            auth_list.append(author_text)\n",
    "            text = text[:auth_start]\n",
    "\n",
    "    # add authors, reverse their order to alphabetic\n",
    "    for auth in reversed(auth_list):\n",
    "        n += 1\n",
    "        author = ElementTree.SubElement(authors, 'author')\n",
    "        author.text = auth\n",
    "\n",
    "    textelem.text = text\n",
    "    with codecs.open(ARTICLES_DIR + filename, 'w', 'utf-8') as f:\n",
    "        f.write(prettify(article))\n",
    "\n",
    "print(\"Authors found in total:\", n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Парсер литературы\n",
    "\n",
    "После извлечения авторов статьи в конце за текстом статьи присутствует только строчка литературы, если вообще присутствует. Поэтому ищется и извлекается фрагмент начиная с \"`Лит.:`\". Он разделяется на сегменты по \"`[num]`\", а сегменты на подфрагменты по запятым. Общий вид сегмента полагается следующим: \"`[Авторы (возможно несколько, определяются по наличию инициалов в конце подфрагмента)], Название (возможно содержит запятые), Номер тома (может отсутствовать), [Информация об издании (может частично или полностью отсутствовать)], Год, [Прочее (главы, страницы и прочее, может отсутствовать)];`\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76613f44ae4946dcb2ff7a795775b07d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Literature found in 1040 articles\n",
      "Publications found in total: 3214\n"
     ]
    }
   ],
   "source": [
    "# 7. Парсинг литературы\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "COMBINATIONS_CORR_LOCAL = dict_merge(dict_merge(COMBINATIONS_CORR_ALPHABET, COMBINATIONS_CORR_UNICODE), {'J':'Л'})\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Unit:\n",
    "    authors = []\n",
    "    title = \"\"\n",
    "    publication = \"\"\n",
    "    year = \"\"\n",
    "    other = \"\"\n",
    "\n",
    "\n",
    "n_lit = 0\n",
    "n_pub = 0\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    article = parse_xml(ARTICLES_DIR + filename)\n",
    "    textelem = get_xml_elem(article, 'text')\n",
    "    text = textelem.text\n",
    "    literature = get_xml_elem(article, 'literature')\n",
    "    literature_orig = get_xml_elem(literature, 'literature_orig')\n",
    "\n",
    "    if textelem.text is not None and len(textelem.text):\n",
    "        #Find literature start position and extract if present\n",
    "        for key in COMBINATIONS_CORR_LOCAL.keys():\n",
    "            while text.find(key) != -1:\n",
    "                text = text[:text.find(key)] + COMBINATIONS_CORR_LOCAL[key] + text[text.find(key)+1:]\n",
    "        text = text.upper()\n",
    "        lit_pos = text.rfind('\\nЛИТ.: ')\n",
    "        lit_pos = text.rfind('\\rЛИТ.: ') if lit_pos == -1 else lit_pos\n",
    "        lit_pos = text.rfind(' ЛИТ.: ') if lit_pos == -1 else lit_pos\n",
    "        if lit_pos != -1:\n",
    "            n_lit += 1\n",
    "            literature_orig.text = textelem.text[lit_pos:]\n",
    "            while literature_orig.text[0] in [' ', '\\n', '\\r']:\n",
    "                literature_orig.text = literature_orig.text[1:]\n",
    "            textelem.text = textelem.text[:lit_pos]\n",
    "            while textelem.text[-1] in [' ', '\\n', '\\r']:\n",
    "                textelem.text = textelem.text[:-1]\n",
    "\n",
    "\n",
    "            # Parse literature string\n",
    "            text = literature_orig.text\n",
    "            units = []\n",
    "            num = 1\n",
    "            while text.find('['+str(num)+']') != -1:\n",
    "                units.append(text[text.find('['+str(num)+']')+len('['+str(num)+']'):(text.find('['+str(num+1)+']') if text.find('['+str(num+1)+']') != -1 else len(text))])\n",
    "                n_pub += 1\n",
    "                num += 1\n",
    "            for unit in units:\n",
    "                logical_parts = Unit()\n",
    "                logical_parts.authors.clear()\n",
    "                subunits = unit.split(',')\n",
    "                while '' in subunits:\n",
    "                    subunits.remove('')\n",
    "                pos_last_auth = -1\n",
    "                pos_last_title = -1\n",
    "                pos_thome = -1\n",
    "                pos_transl = -1\n",
    "                pos_pub_num = -1\n",
    "                pos_pub_place = -1\n",
    "                pos_year = -1\n",
    "\n",
    "\n",
    "                # Define positions of most common pats of literature string\n",
    "                for i in range(len(subunits)):\n",
    "                    text = subunits[i]\n",
    "                    while text[-1] in [' ', '\\n', '\\r', ';']:\n",
    "                        text = text[:-1]\n",
    "                    while text[0] in [' ', '\\n', '\\r']:\n",
    "                        text = text[1:]\n",
    "                    subunits[i] = text\n",
    "\n",
    "                    if pos_last_auth + 1 == i: # Recognize authors\n",
    "                        keep = text\n",
    "                        pos_initials = 0\n",
    "                        for j in range(len(text)):\n",
    "                            if text[j] in COMBINATIONS_CORR_UNICODE:\n",
    "                                text = text[:j] + COMBINATIONS_CORR_UNICODE[text[j]] + text[j+1:]\n",
    "                        if text[-1] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-2]) is not None and text[-3] == ' ' and text[-4] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-5]) is not None:\n",
    "                            # \"X. X.\"\n",
    "                            pos_last_auth = i\n",
    "                            pos_initials = -5\n",
    "                        elif text[-1] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-2]) is not None and text[-3] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-4]) is not None:\n",
    "                            # \"X.X.\"\n",
    "                            pos_last_auth = i\n",
    "                            text = text[:-2] + ' ' + text[-2:]\n",
    "                            pos_initials = -5\n",
    "                        elif text[-1] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-2]) is not None:\n",
    "                            # \"X.\"\n",
    "                            pos_last_auth = i\n",
    "                            pos_initials = -2\n",
    "                        else: # Title starts\n",
    "                            text = keep\n",
    "                        # If correct\n",
    "                        if pos_last_auth == i:\n",
    "                            surname = text[:pos_initials]\n",
    "                            while surname.find(' ') != -1:\n",
    "                                surname = surname[:surname.find(' ')] + surname[surname.find(' ')+1:]\n",
    "                            text = surname + ' ' + text[pos_initials:]\n",
    "                            j = 1\n",
    "                            while j < len(text):\n",
    "                                if re.match(r\"[А-ЯA-Z]\", text[j]) is not None and re.match(r\"[а-яa-z]\", text[j-1]) is not None:\n",
    "                                    text = text[:j] + ' ' + text[j:]\n",
    "                                    j = 1\n",
    "                                else:\n",
    "                                    j += 1\n",
    "                            subunits[i] = text\n",
    "                    else:\n",
    "                        if pos_thome == -1: # Recognize thome\n",
    "                            keep = text\n",
    "                            for j in range(len(text)):\n",
    "                                if text[j] in COMBINATIONS_CORR_GLOBAL:\n",
    "                                    text = text[:j] + COMBINATIONS_CORR_GLOBAL[text[j]] + text[j+1:]\n",
    "                            if text.upper().find('Т.') != -1:\n",
    "                                pos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "                                pos_thome = i\n",
    "                            text = keep\n",
    "                        if pos_transl == -1: # Recognize publication number\n",
    "                            keep = text\n",
    "                            for j in range(len(text)):\n",
    "                                if text[j] in COMBINATIONS_CORR_GLOBAL:\n",
    "                                    text = text[:j] + COMBINATIONS_CORR_GLOBAL[text[j]] + text[j+1:]\n",
    "                            if text.upper().find('ПЕР.') != -1:\n",
    "                                pos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "                                pos_transl = i\n",
    "                            text = keep\n",
    "                        if pos_pub_num == -1: # Recognize publication number\n",
    "                            keep = text\n",
    "                            for j in range(len(text)):\n",
    "                                if text[j] in COMBINATIONS_CORR_GLOBAL:\n",
    "                                    text = text[:j] + COMBINATIONS_CORR_GLOBAL[text[j]] + text[j+1:]\n",
    "                            if text.upper().find('ИЗД.') != -1:\n",
    "                                pos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "                                pos_pub_num = i\n",
    "                            text = keep\n",
    "                        if pos_pub_place == -1: # Recognize publication place\n",
    "                            keep = text\n",
    "                            for j in range(len(text)):\n",
    "                                if text[j] in COMBINATIONS_CORR_GLOBAL:\n",
    "                                    text = text[:j] + COMBINATIONS_CORR_GLOBAL[text[j]] + text[j+1:]\n",
    "                            if text.upper() in ['М.', 'Л.', 'СПБ.', 'М.Л.', 'Л.М.', 'М.СПБ.', 'СПБ.М.']:\n",
    "                                pos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "                                pos_pub_place = i\n",
    "                            text = keep\n",
    "                        # If correct\n",
    "                        if pos_last_auth != i and (pos_thome == i or pos_pub_num == i or pos_pub_place == i):\n",
    "                            for j in range(len(text)):\n",
    "                                if text[j] in COMBINATIONS_CORR_UNICODE:\n",
    "                                    subunits[i] = text[:j] + COMBINATIONS_CORR_UNICODE[text[j]] + text[j+1:]\n",
    "\n",
    "                        if pos_year == -1 and len(text) >= 4: # Recognize year\n",
    "                            numbers = ['0','1','2','3','4','5','6','7','8','9']\n",
    "                            j = 0\n",
    "                            for j in range(len(text) - 3):\n",
    "                                if text[j] in numbers and text[j+1] in numbers and text[j+2] in numbers and text[j+3] in numbers:\n",
    "                                    pos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "                                    pos_year = i\n",
    "                                    break\n",
    "                            # if correct\n",
    "                            if pos_year == i:\n",
    "                                subunits[i] = text[j:j+4]\n",
    "\n",
    "\n",
    "                # Extract info from literature string using positions defined above\n",
    "                for i in range(len(subunits)):\n",
    "                    text = subunits[i]\n",
    "                    if pos_last_auth >= i: # Author\n",
    "                        logical_parts.authors.append(text)\n",
    "                    elif pos_last_auth < i <= pos_last_title: # Title\n",
    "                        logical_parts.title = logical_parts.title + ('' if len(logical_parts.title) == 0 else ', ') + text\n",
    "                    elif pos_year == i: # Year\n",
    "                        logical_parts.year = logical_parts.year + ('' if len(logical_parts.year) == 0 else ', ') + text\n",
    "                    elif ((pos_pub_num <= i and pos_pub_num != -1) or (pos_pub_place <= i and pos_pub_place != -1) or (pos_transl <= i and pos_transl != -1) or (pos_thome + 1 <= i and pos_thome != -1)) and pos_year > i: # Publication\n",
    "                        logical_parts.publication = logical_parts.publication + ('' if len(logical_parts.publication) == 0 else ', ') + text\n",
    "                    else: # Other\n",
    "                        logical_parts.other = logical_parts.other + ('' if len(logical_parts.other) == 0 else ', ') + text\n",
    "\n",
    "\n",
    "                # Debug section\n",
    "                \"\"\"print('\\n', filename, unit)\n",
    "                print('authors:', logical_parts.authors)\n",
    "                print('title:', logical_parts.title)\n",
    "                print('publication:', logical_parts.publication)\n",
    "                print('year:', logical_parts.year)\n",
    "                print('other:', logical_parts.other)\n",
    "                print(pos_last_auth, pos_last_title, pos_thome, pos_transl, pos_pub_num, pos_pub_place, pos_year)\"\"\"\n",
    "\n",
    "\n",
    "                # Add literature unit\n",
    "                unit = ElementTree.SubElement(literature, \"unit\")\n",
    "                for auth_str in logical_parts.authors:\n",
    "                    author = ElementTree.SubElement(unit, \"author\")\n",
    "                    author.text = auth_str\n",
    "                title = ElementTree.SubElement(unit, \"title\")\n",
    "                title.text = logical_parts.title\n",
    "                publication = ElementTree.SubElement(unit, \"publication\")\n",
    "                publication.text = logical_parts.publication\n",
    "                year = ElementTree.SubElement(unit, \"year\")\n",
    "                year.text = logical_parts.year\n",
    "                other = ElementTree.SubElement(unit, \"other\")\n",
    "                other.text = logical_parts.other\n",
    "\n",
    "\n",
    "            # Write xml\n",
    "            with codecs.open(ARTICLES_DIR + filename, 'w', 'utf-8') as f:\n",
    "                f.write(prettify(article))\n",
    "\n",
    "print(\"Literature found in\", n_lit, \"articles\")\n",
    "print(\"Publications found in total:\", n_pub)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Парсер формул\n",
    "\n",
    "Выносит из текстов ранее подготовленных xml-файлов статей сначала выносные, а затем строчные формулы, оставляя на их месте ссылку внутри математического окружения.\n",
    "\n",
    "Минимальная длина в символах, которой должна обладать строчная формула, настраивается."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11033152beb540309dd089dab20cdb26"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found main formulas: 9959\n",
      "Found auxiliary formulas: 38421\n"
     ]
    }
   ],
   "source": [
    "# 8. Парсер формул\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "MIN_INLINE_LEN = 0\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "\n",
    "n_main = 0\n",
    "n_aux = 0\n",
    "for filename in tqdm(filenames):\n",
    "    article = parse_xml(ARTICLES_DIR + filename)\n",
    "    #print('REMOTES: ' + article.attrib['uri'])\n",
    "    text = get_xml_elem(article, 'text')\n",
    "    formulas_main = get_xml_elem(article, 'formulas_main')\n",
    "    formulas_aux = get_xml_elem(article, 'formulas_aux')\n",
    "\n",
    "    # Get main formulas\n",
    "    pos_find = 0\n",
    "    pos_start = 0\n",
    "    pos_end = 0\n",
    "    n = 1\n",
    "    while text.text is not None and text.text.find('\\\\[', pos_find) != -1:\n",
    "        pos_start = text.text.find('\\\\[', pos_find) + 2\n",
    "        pos_end = text.text.find('\\\\]', pos_start)\n",
    "        while text.text[pos_start] == '\\n':\n",
    "            pos_start += 1\n",
    "        while text.text[pos_end-1] == '\\n':\n",
    "            pos_end -= 1\n",
    "        pos_find = pos_start\n",
    "        uri = URI_PREFIX + 'formula/main' + article.attrib['uri'][article.attrib['uri'].rfind('/', 0, article.attrib['uri'].find('_')):article.attrib['uri'].find('_')+1] + str(n) + article.attrib['uri'][article.attrib['uri'].find('_'):]\n",
    "        n += 1\n",
    "        formula = ElementTree.SubElement(formulas_main, 'formula', {'uri':uri})\n",
    "        formula.text = text.text[pos_start:pos_end]\n",
    "        text.text = text.text[:pos_start] + 'URI[[' + uri + ']]/URI' + text.text[pos_end:]\n",
    "    n_main += n\n",
    "\n",
    "    # Get auxiliary formulas\n",
    "    pos_find = 0\n",
    "    pos_start = 0\n",
    "    pos_end = 0\n",
    "    cnt = 0\n",
    "    n = 1\n",
    "    # Count dollar symbols\n",
    "    while text.text is not None and text.text.find('$', pos_find) != -1:\n",
    "        pos_find = text.text.find('$', pos_find) + 1\n",
    "        cnt += 1\n",
    "    # If cnt is not even assume that first one is garbage from title\n",
    "    pos_find = 0\n",
    "    if cnt % 2:\n",
    "        pos_find = text.text.find('$', pos_find)\n",
    "        text.text = text.text[:pos_find] + '#' + text.text[pos_find+1:]\n",
    "    while text.text is not None and text.text.find('$', pos_find) != -1:\n",
    "        pos_start = text.text.find('$', pos_find) + 1\n",
    "        pos_end = text.text.find('$', pos_start)\n",
    "        if not check_in_uri(text.text, pos_start) and not check_in_uri(text.text, pos_end):\n",
    "            while text.text[pos_start] == '\\n':\n",
    "                pos_start += 1\n",
    "            while text.text[pos_end-1] == '\\n':\n",
    "                pos_end -= 1\n",
    "            pos_find = pos_start\n",
    "            if pos_end - pos_start >= MIN_INLINE_LEN:\n",
    "                uri = URI_PREFIX + 'formula/aux' + article.attrib['uri'][article.attrib['uri'].rfind('/', 0, article.attrib['uri'].find('_')):article.attrib['uri'].find('_')+1] + str(n) + article.attrib['uri'][article.attrib['uri'].find('_'):]\n",
    "                n += 1\n",
    "                formula = ElementTree.SubElement(formulas_aux, 'formula', {'uri':uri})\n",
    "                formula.text = text.text[pos_start:pos_end]\n",
    "                text.text = text.text[:pos_start] + 'URI[[' + uri + ']]/URI' + text.text[pos_end:]\n",
    "            pos_find = text.text.find('$', pos_find) + 1\n",
    "        else:\n",
    "            pos_find = pos_end + 1\n",
    "    n_aux += n\n",
    "\n",
    "    with codecs.open(ARTICLES_DIR + filename, 'w', 'utf-8') as f:\n",
    "        f.write(prettify(article))\n",
    "\n",
    "print(\"Found main formulas:\", n_main)\n",
    "print(\"Found auxiliary formulas:\", n_aux)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.1. Вынос формул\n",
    "\n",
    "Выносит все формулы в отдельный файл с указанием типа для возможной последующей обработки."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8e335c55b584077abc8f4f34fd5452e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8.1. Вынос формул\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "EXIT_FILE = \"./results/FMEformulas.xml\"\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "\n",
    "\n",
    "formulas = ElementTree.Element('formulas')\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    root = parse_xml(ARTICLES_DIR + filename)\n",
    "    fmain = get_xml_elem(root, 'formulas_main')\n",
    "    faux = get_xml_elem(root, 'formulas_aux')\n",
    "\n",
    "    for formula in fmain:\n",
    "        formulas.append(formula)\n",
    "    for formula in faux:\n",
    "        formulas.append(formula)\n",
    "\n",
    "with codecs.open(EXIT_FILE, 'w', 'utf-8') as f:\n",
    "    f.write(prettify(formulas))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.2. Проверка формул\n",
    "\n",
    "Случайным образом выбирает 20 случайных формул (из случайных статей) и вставляет их в математическое окружение Markdown для визуальной проверки"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# 8.2. Проверка формул\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "EXIT_FILE = \"./matphys/FMEformulas_check.md\"\n",
    "NUMBER = 20\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "\n",
    "\n",
    "file = ''\n",
    "\n",
    "i = 0\n",
    "while i < NUMBER:\n",
    "    root = parse_xml(ARTICLES_DIR + filenames[randint(0, len(filenames)-1)])\n",
    "\n",
    "    # Get all the info from article\n",
    "    fmain = get_xml_elem(root, 'formulas_main')\n",
    "    start = get_xml_elem(root, 'pages/start').text\n",
    "\n",
    "\n",
    "    # if there's no formulas in the article try another one\n",
    "    total_num = 0\n",
    "    for formula in fmain:\n",
    "        total_num += 1\n",
    "    if not total_num:\n",
    "        continue\n",
    "    i += 1\n",
    "\n",
    "    num = randint(0, 100) % total_num\n",
    "\n",
    "    formula = fmain[num].text\n",
    "\n",
    "    file += f'{i}. Статья: {root.attrib[\"uri\"]}, Начало на стр. {start}, формула {num + 1}:\\n$${formula}$$\\n'\n",
    "\n",
    "with codecs.open(EXIT_FILE, 'w', 'utf-8') as f:\n",
    "    f.write(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Парсер ссылок типа \"смотри также\"\n",
    "\n",
    "Ищет в тексте ссылки начинающиеся на `\"см. [другие опциональные вводные слова]\"` и пытается найти соответствующие им статьи в энциклопедии."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing search base...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5672b7f507a146c48212449223479739"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching relations in articles...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45819f1a0d6a44c69ed7858df1604bee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations found in total: 1970\n"
     ]
    }
   ],
   "source": [
    "# 9. Парсер ссылок типа \"смотри также\"\n",
    "\n",
    "print(\"Preparing search base...\")\n",
    "\n",
    "import relations as r\n",
    "import importlib\n",
    "importlib.reload(r)\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "r.BRUTE_FORCE_MODE = False  # Maximum amount of links to find, but takes more time (very slow)\n",
    "r.USE_MULTIPROCESSING = False  # WARNING: Does not work inside Jupyter!!!; Significantly speeds up scanning process\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "r.run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}