{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Общий код\n",
    "\n",
    "Ячейка подключает библиотеки и задаёт функции, используемые в ячейках 1.1 и далее, для сокращения объёма кода."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# 0. Общий код\n",
    "\n",
    "# noinspection PyUnresolvedReferences\n",
    "from lib import *\n",
    "\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "# Symbols and combinations that have to be corrected after OCR\n",
    "COMBINATIONS_CORR_ALPHABET = {\n",
    "    'A': 'А', 'a': 'а', 'B': 'В', 'b': 'Ь', 'C': 'С', 'c': 'с', 'E': 'Е', 'e': 'е', 'H': 'Н', 'K': 'К', 'M': 'М',\n",
    "    'O': 'О', 'P': 'Р', 'p': 'р', 'T': 'Т', 'X': 'Х', 'y': 'у', 'x': 'х',\n",
    "    'U': 'И',\n",
    "    'u': 'и',\n",
    "    'r': 'г',\n",
    "    'N': 'П',\n",
    "    'n': 'п',\n",
    "    'm': 'т',\n",
    "    'Y': 'У',\n",
    "    # 'S' : 'Я',\t\t# Seems irrelevant\n",
    "}\n",
    "COMBINATIONS_CORR_UNICODE = {\n",
    "    'І': 'I',  # These two \"I\" are different!\n",
    "    'ก': 'п',\n",
    "    '山': 'Ц',\n",
    "    'כ': 'э',\n",
    "    'חи': 'пи',\n",
    "    'प': 'Ч',\n",
    "    'иั': 'й'\n",
    "}\n",
    "COMBINATIONS_CORR_OTHER = {\n",
    "    ' -': '-',\n",
    "    '- ': '-',\n",
    "    '0': 'О',\n",
    "    '3': 'З',\n",
    "    '6': 'б',\n",
    "}\n",
    "COMBINATIONS_CORR_GLOBAL = dict_merge(COMBINATIONS_CORR_ALPHABET,\n",
    "                                      dict_merge(COMBINATIONS_CORR_UNICODE, COMBINATIONS_CORR_OTHER))\n",
    "# Symbols excluded in xml have to be converted back\n",
    "XML_EXCLUDES = {\n",
    "    '&quot;': '\"',\n",
    "    '&apos;': \"'\",\n",
    "    '&lt;': '<',\n",
    "    '&gt;': '>',\n",
    "    '&amp;': '&'\n",
    "}\n",
    "PERSONAL_WORD_LIST = \"./matphys/PWL.txt\"\n",
    "URI_PREFIX = \"http://libmeta.ru/fme/\"\n",
    "# ----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Базовый парсер заголовков\n",
    "\n",
    "Вытаскивает из latex-кода заголовки статей и их расположение в файлах.\n",
    "\n",
    "Разбивка происходит в полуручном режиме, т.к. нет уверенности в формате заголовков.\n",
    "\n",
    "В тексте ищутся слова, содержащие в своём составе заглавные буквы на русском и английском языках в отношении, большем или равным заданному (по умолчанию 0.51, при меньших значениях количество вхождений значительно возрастает, например за счёт двухбуквенных предлогов). Предполагается, что таким образом удаётся обнаруживать неправильно машинно распознанный капс. Слова или цепочки слов, состоящие из одного строчного символа включаются в заголовок, если стоят между слов, определённых как часть заголовка. При этом, одиночные заглавные буквы, а также инициалы не воспринимаются как начало заголовка.\n",
    "\n",
    "## Использование\n",
    "- При удовлетворительном определении заголовка нажать `Enter` без дополнительного ввода.\n",
    "- Если предложенное место заголовком не является ввести `\"n\"`\n",
    "- При неправильном определении границ заголовка ввести два корректировочных числа для сдвига левой и правой границы.\n",
    "  - ЗАМЕЧАНИЕ: сдвиг производится попробельно, т.е. двойной пробел будет распознан как слово нулевой длины.\n",
    "  - ЗАМЕЧАНИЕ: границы отображаемого фрагмента текста будут передвинуты автоматически. Длины левой и правой границ в словах задаются в параметрах.\n",
    "  - ПРИМЕРЫ:\n",
    "    - `out: a [B C] d e f` -> `in: 0 2` -> `out: a [B C D E] f`\n",
    "    - `out: a b c [D E] f` -> `in: 2 -1` -> `out: a [B C D] e f`\n",
    "- Также возможен посимвольный сдвиг правой границы в случае \"сращивания\" заголовка статьи и её текста. Ввести одно число, начиная с точки.\n",
    "  - ПРИМЕРЫ:\n",
    "    - `out: a[BC]def` -> `in: .2` -> `out: a[BCDE]f`\n",
    "    - `out: a[BCDE]f` -> `in: .-1` -> `out: a[BCD]ef`\n",
    "\n",
    "В выводе в терминале переносы строк для удобства заменены на `\"$\"`\n",
    "\n",
    "### Прочее\n",
    "- Для определителя капса доступны исключения, которые никогда не будут рассматриваться, как потенциальные начала заголовков, см. опции. По умолчанию: первые 10 римских цифр, \"МэВ\" и \"ГэВ\". Также определитель не реагирует на \"СМ.\", что часто встречается в ссылках сразу после заголовков.\n",
    "- Использовать системный терминал для взаимодействия оказывается удобнее, чем использовать jupyter, поэтому можно скопировать ячейку с кодом в файл `scripter.py` и запускать его.\n",
    "- При положительном определении заголовка файл дополняется немедленно, прервать процесс можно в любой момент, как и продолжить после -- итоговый файл будет дополняться, а не перезаписываться с нуля при новом запуске программы (главное не забыть предварительно удалить из конца файла дубликаты, если вы начинаете с той страницы, на которой закончили в прошлый раз, а не со следующей).\n",
    "- В случае пропуска парсером заголовка его можно добавить вручную двумя способами:\n",
    "  1) Сдвинуть границы заголовка назад, как описано в инструкции выше. Подходит, если была пропущена небольшая (обычно ссылочная) статья, примерно 20 слов, плюс-минус. При этом после ввода заголовка поиск продолжится с __его__ конца, поэтому следующий заголовок \"вместо\" которого был введён пропущенный будет определён заново и пропущен не будет.\n",
    "  2) Воспользоваться ячейкой 1.1. Для этого в сыром tex-файле страницы нужно отыскать заголовок, скопировать его и __в точности__ вставить в разделе параметров, а также указать номер страницы. Скрипт парсера при этом можно не закрывать, последующая нумерация подстроится автоматически."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Базовый парсер заголовков\n",
    "\n",
    "# noinspection PyUnresolvedReferences\n",
    "from os import walk\n",
    "# noinspection PyUnresolvedReferences\n",
    "import xml.etree.ElementTree as ElementTree\n",
    "# noinspection PyUnresolvedReferences\n",
    "from xml.dom import minidom\n",
    "# noinspection PyUnresolvedReferences\n",
    "import re\n",
    "# noinspection PyUnresolvedReferences\n",
    "import codecs\n",
    "\n",
    "\n",
    "############################ VARS ################################\n",
    "PAGES_DIR = \"./matphys/rpages/\"\n",
    "EXIT_DIR = \"./matphys/\"\n",
    "EXIT_FILE = \"FMEv2.xml\"\n",
    "# First and last pages to be parsed\n",
    "START_PAGE = 639\n",
    "END_PAGE = 700\n",
    "# How many words to display before and after a potential title\n",
    "LEAD_WORDS = 5\n",
    "AFT_WORDS = 5\n",
    "# Look in the description\n",
    "CAPS_QUOT = 0.51\n",
    "EXCEPTIONS = ['I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX', 'X', 'МэВ', 'ГэВ']\n",
    "##################################################################\n",
    "\n",
    "\n",
    "\n",
    "class Article:\n",
    "    start_title = 0\n",
    "    end_title = 0\n",
    "    filename = ''\n",
    "\n",
    "\n",
    "\n",
    "# Write xml tree to file\n",
    "def prettify_1(elem_local:ElementTree.Element) -> str:\n",
    "    # Pretty-printed XML string for the Element.\n",
    "    rough_string = ElementTree.tostring(elem_local, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"  \")\n",
    "def xml_write_1(root_local:ElementTree.Element):\n",
    "    with codecs.open(EXIT_DIR + EXIT_FILE, 'w', 'utf-8') as f_out:\n",
    "        f_out.write(prettify_1(root_local))\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames_raw = next(walk(PAGES_DIR), (None, None, []))[2]  # [] if no file\n",
    "filenames = []\n",
    "for i in range(START_PAGE, END_PAGE + 1):\n",
    "    for filename in filenames_raw:\n",
    "        beginning = \"rp-\" + str(i) + \"_\"\n",
    "        if filename[:len(beginning)] == beginning and filename[-4:] == \".mmd\":\n",
    "            filenames.append(filename)\n",
    "\n",
    "\n",
    "# Check for existing xml\n",
    "filenames_raw = next(walk(EXIT_DIR), (None, None, []))[2]  # [] if no file\n",
    "if not(EXIT_FILE in filenames_raw):\n",
    "    root = ElementTree.Element('data')\n",
    "    xml_write_1(root)\n",
    "\n",
    "\n",
    "# Convert xml excluded symbols\n",
    "def xml_excluded_convert (text_local:str) -> str:\n",
    "    for key_local in XML_EXCLUDES.keys():\n",
    "        while text_local.find(key_local) != -1:\n",
    "            pos_local = text_local.find(key_local)\n",
    "            text_local = text_local[:pos_local] + XML_EXCLUDES[key_local] + text_local[pos_local + len(key_local):]\n",
    "    return text_local\n",
    "def remove_xml_spaces_1(elem_local:ElementTree.Element) -> ElementTree.Element:\n",
    "    elem_local.tail = None\n",
    "    if elem_local.text is not None:\n",
    "        is_space = True\n",
    "        for letter in elem_local.text:\n",
    "            is_space = False if letter != ' ' else is_space\n",
    "        elem_local.text = None if is_space else xml_excluded_convert(elem_local.text)\n",
    "    for subelem in elem_local:\n",
    "        subelem = remove_xml_spaces_1(subelem)\n",
    "        subelem.tail = None\n",
    "    return elem_local\n",
    "def parse_xml_1() -> ElementTree.Element:\n",
    "    # Parse existing xml (string parsing is needed to avoid extra newlines appearing)\n",
    "    exit_string = ''\n",
    "    with codecs.open(EXIT_DIR + EXIT_FILE, 'r', 'utf-8') as f_in:\n",
    "        for p in f_in.readlines():\n",
    "            exit_string += p[:-1]\n",
    "    root_local = ElementTree.fromstring(exit_string)\n",
    "    # Remove empty tails and texts\n",
    "    root_local = remove_xml_spaces_1(root_local)\n",
    "    return root_local\n",
    "root = parse_xml_1()\n",
    "num = len(root) + 1\n",
    "\n",
    "\n",
    "# Add article title and metadata to xml tree\n",
    "def add_article_1(elem_local:Article) -> int:\n",
    "    # Update root in case it's been changed\n",
    "    elem_root = parse_xml_1()\n",
    "    elem_num = len(elem_root) + 1\n",
    "    elem_article = ElementTree.SubElement(elem_root, 'article', {'n':str(elem_num)})\n",
    "    elem_title = ElementTree.SubElement(elem_article, 'title')\n",
    "    elem_title.text = file[elem_local.start_title + 1:elem_local.end_title]\n",
    "    elem_title_meta = ElementTree.SubElement(elem_article, 'title-meta')\n",
    "    elem_title_file = ElementTree.SubElement(elem_title_meta, 'title-file')\n",
    "    elem_title_file.text = elem_local.filename\n",
    "    elem_title_start = ElementTree.SubElement(elem_title_meta, 'title-start')\n",
    "    elem_title_start.text = str(elem_local.start_title + 1)\n",
    "    elem_title_end = ElementTree.SubElement(elem_title_meta, 'title-end')\n",
    "    elem_title_end.text = str(elem_local.end_title)\n",
    "    xml_write_1(elem_root)\n",
    "    return elem_num\n",
    "\n",
    "\n",
    "# Count number of alphabetic letters in word\n",
    "def count_letters_1(word_local:str) -> int:\n",
    "    number = 0\n",
    "    for letter in word_local:\n",
    "        number += 0 if re.match(r\"[A-ZА-Яa-zа-я]\", letter) is None else 1\n",
    "    return number\n",
    "\n",
    "# Check if word is written in CAPS\n",
    "def check_caps_1(word_local:str) -> int:\n",
    "    number = 0\n",
    "    len_word = 0\n",
    "    while len(word_local) and re.match(r\"[!#$%&'*+-.^_`|~:]\", word_local[-1]) is not None:\n",
    "        word_local = word_local[:-1]\n",
    "    while len(word_local) and re.match(r\"[!#$%&'*+-.^_`|~:]\", word_local[0]) is not None:\n",
    "        word_local = word_local[1:]\n",
    "    for letter in word_local:\n",
    "        #num += 0 if re.match(r\"[A-ZА-Я0-9]|[!#$%&'*+-.^_`|~:]\", letter) is None else 1\t\t\t\t\t# Too many symbols, math formulas are being detected\n",
    "        len_word += 1 if re.match(r\"[!#$%&'*+-.^_`|~:]\", letter) is None else 0\n",
    "        number += 0 if re.match(r\"[A-ZА-Я]\", letter) is None else 1\n",
    "    return 0 if len_word == 0 or number / len_word < CAPS_QUOT or word_local in EXCEPTIONS else number\t\t\t\t# Also exclude common roman numbers\n",
    "\n",
    "# Check for initials like \"I.E.\"\n",
    "def check_initials_1(word_to_check:str) -> bool:\n",
    "    initials = True\n",
    "    for p in range(len(word_to_check) - 1):\n",
    "        type_1 = 0 if re.match(r\"[A-ZА-Яa-zа-я]\", word_to_check[p]) is None else 1\n",
    "        type_2 = 0 if re.match(r\"[A-ZА-Яa-zа-я]\", word_to_check[p + 1]) is None else 1\n",
    "        initials = False if type_1 and type_2 else initials\n",
    "    return initials\n",
    "\n",
    "# Check if the word is \"CM.\" which happens often\n",
    "def check_link_1(word_local:str) -> bool:\n",
    "    word_local = word_local.upper()\n",
    "    # Convert to cyrillic\n",
    "    for p in range(len(word_local)):\n",
    "        word_local = (word_local[:p] + 'С' + word_local[p + 1:]) if word_local[p] == 'C' else word_local\n",
    "        word_local = (word_local[:p] + 'М' + word_local[p + 1:]) if word_local[p] == 'M' else word_local\n",
    "    return True if word_local == 'СМ.' else False\n",
    "\n",
    "\n",
    "# Find next ot prev word boundary (space / newline)\n",
    "def prev_from_1(position:int, file_name:str) -> int:\n",
    "    position = max(position, 0)\n",
    "    prev_space = file_name.rfind(' ', 0, position)\n",
    "    prev_nl = file_name.rfind('\\n', 0, position)\n",
    "    prev_space = -1 if prev_space == -1 else prev_space\n",
    "    prev_nl = -1 if prev_nl == -1 else prev_nl\n",
    "    return max(prev_nl, prev_space)\n",
    "def next_from_1(position:int, file_name:str, end_replace = True) -> int:\n",
    "    next_space = file_name.find(' ', position + 1)\n",
    "    next_nl = file_name.find('\\n', position + 1)\n",
    "    if end_replace:\n",
    "        next_space = len(file_name) if next_space == -1 else next_space\n",
    "        next_nl = len(file_name) if next_nl == -1 else next_nl\n",
    "    return max(next_nl, next_space) if next_space == -1 or next_nl == -1 else min(next_nl, next_space)\n",
    "\n",
    "\n",
    "# Main loop\n",
    "for filename in filenames:\n",
    "    print()\n",
    "    print(\"################################ \" + filename + \" ################################\")\n",
    "    with codecs.open(PAGES_DIR + filename, 'r', 'utf-8') as f:\n",
    "        file = f.read()\n",
    "\n",
    "    word_bound_l = -1\n",
    "    word_bound_r = next_from_1(word_bound_l, file, end_replace=False)\n",
    "    EOF_reached = False\n",
    "\n",
    "    while not EOF_reached:\n",
    "        if word_bound_r == -1:\n",
    "            word_bound_r = len(file)\n",
    "            EOF_reached = True\n",
    "\n",
    "\n",
    "        if check_caps_1(file[word_bound_l+1:word_bound_r]) < 2 or check_initials_1(file[word_bound_l+1:word_bound_r]) or check_link_1(file[word_bound_l+1:word_bound_r]):\n",
    "            word_bound_l = word_bound_r\n",
    "            word_bound_r = next_from_1(word_bound_l, file, end_replace=False)\n",
    "\n",
    "        else: # Possibly found a title\n",
    "            # Left border of a title is already known\n",
    "            start_title = word_bound_l\n",
    "\n",
    "            # Define right border of a title\n",
    "            defined_end = False\n",
    "            end_title = word_bound_r\n",
    "            while not defined_end:\n",
    "                word_bound_l = word_bound_r\n",
    "                word_bound_r = next_from_1(word_bound_l, file)\n",
    "\n",
    "                if word_bound_l == len(file):\n",
    "                    defined_end = True\n",
    "                elif check_link_1(file[word_bound_l+1:word_bound_r]):\n",
    "                    # A \"CM.\" link, not a title\n",
    "                    pass\n",
    "                elif not check_caps_1(file[word_bound_l+1:word_bound_r]) and count_letters_1(file[word_bound_l+1:word_bound_r]) < 2:\n",
    "                    if re.match(r\"[A-ZА-Яa-zа-я]\", file[word_bound_l+1]) is not None:\n",
    "                        # Most possibly belongs to title\n",
    "                        end_title = word_bound_r\n",
    "                    else:\n",
    "                        # Most possibly NOT belongs to title\n",
    "                        pass\n",
    "                elif check_caps_1(file[word_bound_l+1:word_bound_r]):\n",
    "                    end_title = word_bound_r\n",
    "                else:\n",
    "                    defined_end = True\n",
    "\n",
    "            next_title = False\n",
    "            while not next_title:\n",
    "                # Update root in case it's been changed\n",
    "                root = parse_xml_1()\n",
    "                num = len(root) + 1\n",
    "\n",
    "                # Console output for further user actions\n",
    "                segment_start = start_title\n",
    "                segment_end = end_title\n",
    "                for i in range(LEAD_WORDS):\n",
    "                    segment_start = prev_from_1(segment_start, file)\n",
    "                for i in range(AFT_WORDS):\n",
    "                    segment_end = next_from_1(segment_end, file)\n",
    "\n",
    "                out_str = file[segment_start+1:segment_end]\n",
    "\n",
    "                # Format\n",
    "                for i in range(len(out_str)):\n",
    "                    out_str = out_str[:i] + ('$' if out_str[i] == '\\n' else out_str[i]) + out_str[i+1:]\n",
    "                out_str = f\"{num})\\n\" + out_str + '\\n' + ' ' * (start_title - segment_start) + '^' * (end_title - start_title - 1)\n",
    "                # Check for \"section\" in the string. This is referred to alphabetic tip at the bottom of the page\n",
    "                \"\"\"if 'section' in out_str or 'title' in out_str:\n",
    "                    out_str += '     ############################### Title or section found! ###############################'\"\"\" # Not Used\n",
    "                print(out_str)\n",
    "\n",
    "                # User actions\n",
    "                response = input()\n",
    "                # noinspection PyBroadException\n",
    "                try:\n",
    "                    if response == '':\n",
    "                        # Add article\n",
    "                        article = Article()\n",
    "                        article.start_title = start_title\n",
    "                        article.end_title = end_title\n",
    "                        article.filename = filename\n",
    "                        num = add_article_1(article)\n",
    "                        next_title = True\n",
    "                        word_bound_l = end_title\n",
    "                        word_bound_r = next_from_1(word_bound_l, file, end_replace=False)\n",
    "                        print(f'Adding article, n=\"{num}\", title=\"{file[start_title+1:end_title]}\"\\n\\n')\n",
    "                    elif response == 'n' or response == 'т':\n",
    "                        # Do not add this one\n",
    "                        next_title = True\n",
    "                        print(\"Not an article, skipping\\n\\n\")\n",
    "                    elif response[0] == '.':\n",
    "                        end_title += int(response[1:])\n",
    "                        print(\"Changing title right border\\n\\n\")\n",
    "                    else:\n",
    "                        # Change title borders\n",
    "                        corrections = response.split(' ')\n",
    "                        corrections[0] = int(corrections[0])\n",
    "                        corrections[1] = int(corrections[1])\n",
    "                        if corrections[0] > 0:\n",
    "                            for i in range(abs(corrections[0])):\n",
    "                                start_title = prev_from_1(start_title, file)\n",
    "                        if corrections[0] < 0:\n",
    "                            for i in range(abs(corrections[0])):\n",
    "                                start_title = next_from_1(start_title, file)\n",
    "                        if corrections[1] < 0:\n",
    "                            for i in range(abs(corrections[1])):\n",
    "                                end_title = prev_from_1(end_title, file)\n",
    "                        if corrections[1] > 0:\n",
    "                            for i in range(abs(corrections[1])):\n",
    "                                end_title = next_from_1(end_title, file)\n",
    "                        print(\"Changing title borders\\n\\n\")\n",
    "                except:\n",
    "                    print(\"########## !!! Failed on input, try again !!! ##########\\n\\n\")\n",
    "\n",
    "\n",
    "# End reached\n",
    "print('###########################################################################################')\n",
    "print('Last requested page processed. Press \"Enter\" to close this window.')\n",
    "response = input()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1. Добавление заголовков по одному\n",
    "\n",
    "В разделе параметров указать номер страницы и ТОЧНУЮ формулировку заголовка из сырого latex-текста, а также номер страницы, после чего запустить ячейку.\n",
    "\n",
    "Закрывать скрипт парсера не обязательно, это не вызовет ошибок и его нумерация подстроится автоматически."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1.1. Добавление заголовков по одному\n",
    "\n",
    "############################ VARS ################################\n",
    "PAGES_DIR = \"./matphys/rpages/\"\n",
    "EXIT_DIR = \"./results/FMEtitles/\"\n",
    "EXIT_FILE = \"FMEtitles-added-manually.xml\"\n",
    "# Search parameters\n",
    "PAGE = 146\n",
    "TITLE = 'глюоний'\n",
    "##################################################################\n",
    "\n",
    "\n",
    "\n",
    "class Article:\n",
    "    start_title = 0\n",
    "    end_title = 0\n",
    "    filename = ''\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames_raw = get_filenames(PAGES_DIR)\n",
    "filenames = []\n",
    "for i in range(PAGE, PAGE + 1):\n",
    "    for filename in filenames_raw:\n",
    "        beginning = \"rp-\" + str(i) + \"_\"\n",
    "        if filename[:len(beginning)] == beginning and filename[-4:] == \".mmd\":\n",
    "            filenames.append(filename)\n",
    "\n",
    "\n",
    "# Check for existing xml\n",
    "filenames_raw = get_filenames(EXIT_DIR)\n",
    "if not(EXIT_FILE in filenames_raw):\n",
    "    root = ElementTree.Element('data')\n",
    "    xml_write(root, EXIT_DIR + EXIT_FILE)\n",
    "\n",
    "\n",
    "root = parse_xml(EXIT_DIR + EXIT_FILE)\n",
    "\n",
    "\n",
    "# Add article title and metadata to xml tree\n",
    "def add_article(article_local:Article, etree_root:ElementTree.Element, number:int):\n",
    "    elem_article = ElementTree.SubElement(etree_root, 'article', {'n':str(number)})\n",
    "    elem_title = ElementTree.SubElement(elem_article, 'title')\n",
    "    elem_title.text = file[article_local.start_title + 1:article_local.end_title]\n",
    "    elem_title_meta = ElementTree.SubElement(elem_article, 'title-meta')\n",
    "    elem_title_file = ElementTree.SubElement(elem_title_meta, 'title-file')\n",
    "    elem_title_file.text = article_local.filename\n",
    "    elem_title_start = ElementTree.SubElement(elem_title_meta, 'title-start')\n",
    "    elem_title_start.text = str(article_local.start_title + 1)\n",
    "    elem_title_end = ElementTree.SubElement(elem_title_meta, 'title-end')\n",
    "    elem_title_end.text = str(article_local.end_title)\n",
    "    xml_write(etree_root, EXIT_DIR + EXIT_FILE)\n",
    "\n",
    "# Read requested file\n",
    "with codecs.open(PAGES_DIR + filenames[0], 'r', 'utf-8') as f:\n",
    "    file = f.read()\n",
    "\n",
    "# Find titles and add them\n",
    "start_title = 0\n",
    "end_title = 0\n",
    "num = len(root) + 1\n",
    "while file.find(TITLE, end_title) != -1:\n",
    "    start_title = file.find(TITLE, start_title)\n",
    "    end_title = start_title + len(TITLE)\n",
    "    start_title -= 1 # Set on space before the title\n",
    "\n",
    "    article = Article()\n",
    "    article.start_title = max(start_title, 0)\n",
    "    article.end_title = min(end_title, len(file))\n",
    "    article.filename = filenames[0]\n",
    "    add_article(article, root, num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Исправление ошибок в заголовках\n",
    "\n",
    "Состоит из двух частей: \"составитель пар\" и \"подстановщик\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1. Составитель пар \"оригинальный - исправленный\" для заголовков\n",
    "\n",
    "Формирует xml-список всех заголовков с возможными автоматическими исправлениями (в формате было / стало):\n",
    "1. замена латиницы на аналогичную кириллицу;\n",
    "2. замена заданных буквосочетаний (см. параметры)\n",
    "3. удаление обрамляющих знаков препинания;\n",
    "4. замена всех букв на заглавные (в том числе это избавляет дальнейшей необходимости исправлять имена);\n",
    "5. слияние разорванных на отдельные буквы слов (если рядом оказываются несколько таких слов, то они оказываются слиты вместе).\n",
    "\n",
    "Этот список необходимо просмотреть и исправить оставшиеся ошибки.\n",
    "\n",
    "Дополнительно, для помощи в поиске орфографических ошибок, формируется строка с изменениями, предложенными спеллчекером. ВНИМАНИЕ: спеллчекер может делать ошибки в именах, специфических терминах и т.п., поэтому следует использовать его результаты лишь для ориентира."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2.1. Составитель пар \"оригинальный - исправленный\" для заголовков:\n",
    "\n",
    "############################ VARS ################################\n",
    "WORK_DIR = \"./matphys/\"\n",
    "INPUT_FILE = \"FMEv2.xml\"\n",
    "CORRECTION_FILE = \"FMEcorr.xml\"\n",
    "COMBINATIONS_CORR = dict_merge(COMBINATIONS_CORR_GLOBAL, {\n",
    "    'ХК' : 'Ж',\n",
    "    'ЬI' : 'Ы',\n",
    "    'II' : 'Ш',\n",
    "    'I' : 'П',\n",
    "    'J' : 'Л',\n",
    "    'ЛАГРАНХ' : 'ЛАГРАНЖ',\n",
    "    'ЛАТРАНХ' : 'ЛАГРАНЖ',\n",
    "})\n",
    "SPELLCHECK_ONLY = True # Use if the only thing you need from this script is spellcheck\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Check for existing xml\n",
    "filenames_raw = get_filenames(WORK_DIR)\n",
    "if not(INPUT_FILE in filenames_raw):\n",
    "    root = ElementTree.Element('data')\n",
    "    xml_write(root, WORK_DIR + CORRECTION_FILE)\n",
    "\n",
    "\n",
    "root = parse_xml(WORK_DIR + INPUT_FILE)\n",
    "\n",
    "\n",
    "# Get all the titles into a dict\n",
    "titles_dict = {}\n",
    "pages_dict = {}\n",
    "for article in root:\n",
    "    title = get_xml_elem(article, 'title').text\n",
    "    titles_dict[title] = (title, title)\n",
    "    title_file = get_xml_elem(article, 'title-meta/title-file')\n",
    "    pages_dict[title] = title_file.text[title_file.text.find('-')+1:title_file.text.find('_')]\n",
    "\n",
    "\n",
    "if not SPELLCHECK_ONLY:\n",
    "    # Correct preferred combinations and latin letters\n",
    "    for title in titles_dict.keys():\n",
    "        title_new = title_handle_latin(titles_dict[title][0], COMBINATIONS_CORR)\n",
    "        titles_dict[title] = (title_new, title_new)\n",
    "\n",
    "    # Remove bounding symbols\n",
    "    for title in titles_dict.keys():\n",
    "        title_new = title_handle_bounding(titles_dict[title][0])\n",
    "        titles_dict[title] = (title_new, title_new)\n",
    "\n",
    "    # CAPS\n",
    "    for title in titles_dict.keys():\n",
    "        title_new = titles_dict[title][0].upper()\n",
    "        titles_dict[title] = (title_new, title_new)\n",
    "\n",
    "    # Merge single-lettered words\n",
    "    for title in titles_dict.keys():\n",
    "        title_new = title_handle_merge(titles_dict[title][0])\n",
    "        titles_dict[title] = (title_new, title_new)\n",
    "\n",
    "    # Revert changes for aux formulas in titles\n",
    "    for title in titles_dict.keys():\n",
    "        title_new = title_handle_formulas(titles_dict[title][0], title)\n",
    "        titles_dict[title] = (title_new, title_new)\n",
    "\n",
    "# Try spellcheck on titles\n",
    "spellcheck_dict_update()\n",
    "for title in titles_dict.keys():\n",
    "    title_new = titles_dict[title][0]\n",
    "    title_suggestions = do_spellcheck(title_new)\n",
    "    for i in range(len(title_new)):\n",
    "        title_new = title_new[:i] + ('_' if title_new[i] not in [' ', '\\n', '\\r'] else title_new[i]) + (title_new[i+1:] if i + 1 <= len(title_new) else '')\n",
    "    for pos in sorted(title_suggestions.keys(), reverse=True):\n",
    "        title_new = title_new[:pos] + title_suggestions[pos][1] + title_new[pos+len(title_suggestions[pos][0]):]\n",
    "    titles_dict[title] = (titles_dict[title][0], title_new)\n",
    "\n",
    "\n",
    "# Write corrections xml\n",
    "root = ElementTree.Element('data')\n",
    "for i in titles_dict.items():\n",
    "    pair = ElementTree.SubElement(root, 'pair')\n",
    "    title_old = ElementTree.SubElement(pair, 'title_old')\n",
    "    title_old.text = i[0]\n",
    "    title_new = ElementTree.SubElement(pair, 'title_new')\n",
    "    title_new.text = i[1][0]\n",
    "    title_new = ElementTree.SubElement(pair, 'title__sc')\n",
    "    title_new.text = i[1][1]\n",
    "    page = ElementTree.SubElement(pair, 'page')\n",
    "    page.text = pages_dict[i[0]]\n",
    "xml_write(root, WORK_DIR + CORRECTION_FILE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2. Подстановщик исправленных заголовков\n",
    "\n",
    "Заменяет все заголовки на исправленные согласно списку пар."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2.2. Подстановщик исправленных заголовков:\n",
    "\n",
    "############################ VARS ################################\n",
    "WORK_DIR = \"./matphys/\"\n",
    "INPUT_FILE = \"FMEv2.xml\"\n",
    "CORRECTION_FILE = \"FMEcorr.xml\"\n",
    "EXIT_FILE = \"FMEtitles.xml\"\n",
    "##################################################################\n",
    "\n",
    "\n",
    "\n",
    "root = parse_xml(WORK_DIR + CORRECTION_FILE)\n",
    "\n",
    "\n",
    "# Get all the corrections into a dict\n",
    "titles_dict = {}\n",
    "for pair in root:\n",
    "    titles_dict[get_xml_elem(pair, 'title_old').text] = get_xml_elem(pair, 'title_new').text\n",
    "\n",
    "\n",
    "root = parse_xml(WORK_DIR + INPUT_FILE)\n",
    "\n",
    "\n",
    "# Replace titles\n",
    "for article in root:\n",
    "    get_xml_elem(article, 'title').text = titles_dict[get_xml_elem(article, 'title').text]\n",
    "xml_write(root, WORK_DIR + EXIT_FILE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Сортировщик / сливщик файлов с заголовками\n",
    "\n",
    "Сортирует статьи в файлах из основного списка в порядке страница-расположение, т.е. (если не сказано иного) в алфавитном порядке и выводит в один выходной файл. Также порядковый номер заменяется uri формата \"http://libmeta.ru/fme/article/1_Kraevaya\". (Созданные uri кешируются по номеру страницы и позиции заголовка в тексте и при последующих запусках остаются неизменными, если включен `URI_SAFER`).\n",
    "\n",
    "Также в конец выходного файл добавляются заголовки из \"ручного\" файла, в том же формате, но без сортировки, что позволяет добавлять случайно забытые статьи без изменения uri и имён файлов всех остальных статей."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing main input files...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f82b8a9141174489b2eb4768b3b4879c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing \"manual\" file...\n",
      "Writing main articles...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3584 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3729f14cb8a4ad48a2b2c2abd170c8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0e67556e420456fbf3629a45c2ac819"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Сортировщик / сливщик файлов с заголовками\n",
    "\n",
    "############################ VARS ################################\n",
    "WORK_DIR = \"./results/\"\n",
    "TITLES_DIR = \"FMEtitles/\"\n",
    "INPUT_FILES = [\"FMEtitles-p5-100.xml\", \"FMEtitles-p101-200.xml\", \"FMEtitles-p201-300.xml\", \"FMEtitles-p301-400.xml\", \"FMEtitles-p301-400-add.xml\",\n",
    "               \"FMEtitles-p401-500.xml\", \"FMEtitles-p501-600.xml\", \"FMEtitles-p601-692.xml\", \"FMEtitles-p601-692-add.xml\"]\n",
    "MANUALLY_ADDED_FILE = \"FMEtitles-added-manually.xml\"\n",
    "URI_CACHE = \"FMEtitles-uri-cache.xml\"\n",
    "EXIT_FILE = \"FMEtitles-merged.xml\"\n",
    "# Uri safer prevents already existing uri from being changed. Set to False ONLY IF you need to update an existing uris.\n",
    "URI_SAFER = True\n",
    "##################################################################\n",
    "\n",
    "\n",
    "\n",
    "class Article:\n",
    "    title = ''\n",
    "    start_title = ''\n",
    "    end_title = ''\n",
    "    filename = ''\n",
    "\n",
    "\n",
    "\n",
    "# Try to get uri from the cache for title with given page and pos\n",
    "def get_uri(title_page:str, title_pos:str) -> str:\n",
    "    global cache_root\n",
    "    for elem_uri in cache_root:\n",
    "        if elem_uri.tag == 'uri' and elem_uri.attrib['page'] == title_page and elem_uri.attrib['pos'] == title_pos:\n",
    "            return elem_uri.text\n",
    "    return ''\n",
    "# Cache given uri\n",
    "def cache_uri(title_page:str, title_pos:str, uri_str:str):\n",
    "    global cache_root\n",
    "    elem_uri = ElementTree.SubElement(cache_root, 'uri', {'page':title_page, 'pos':title_pos})\n",
    "    elem_uri.text = uri_str\n",
    "    xml_write(cache_root, WORK_DIR + TITLES_DIR + URI_CACHE)\n",
    "\n",
    "\n",
    "# Add article title and metadata to xml tree\n",
    "def add_article(article_local:Article, etree_root:ElementTree.Element, number:int):\n",
    "    page_str = article_local.filename[article_local.filename.find('-') + 1: article_local.filename.find('_')]\n",
    "    uri_cached = get_uri(page_str, article_local.start_title)\n",
    "    translitted = translit(article_local.title[:article_local.title.find(' ')], 'ru', True)\n",
    "    while translitted.find('/') != -1:\n",
    "        translitted = translitted[:translitted.find('/')] + '_' + translitted[translitted.find('/')+1:]\t\t# Prevent slash being counted as subfolder in further\n",
    "    uri_str = URI_PREFIX + \"article/\" + str(number) + \"_\" + translitted\n",
    "    if URI_SAFER and uri_cached != '':\n",
    "        uri_str = uri_cached\n",
    "    else:\n",
    "        cache_uri(page_str, article_local.start_title, uri_str)\n",
    "    elem_article = ElementTree.SubElement(etree_root, 'article', {'uri':uri_str})\n",
    "    elem_title = ElementTree.SubElement(elem_article, 'title')\n",
    "    elem_title.text = article_local.title\n",
    "    elem_title_meta = ElementTree.SubElement(elem_article, 'title-meta')\n",
    "    elem_title_file = ElementTree.SubElement(elem_title_meta, 'title-file')\n",
    "    elem_title_file.text = article_local.filename\n",
    "    elem_title_start = ElementTree.SubElement(elem_title_meta, 'title-start')\n",
    "    elem_title_start.text = str(int(article_local.start_title) + 1)\n",
    "    elem_title_end = ElementTree.SubElement(elem_title_meta, 'title-end')\n",
    "    elem_title_end.text = article_local.end_title\n",
    "\n",
    "\n",
    "# Check for existing uri list\n",
    "filenames_raw = get_filenames(WORK_DIR + TITLES_DIR)\n",
    "if not(URI_CACHE in filenames_raw):\n",
    "    root = ElementTree.Element('data')\n",
    "    xml_write(root, WORK_DIR + TITLES_DIR + URI_CACHE)\n",
    "cache_root = parse_xml(WORK_DIR + TITLES_DIR + URI_CACHE)\n",
    "\n",
    "\n",
    "# Collect all the articles\n",
    "print(\"Parsing main input files...\")\n",
    "articles_dict = {}\n",
    "for filename in tqdm(INPUT_FILES):\n",
    "    root = parse_xml(WORK_DIR + TITLES_DIR + filename)\n",
    "    for article in root:\n",
    "        title = get_xml_elem(article, 'title').text\n",
    "        elem = get_xml_elem(article, 'title-meta/title-file')\n",
    "        page = elem.text[elem.text.find('-')+1:elem.text.find('_')]\n",
    "        pos = get_xml_elem(article, 'title-meta/title-start').text\n",
    "        start = get_xml_elem(article, 'title-meta/title-start').text\n",
    "        end = get_xml_elem(article, 'title-meta/title-end').text\n",
    "        file = get_xml_elem(article, 'title-meta/title-file').text\n",
    "        num = (int(page), int(pos))\n",
    "        articles_dict[num] = {'title':title, 'file':file, 'start':start, 'end':end}\n",
    "\n",
    "# Same for manually added articles\n",
    "print(\"Parsing \\\"manual\\\" file...\")\n",
    "articles_dict_man = {}\n",
    "nums_list_man = []\n",
    "root = parse_xml(WORK_DIR + TITLES_DIR + MANUALLY_ADDED_FILE)\n",
    "for article in root:\n",
    "    title = get_xml_elem(article, 'title').text\n",
    "    elem = get_xml_elem(article, 'title-meta/title-file')\n",
    "    page = elem.text[elem.text.find('-')+1:elem.text.find('_')]\n",
    "    pos = get_xml_elem(article, 'title-meta/title-start').text\n",
    "    start = get_xml_elem(article, 'title-meta/title-start').text\n",
    "    end = get_xml_elem(article, 'title-meta/title-end').text\n",
    "    file = get_xml_elem(article, 'title-meta/title-file').text\n",
    "    num = (int(page), int(pos))\n",
    "    articles_dict_man[num] = {'title':title, 'file':file, 'start':start, 'end':end}\n",
    "    nums_list_man.append(num)\n",
    "\n",
    "\n",
    "# Sort keys and write articles accordingly\n",
    "root = ElementTree.Element('data')\n",
    "nums_list = sorted(list(i for i in articles_dict.keys()))\n",
    "print(\"Writing main articles...\")\n",
    "for num in tqdm(range(len(nums_list))):\n",
    "    article = Article()\n",
    "    article.title = articles_dict[nums_list[num]]['title']\n",
    "    article.start_title = articles_dict[nums_list[num]]['start']\n",
    "    article.end_title = articles_dict[nums_list[num]]['end']\n",
    "    article.filename = articles_dict[nums_list[num]]['file']\n",
    "    add_article(article, root, num + 1)\n",
    "for num in tqdm(range(len(nums_list_man))):\n",
    "    article = Article()\n",
    "    article.title = articles_dict_man[nums_list_man[num]]['title']\n",
    "    article.start_title = articles_dict_man[nums_list_man[num]]['start']\n",
    "    article.end_title = articles_dict_man[nums_list_man[num]]['end']\n",
    "    article.filename = articles_dict_man[nums_list_man[num]]['file']\n",
    "    add_article(article, root, num + 1 + len(nums_list))\n",
    "xml_write(root, WORK_DIR + EXIT_FILE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Парсер текстов статей\n",
    "\n",
    "По информации из указанного файла с заголовками вытаскивает в сыром виде тексты статей. Каждая статья помещается в свой .xml файл, с именем, содержащим номер статьи и первое слово из заголовка транслитом."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting articles info...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b7ebe05537a4e5fb44cf94900cef8b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing articles...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4e79203cd2f4599a2e12741ebf6e5a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Парсер текстов статей\n",
    "\n",
    "############################ VARS ################################\n",
    "TITLES_FILE = \"./results/FMEtitles-merged.xml\"\n",
    "PAGES_DIR = \"./matphys/rpages/\"\n",
    "EXIT_DIR = \"./results/FMEarticles/\"\n",
    "COMBINATIONS_CORR = {\n",
    "    'І' : 'I'\t\t# This teo are different!\n",
    "}\n",
    "##################################################################\n",
    "\n",
    "\n",
    "class Article:\n",
    "    start_file = ''\n",
    "    start_pos = 0\n",
    "    end_file = ''\n",
    "    end_pos = 0\n",
    "    text = ''\n",
    "    text_orig = ''\n",
    "    uri = ''\n",
    "    num = ''\n",
    "    title = ''\n",
    "    xml = ''\n",
    "\n",
    "    def get_text(self):\n",
    "        # Get filenames\n",
    "        filenames_raw_local = get_filenames(PAGES_DIR)\n",
    "        filenames_local = []\n",
    "        for filename_local in filenames_raw_local:\n",
    "            if filename_local[-4:] == \".mmd\":\n",
    "                filenames_local.append(filename_local)\n",
    "        if self.start_file == self.end_file:\n",
    "            with codecs.open(PAGES_DIR + self.start_file, 'r', 'utf-8') as f_in:\n",
    "                self.text += f_in.read()[self.start_pos:self.end_pos]\n",
    "        else:\n",
    "            with codecs.open(PAGES_DIR + self.start_file, 'r', 'utf-8') as f_in:\n",
    "                self.text += f_in.read()[self.start_pos:]\n",
    "            for page_local in range(int(self.start_file[3:self.start_file.find('_')]) + 1, int(self.end_file[3:self.end_file.find('_')])):\n",
    "                for filename_local in filenames_local:\n",
    "                    if int(filename_local[3:filename_local.find('_')]) == page_local:\n",
    "                        self.text += ' ' # Add a space to prevent word merging\n",
    "                        with codecs.open(PAGES_DIR + filename_local, 'r', 'utf-8') as f_in:\n",
    "                            self.text += f_in.read()\n",
    "            self.text += ' ' # Add a space to prevent word merging\n",
    "            with codecs.open(PAGES_DIR + self.end_file, 'r', 'utf-8') as f_in:\n",
    "                self.text += f_in.read()[:self.end_pos]\n",
    "        for comb_local in COMBINATIONS_CORR.keys():\n",
    "            while self.text.find(comb_local) != -1:\n",
    "                self.text = self.text[:self.text.find(comb_local)] + COMBINATIONS_CORR[comb_local] + self.text[self.text.find(comb_local) + len(comb_local):]\n",
    "        while self.text is not None and len(self.text) and self.text[0] in [' ', ',', '.', ':', ';', '-', '\\n', '\\r']:\n",
    "            self.text = self.text[1:]\n",
    "        while self.text is not None and len(self.text) and self.text[-1] in [' ', '\\n', '\\r']:\n",
    "            self.text = self.text[:-1]\n",
    "        self.text_orig = self.text\n",
    "        # Fix several capital symbols per word\n",
    "        word_left = 0\n",
    "        while word_left < len(self.text):\n",
    "            word_right = min(len(self.text), self.text.find(' ', word_left) if self.text.find(' ', word_left) != -1 else len(self.text))\n",
    "            word_right = min(word_right, self.text.find('\\n', word_left) if self.text.find('\\n', word_left) != -1 else len(self.text))\n",
    "            word_right = min(word_right, self.text.find('\\r', word_left) if self.text.find('\\r', word_left) != -1 else len(self.text))\n",
    "            word_right = min(word_right, self.text.find('-', word_left) if self.text.find('-', word_left) != -1 else len(self.text))\n",
    "            word_right = min(word_right, self.text.find('.', word_left) if self.text.find('.', word_left) != -1 else len(self.text))\n",
    "            word_str = self.text[word_left:word_right]\n",
    "            if word_str is not None and len(word_str) > 1 and not check_in_uri(self.text, word_left) and not check_in_formula(self.text, word_left) and not check_in_link(self.text, word_left):\n",
    "                word_str = word_str[0] + word_str[1:len(word_str)].lower()\n",
    "                self.text = self.text[:word_left] + word_str + self.text[word_right:]\n",
    "            word_left = word_right + 1\n",
    "\n",
    "    def make_xml(self):\n",
    "        self.get_text()\n",
    "\n",
    "        elem_article = ElementTree.Element(\"article\", {'uri':self.uri, 'alphabetic_pos':self.num})\n",
    "        elem_title = ElementTree.SubElement(elem_article, 'title')\n",
    "        elem_title.text = self.title\n",
    "        elem_author = ElementTree.SubElement(elem_article, 'authors')\n",
    "        elem_author.text = None\n",
    "        elem_title_short = ElementTree.SubElement(elem_article, 'title_short')\n",
    "        elem_title_short.text = None\n",
    "        elem_pages = ElementTree.SubElement(elem_article, 'pages')\n",
    "        elem_start = ElementTree.SubElement(elem_pages, 'start')\n",
    "        elem_start.text = self.start_file[3:self.start_file.find('_', 3)]\n",
    "        elem_end = ElementTree.SubElement(elem_pages, 'end')\n",
    "        elem_end.text = self.end_file[3:self.end_file.find('_', 3)]\n",
    "        elem_literature = ElementTree.SubElement(elem_article, 'literature')\n",
    "        elem_literature_orig = ElementTree.SubElement(elem_literature, 'literature_orig')\n",
    "        elem_literature_orig.text = None\n",
    "        elem_formulas_remote = ElementTree.SubElement(elem_article, 'formulas_main')\n",
    "        elem_formulas_remote.text = None\n",
    "        elem_formulas_inline = ElementTree.SubElement(elem_article, 'formulas_aux')\n",
    "        elem_formulas_inline.text = None\n",
    "        elem_relations = ElementTree.SubElement(elem_article, 'relations')\n",
    "        elem_relations.text = None\n",
    "        elem_text = ElementTree.SubElement(elem_article, 'text')\n",
    "        elem_text.text = self.text\n",
    "        elem_text_orig = ElementTree.SubElement(elem_article, 'text_orig')\n",
    "        elem_text_orig.text = self.text_orig\n",
    "\n",
    "        self.xml = prettify(elem_article)\n",
    "\n",
    "\n",
    "\n",
    "class Title:\n",
    "    text = ''\n",
    "    file = ''\n",
    "    start_pos = 0\n",
    "    end_pos = 0\n",
    "    uri = ''\n",
    "\n",
    "\n",
    "def get_titles_dict(etree_root:ElementTree.Element) -> dict:\n",
    "    titles_dict_local = {}\n",
    "    for elem_title in etree_root:\n",
    "        elem_uri = elem_title.attrib['uri']\n",
    "        elem_text = get_xml_elem(elem_title, 'title').text\n",
    "        elem_file = get_xml_elem(elem_title, 'title-meta/title-file').text\n",
    "        elem_page = int(elem_file[elem_file.find('-') + 1 : elem_file.find('_')])\n",
    "        elem_start_pos = int(get_xml_elem(elem_title, 'title-meta/title-start').text)\n",
    "        elem_end_pos = int(get_xml_elem(elem_title, 'title-meta/title-end').text)\n",
    "        titles_dict_local[(elem_page, elem_start_pos)] = Title()\n",
    "        titles_dict_local[(elem_page, elem_start_pos)].uri = elem_uri\n",
    "        titles_dict_local[(elem_page, elem_start_pos)].text = elem_text\n",
    "        titles_dict_local[(elem_page, elem_start_pos)].file = elem_file\n",
    "        titles_dict_local[(elem_page, elem_start_pos)].start_pos = elem_start_pos\n",
    "        titles_dict_local[(elem_page, elem_start_pos)].end_pos = elem_end_pos\n",
    "    return titles_dict_local\n",
    "\n",
    "\n",
    "def get_title(number:int, dict_with_titles:dict) -> Title:\n",
    "    out_title = Title()\n",
    "    titles_dict_keys = sorted(dict_with_titles.keys())\n",
    "    for p in range(len(titles_dict_keys)):\n",
    "        if p == number:\n",
    "            out_title = dict_with_titles[titles_dict_keys[p]]\n",
    "    return out_title\n",
    "\n",
    "\n",
    "root = parse_xml(TITLES_FILE)\n",
    "\n",
    "# Create articles list\n",
    "articles_list = []\n",
    "title = Title()\n",
    "titles_dict = get_titles_dict(root)\n",
    "print(\"Getting articles info...\")\n",
    "for i in tqdm(range(len(root))):\n",
    "    title = get_title(i, titles_dict)\n",
    "    if i:\n",
    "        articles_list[-1].end_file = title.file\n",
    "        articles_list[-1].end_pos = max(title.start_pos - 2, 0) # There is a shift for some reason\n",
    "    articles_list.append(Article())\n",
    "    articles_list[-1].uri = title.uri\n",
    "    articles_list[-1].num = str(i + 1)\n",
    "    articles_list[-1].title = title.text\n",
    "    articles_list[-1].start_file = title.file\n",
    "    articles_list[-1].start_pos = title.end_pos\n",
    "    articles_list[-1].end_file = title.file\n",
    "    with codecs.open(PAGES_DIR + title.file, 'r', 'utf-8') as f:\n",
    "        articles_list[-1].end_pos = len(f.read())\n",
    "\n",
    "# Parse texts themselves and write\n",
    "print(\"Parsing articles...\")\n",
    "for i in tqdm(range(len(articles_list))):\n",
    "    articles_list[i].make_xml()\n",
    "    with codecs.open(EXIT_DIR + '' + articles_list[i].uri[len(URI_PREFIX) + 8:] + '.xml', 'w', 'utf-8') as f:\n",
    "        f.write(articles_list[i].xml)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Проверка правописания в текстах\n",
    "\n",
    "## 5.1. Сканер\n",
    "\n",
    "Сканирует тексты из указанного диапазона статей и выносит все показавшиеся подозрительными слова в отдельный xml следующего формата:\n",
    "- Статья (имя файла в аттрибутах)\n",
    "  - Слово (позиция в тексте и флаги в аттрибутах)\n",
    "    - Исходный вариант\n",
    "    - Контекстная строка (размер задаётся в разделе параметров скрипта)\n",
    "    - Предложенная замена\n",
    "\n",
    "Предлагается два флага для определения дальнейшей \"судьбы\" слова: \"результат\" (0 -- исходное, 1 -- предложенное) и \"добавление в словарь\" (0 -- не добавлять, 1 -- добавить как есть, 2 -- перевести в нижний регистр и добавить (для первого слова в предложении), 3 -- сделать первую букву заглавной и добавить (для имён, случайно распознанных без заглавной); применяется к выбранному результату)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1656819e82d48939fdfc69135ee3dbb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases found: 115\n"
     ]
    }
   ],
   "source": [
    "# 5.1. Проверка правописания в текстах. Сканер.\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "EXIT_DIR = \"./matphys/\"\n",
    "CONTEXT_SIZE = 20\n",
    "START_ARTICLE = 701\n",
    "END_ARTICLE = 750\n",
    "# Flags for usual cases\n",
    "DEFAULT_RESULT_FLAG = '1'\n",
    "DEFAULT_ADD_TO_PWL_FLAG = '0'\n",
    "# Flags is name is detected\n",
    "\"\"\"NAME_RESULT_FLAG = '0'\n",
    "NAME_ADD_TO_PWL_FLAG = '1'\"\"\"\n",
    "# Cases that have to be overriden\n",
    "OVERRIDE_FORCE_CYRILLIC = {\n",
    "    'Ссср' : 'СССР',\n",
    "    'Церн' : 'ЦЕРН'\n",
    "}\n",
    "OVERRIDE_AS_IS = {\n",
    "}\n",
    "##################################################################\n",
    "\n",
    "\n",
    "spellcheck_dict_update()\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "#filenames = ['4_ABELEVA.xml']\n",
    "\n",
    "root = ElementTree.Element('data')\n",
    "\n",
    "total_wois = 0\n",
    "for filename in tqdm(filenames):\n",
    "    article_number = int(filename[:filename.find('_')])\n",
    "    if article_number < START_ARTICLE or article_number > END_ARTICLE:\n",
    "        continue\n",
    "\n",
    "    #print(f'{filename}: found ', end='')\n",
    "    article = parse_xml(ARTICLES_DIR + filename)\n",
    "    text = get_xml_elem(article, 'text')\n",
    "\n",
    "    #add_to_pwl(filename[filename.find('_')+1:filename.find('.xml')])\n",
    "\n",
    "    text_suggestions = do_spellcheck(text.text)\n",
    "    #print(len(text_suggestions.keys()))\n",
    "    total_wois += len(text_suggestions.keys())\n",
    "    if len(text_suggestions.keys()):\n",
    "        article = ElementTree.SubElement(root, 'article', {'filename': filename})\n",
    "        for pos in text_suggestions.keys():\n",
    "            #print(f'{pos}: {text_suggestions[pos][0]} -> {text_suggestions[pos][1]}')\n",
    "            local_result_flag = DEFAULT_RESULT_FLAG\n",
    "            local_add_to_pwl_flag = DEFAULT_ADD_TO_PWL_FLAG\n",
    "            # Process possible name case\n",
    "            '''if len(text_suggestions[pos][0]) >= 2 and len(text_suggestions[pos][1]) >= 2:\n",
    "                is_name_orig = re.match(r\"[А-ЯA-Z]\", text_suggestions[pos][0][0]) is not None and re.match(r\"[а-яa-z]\", text_suggestions[pos][0][1]) is not None\n",
    "                is_name_sugg = re.match(r\"[А-ЯA-Z]\", text_suggestions[pos][1][0]) is not None and re.match(r\"[а-яa-z]\", text_suggestions[pos][1][1]) is not None\n",
    "                if is_name_orig and is_name_sugg:\n",
    "                    local_result_flag = NAME_RESULT_FLAG\n",
    "                    local_add_to_pwl_flag = NAME_ADD_TO_PWL_FLAG'''\n",
    "            # Override specific cases\n",
    "            suggestion_text = text_suggestions[pos][1]\n",
    "            if title_handle_latin(text_suggestions[pos][0], COMBINATIONS_CORR_GLOBAL) in OVERRIDE_FORCE_CYRILLIC.keys():\n",
    "                suggestion_text = OVERRIDE_FORCE_CYRILLIC[title_handle_latin(text_suggestions[pos][0], COMBINATIONS_CORR_GLOBAL)]\n",
    "                local_result_flag = DEFAULT_RESULT_FLAG\n",
    "                local_add_to_pwl_flag = DEFAULT_ADD_TO_PWL_FLAG\n",
    "            if text_suggestions[pos][0] in OVERRIDE_AS_IS.keys():\n",
    "                suggestion_text = OVERRIDE_AS_IS[text_suggestions[pos][0]]\n",
    "                local_result_flag = DEFAULT_RESULT_FLAG\n",
    "                local_add_to_pwl_flag = DEFAULT_ADD_TO_PWL_FLAG\n",
    "            word = ElementTree.SubElement(article, 'word', {'pos': str(pos), 'result': local_result_flag, 'add_to_pwl': local_add_to_pwl_flag})\n",
    "            source = ElementTree.SubElement(word, 'source')\n",
    "            source.text = text_suggestions[pos][0]\n",
    "            context = ElementTree.SubElement(word, 'context')\n",
    "            context_string = text.text[max(0, pos - CONTEXT_SIZE):min(len(text.text), pos + len(text_suggestions[pos][0]) + CONTEXT_SIZE)]\n",
    "            while context_string.find('\\n') != -1:\n",
    "                context_string = context_string[:context_string.find('\\n')] + '\\\\n' + context_string[context_string.find('\\n')+1:]\n",
    "            while context_string.find('\\r') != -1:\n",
    "                context_string = context_string[:context_string.find('\\r')] + '\\\\r' + context_string[context_string.find('\\r')+1:]\n",
    "            context.text = context_string\n",
    "            suggestion = ElementTree.SubElement(word, 'suggestion')\n",
    "            suggestion.text = suggestion_text\n",
    "\n",
    "print(\"Total cases found:\", total_wois)\n",
    "\n",
    "\n",
    "with codecs.open(EXIT_DIR + f'FMEspellcheck-a{START_ARTICLE}-{END_ARTICLE}.xml', 'w', 'utf-8') as f:\n",
    "    f.write(prettify(root))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2. Пополнение словаря\n",
    "\n",
    "Добавляет отмеченные флагом \"добавление в словарь\" слова из всех файлов в директории спеллчека\n",
    "- Учитывается, было ли выбрано оригинальное слово или исправленное флагом \"результат\".\n",
    "- Словарь сортируется по алфавиту при каждом запуске.\n",
    "- Дубликаты удаляются при каждом запуске (символы разного регистра одинаковыми не считаются).\n",
    "- Слова добавленные вручную при запуске не удаляются.\n",
    "\n",
    "Чтобы объединить ваш словарь с другим, скопируйте и вставьте всё содержимое нового словаря в ваш, после чего запустите скрипт. Дубликаты будут удалены, итоговый словарь будет отсортирован."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for PWL additions...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/11 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad213a496ecb4f75b16dd4fcefe56a91"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PWL...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2782 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2b15880616e4caead6ed073fb0f2830"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing PWL...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1878 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3bfdf54f76c44c6bb602644f82c28015"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5.2. Проверка правописания в текстах. Пополнение словаря.\n",
    "\n",
    "############################ VARS ################################\n",
    "SPELLCHECK_DIR = \"./results/FMEspellcheck/\"\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Read PWL and form word list\n",
    "with codecs.open(PERSONAL_WORD_LIST, 'r', 'utf-8') as f:\n",
    "    PWL_text = f.read()\n",
    "additions = [i.strip() for i in PWL_text.split('\\n')]\n",
    "while '' in additions:\n",
    "    additions.remove('')\n",
    "PWL_text = ''\n",
    "\n",
    "# Read all spellcheck outputs and create additions list\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(SPELLCHECK_DIR)\n",
    "\n",
    "print(\"Scanning for PWL additions...\")\n",
    "for filename in tqdm(filenames):\n",
    "    root = parse_xml(SPELLCHECK_DIR + filename)\n",
    "    for article in root:\n",
    "        if article.tag == \"article\":\n",
    "            for word in article:\n",
    "                if word.tag == \"word\" and word.attrib[\"add_to_pwl\"] != '0':\n",
    "                    word_text = get_xml_elem(word, 'suggestion').text.strip() if word.attrib[\"result\"] == '1' else get_xml_elem(word, 'source').text.strip()\n",
    "                    if word.attrib[\"add_to_pwl\"] == '2':\n",
    "                        additions.append(word_text.lower())\n",
    "                    elif word.attrib[\"add_to_pwl\"] == '3' and len(word_text):\n",
    "                        additions.append(word_text[0].upper() + word_text[1:] if len(word_text) > 1 else '')\n",
    "                    else:\n",
    "                        additions.append(word_text)\n",
    "\n",
    "# Make new PWL list and sort it\n",
    "print(\"Processing PWL...\")\n",
    "PWL_list_new = []\n",
    "for word in tqdm(additions):\n",
    "    # Check (and correct) that the word has no latin and cyrillic letters at the same time\n",
    "    if word is not None and len(word):\n",
    "        exist_from_comb = False\n",
    "        exist_rus = False\n",
    "        for i in range(len(word)):\n",
    "            exist_from_comb = True if word[i] in COMBINATIONS_CORR_GLOBAL.keys() else exist_from_comb\n",
    "            exist_rus = True if re.match(r\"[А-Яа-я]\", word[i]) is not None else exist_rus\n",
    "        if exist_from_comb and exist_rus:\n",
    "            for i in range(len(word)):\n",
    "                word = word[:i] + (COMBINATIONS_CORR_GLOBAL[word[i]] if word[i] in COMBINATIONS_CORR_GLOBAL.keys() else word[i]) + (word[i+1:] if (i + 1) <= len(word) else '')\n",
    "    # Append word to the list if not present yet\n",
    "    if word is not None and len(word) and not word in PWL_list_new:\n",
    "        PWL_list_new.append(word)\n",
    "PWL_list_new.sort()\n",
    "\n",
    "# Write PWL\n",
    "print(\"Writing PWL...\")\n",
    "for word in tqdm(PWL_list_new):\n",
    "    PWL_text = PWL_text + word + '\\n'\n",
    "with codecs.open(PERSONAL_WORD_LIST, 'w', 'utf-8') as f:\n",
    "    f.write(PWL_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3. Подстановка исправленной орфографии\n",
    "\n",
    "Подставляет в исходный текст исправленные слова или оригиналы, в зависимости от установленного флага \"результат\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/11 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9bdf56381fd44cebc3692b026668dc3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5.3. Проверка правописания в текстах. Подстановка исправленной орфографии.\n",
    "\n",
    "############################ VARS ################################\n",
    "SPELLCHECK_DIR = \"./results/FMEspellcheck/\"\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(SPELLCHECK_DIR)\n",
    "\n",
    "for filename_data in tqdm(filenames):\n",
    "    # Parse articles corrections file\n",
    "    root_data = parse_xml(SPELLCHECK_DIR + filename_data)\n",
    "    for article in root_data:\n",
    "        if article.tag == 'article':\n",
    "            filename_article = article.attrib['filename']\n",
    "            root_article = parse_xml(ARTICLES_DIR + filename_article)\n",
    "            text = get_xml_elem(root_article, 'text')\n",
    "            # Parse corrections in one article\n",
    "            words = []\n",
    "            for word in article:\n",
    "                if word.tag == 'word':\n",
    "                    pos = int(word.attrib['pos'])\n",
    "                    len_src = len(get_xml_elem(word, 'source').text)\n",
    "                    word_text = get_xml_elem(word, 'suggestion').text if word.attrib['result'] == '1' else get_xml_elem(word, 'source').text\n",
    "                    words.append((pos, word_text, len_src))\n",
    "            words.sort(reverse=True)\n",
    "            # Apply corrections\n",
    "            for word in words:\n",
    "                pos = word[0]\n",
    "                len_src = word[2]\n",
    "                word_text = word[1]\n",
    "                text.text = text.text[:pos] + word_text + (text.text[pos+len_src:] if pos+len_src <= len(text.text) else '')\n",
    "            # Write corrected article xml\n",
    "            with codecs.open(ARTICLES_DIR + filename_article, 'w', 'utf-8') as f:\n",
    "                f.write(prettify(root_article))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Парсер авторов статьи\n",
    "\n",
    "Ищет в конце текста статей конструкции типа ` [Xxxx]. [Xxxx]. [Xxxx]` или ` [Xxxx].[Xxxx]. [Xxxx]` и интерпретирует её как автора статьи."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d4c7b6eb76146a4b03d63cd9f43c9f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6. Парсинг авторов статьи\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "COMBINATIONS_CORR = dict_merge(COMBINATIONS_CORR_UNICODE, {\n",
    "    'II' : 'П'\n",
    "})\n",
    "LOCAL_DICT = {'0':'О', '3':'З', '6':'б'}\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    article = parse_xml(ARTICLES_DIR + filename)\n",
    "    textelem = get_xml_elem(article, 'text')\n",
    "    text = textelem.text\n",
    "    authors = get_xml_elem(article, 'authors')\n",
    "\n",
    "    auth_start = 1\n",
    "    auth_list = []\n",
    "    while auth_start and text is not None:\n",
    "        # Find first non-space from the end\n",
    "        while text[-1] == ' ' or text[-1] == '\\n' or text[-1] == '\\r':\n",
    "            text = text[:-1]\n",
    "\n",
    "        auth_start = 0\n",
    "        # Try recognize\n",
    "        first_space = max(text.rfind(' ', 0, len(text)), text.rfind('\\n', 0, len(text)), text.rfind('\\r', 0, len(text)))\n",
    "        second_space = max(text.rfind(' ', 0, first_space), text.rfind('\\n', 0, first_space), text.rfind('\\r', 0, first_space))\n",
    "        third_space = max(text.rfind(' ', 0, second_space), text.rfind('\\n', 0, second_space), text.rfind('\\r', 0, second_space))\n",
    "        if first_space >= 0 and text[first_space-1] == '.' and second_space >= 0:\n",
    "            if text.find('.', second_space, first_space-1) != -1: # If there's no space between initials\n",
    "                third_space = second_space\n",
    "                second_space = first_space\n",
    "            if text[second_space-1] == '.' and third_space >= 0:\n",
    "                # Check if first letters of each word are capitals\n",
    "                keep = text\n",
    "                for comb in LOCAL_DICT.keys():\n",
    "                    while text[third_space+1:].find(comb) != -1:\n",
    "                        text = text[:third_space+1+text[third_space+1:].find(comb)] + LOCAL_DICT[comb] + text[third_space+2+text[third_space+1:].find(comb):]\n",
    "                if re.match(r\"[A-ZА-ЯІ]\", text[first_space+1]) is not None and re.match(r\"[A-ZА-ЯІ]\", text[second_space+1]) is not None and re.match(r\"[A-ZА-ЯІ]\", text[third_space+1]) is not None:\n",
    "                    auth_start = third_space + 1\n",
    "                text = keep\n",
    "\n",
    "        if auth_start: # Suggest that an article cannot consist of author only and therefore auth_start should be > 0\n",
    "            #print(article.attrib['uri'], author_text)\n",
    "            author_text = text[auth_start:]\n",
    "            if author_text[author_text.find('.')+1] != ' ': # Add space if there's no one between initials\n",
    "                author_text = author_text[:author_text.find('.')+1] + ' ' + author_text[author_text.find('.')+1:]\n",
    "            if author_text[-1] == '.' or author_text[-1] == ',':\n",
    "                author_text = author_text[:-1]\n",
    "            # convert wrong symbols\n",
    "            for comb in dict_merge(COMBINATIONS_CORR, LOCAL_DICT).keys():\n",
    "                while author_text.find(comb) != -1:\n",
    "                    author_text = author_text[:author_text.find(comb)] + dict_merge(COMBINATIONS_CORR, LOCAL_DICT)[comb] + author_text[author_text.find(comb) + len(comb):]\n",
    "\n",
    "            auth_list.append(author_text)\n",
    "            text = text[:auth_start]\n",
    "\n",
    "    # add authors, reverse their order to alphabetic\n",
    "    for auth in reversed(auth_list):\n",
    "        author = ElementTree.SubElement(authors, 'author')\n",
    "        author.text = auth\n",
    "\n",
    "    textelem.text = text\n",
    "    with codecs.open(ARTICLES_DIR + filename, 'w', 'utf-8') as f:\n",
    "        f.write(prettify(article))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Парсер литературы\n",
    "\n",
    "После извлечения авторов статьи в конце за текстом статьи присутствует только строчка литературы, если вообще присутствует. Поэтому ищется и извлекается фрагмент начиная с \"`Лит.:`\". Он разделяется на сегменты по \"`[num]`\", а сегменты на подфрагменты по запятым. Общий вид сегмента полагается следующим: \"`[Авторы (возможно несколько, определяются по наличию инициалов в конце подфрагмента)], Название (возможно содержит запятые), Номер тома (может отсутствовать), [Информация об издании (может частично или полностью отсутствовать)], Год, [Прочее (главы, страницы и прочее, может отсутствовать)];`\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56c04789c992464ebf8fefe50dc5b544"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 7. Парсинг литературы\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "COMBINATIONS_CORR_LOCAL = dict_merge(dict_merge(COMBINATIONS_CORR_ALPHABET, COMBINATIONS_CORR_UNICODE), {'J':'Л'})\n",
    "##################################################################\n",
    "\n",
    "\n",
    "class Unit:\n",
    "    authors = []\n",
    "    title = \"\"\n",
    "    publication = \"\"\n",
    "    year = \"\"\n",
    "    other = \"\"\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    article = parse_xml(ARTICLES_DIR + filename)\n",
    "    textelem = get_xml_elem(article, 'text')\n",
    "    text = textelem.text\n",
    "    literature = get_xml_elem(article, 'literature')\n",
    "    literature_orig = get_xml_elem(literature, 'literature_orig')\n",
    "\n",
    "    if textelem.text is not None and len(textelem.text):\n",
    "        #Find literature start position and extract if present\n",
    "        for key in COMBINATIONS_CORR_LOCAL.keys():\n",
    "            while text.find(key) != -1:\n",
    "                text = text[:text.find(key)] + COMBINATIONS_CORR_LOCAL[key] + text[text.find(key)+1:]\n",
    "        text = text.upper()\n",
    "        lit_pos = text.rfind('\\nЛИТ.: ')\n",
    "        lit_pos = text.rfind('\\rЛИТ.: ') if lit_pos == -1 else lit_pos\n",
    "        lit_pos = text.rfind(' ЛИТ.: ') if lit_pos == -1 else lit_pos\n",
    "        if lit_pos != -1:\n",
    "            literature_orig.text = textelem.text[lit_pos:]\n",
    "            while literature_orig.text[0] in [' ', '\\n', '\\r']:\n",
    "                literature_orig.text = literature_orig.text[1:]\n",
    "            textelem.text = textelem.text[:lit_pos]\n",
    "            while textelem.text[-1] in [' ', '\\n', '\\r']:\n",
    "                textelem.text = textelem.text[:-1]\n",
    "\n",
    "\n",
    "            # Parse literature string\n",
    "            text = literature_orig.text\n",
    "            units = []\n",
    "            num = 1\n",
    "            while text.find('['+str(num)+']') != -1:\n",
    "                units.append(text[text.find('['+str(num)+']')+len('['+str(num)+']'):(text.find('['+str(num+1)+']') if text.find('['+str(num+1)+']') != -1 else len(text))])\n",
    "                num += 1\n",
    "            for unit in units:\n",
    "                logical_parts = Unit()\n",
    "                logical_parts.authors.clear()\n",
    "                subunits = unit.split(',')\n",
    "                while '' in subunits:\n",
    "                    subunits.remove('')\n",
    "                pos_last_auth = -1\n",
    "                pos_last_title = -1\n",
    "                pos_thome = -1\n",
    "                pos_transl = -1\n",
    "                pos_pub_num = -1\n",
    "                pos_pub_place = -1\n",
    "                pos_year = -1\n",
    "\n",
    "\n",
    "                # Define positions of most common pats of literature string\n",
    "                for i in range(len(subunits)):\n",
    "                    text = subunits[i]\n",
    "                    while text[-1] in [' ', '\\n', '\\r', ';']:\n",
    "                        text = text[:-1]\n",
    "                    while text[0] in [' ', '\\n', '\\r']:\n",
    "                        text = text[1:]\n",
    "                    subunits[i] = text\n",
    "\n",
    "                    if pos_last_auth + 1 == i: # Recognize authors\n",
    "                        keep = text\n",
    "                        pos_initials = 0\n",
    "                        for j in range(len(text)):\n",
    "                            if text[j] in COMBINATIONS_CORR_UNICODE:\n",
    "                                text = text[:j] + COMBINATIONS_CORR_UNICODE[text[j]] + text[j+1:]\n",
    "                        if text[-1] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-2]) is not None and text[-3] == ' ' and text[-4] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-5]) is not None:\n",
    "                            # \"X. X.\"\n",
    "                            pos_last_auth = i\n",
    "                            pos_initials = -5\n",
    "                        elif text[-1] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-2]) is not None and text[-3] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-4]) is not None:\n",
    "                            # \"X.X.\"\n",
    "                            pos_last_auth = i\n",
    "                            text = text[:-2] + ' ' + text[-2:]\n",
    "                            pos_initials = -5\n",
    "                        elif text[-1] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-2]) is not None:\n",
    "                            # \"X.\"\n",
    "                            pos_last_auth = i\n",
    "                            pos_initials = -2\n",
    "                        else: # Title starts\n",
    "                            text = keep\n",
    "                        # If correct\n",
    "                        if pos_last_auth == i:\n",
    "                            surname = text[:pos_initials]\n",
    "                            while surname.find(' ') != -1:\n",
    "                                surname = surname[:surname.find(' ')] + surname[surname.find(' ')+1:]\n",
    "                            text = surname + ' ' + text[pos_initials:]\n",
    "                            j = 1\n",
    "                            while j < len(text):\n",
    "                                if re.match(r\"[А-ЯA-Z]\", text[j]) is not None and re.match(r\"[а-яa-z]\", text[j-1]) is not None:\n",
    "                                    text = text[:j] + ' ' + text[j:]\n",
    "                                    j = 1\n",
    "                                else:\n",
    "                                    j += 1\n",
    "                            subunits[i] = text\n",
    "                    else:\n",
    "                        if pos_thome == -1: # Recognize thome\n",
    "                            keep = text\n",
    "                            for j in range(len(text)):\n",
    "                                if text[j] in COMBINATIONS_CORR_GLOBAL:\n",
    "                                    text = text[:j] + COMBINATIONS_CORR_GLOBAL[text[j]] + text[j+1:]\n",
    "                            if text.upper().find('Т.') != -1:\n",
    "                                pos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "                                pos_thome = i\n",
    "                            text = keep\n",
    "                        if pos_transl == -1: # Recognize publication number\n",
    "                            keep = text\n",
    "                            for j in range(len(text)):\n",
    "                                if text[j] in COMBINATIONS_CORR_GLOBAL:\n",
    "                                    text = text[:j] + COMBINATIONS_CORR_GLOBAL[text[j]] + text[j+1:]\n",
    "                            if text.upper().find('ПЕР.') != -1:\n",
    "                                pos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "                                pos_transl = i\n",
    "                            text = keep\n",
    "                        if pos_pub_num == -1: # Recognize publication number\n",
    "                            keep = text\n",
    "                            for j in range(len(text)):\n",
    "                                if text[j] in COMBINATIONS_CORR_GLOBAL:\n",
    "                                    text = text[:j] + COMBINATIONS_CORR_GLOBAL[text[j]] + text[j+1:]\n",
    "                            if text.upper().find('ИЗД.') != -1:\n",
    "                                pos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "                                pos_pub_num = i\n",
    "                            text = keep\n",
    "                        if pos_pub_place == -1: # Recognize publication place\n",
    "                            keep = text\n",
    "                            for j in range(len(text)):\n",
    "                                if text[j] in COMBINATIONS_CORR_GLOBAL:\n",
    "                                    text = text[:j] + COMBINATIONS_CORR_GLOBAL[text[j]] + text[j+1:]\n",
    "                            if text.upper() in ['М.', 'Л.', 'СПБ.', 'М.Л.', 'Л.М.', 'М.СПБ.', 'СПБ.М.']:\n",
    "                                pos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "                                pos_pub_place = i\n",
    "                            text = keep\n",
    "                        # If correct\n",
    "                        if pos_last_auth != i and (pos_thome == i or pos_pub_num == i or pos_pub_place == i):\n",
    "                            for j in range(len(text)):\n",
    "                                if text[j] in COMBINATIONS_CORR_UNICODE:\n",
    "                                    subunits[i] = text[:j] + COMBINATIONS_CORR_UNICODE[text[j]] + text[j+1:]\n",
    "\n",
    "                        if pos_year == -1 and len(text) >= 4: # Recognize year\n",
    "                            numbers = ['0','1','2','3','4','5','6','7','8','9']\n",
    "                            j = 0\n",
    "                            for j in range(len(text) - 3):\n",
    "                                if text[j] in numbers and text[j+1] in numbers and text[j+2] in numbers and text[j+3] in numbers:\n",
    "                                    pos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "                                    pos_year = i\n",
    "                                    break\n",
    "                            # if correct\n",
    "                            if pos_year == i:\n",
    "                                subunits[i] = text[j:j+4]\n",
    "\n",
    "\n",
    "                # Extract info from literature string using positions defined above\n",
    "                for i in range(len(subunits)):\n",
    "                    text = subunits[i]\n",
    "                    if pos_last_auth >= i: # Author\n",
    "                        logical_parts.authors.append(text)\n",
    "                    elif pos_last_auth < i <= pos_last_title: # Title\n",
    "                        logical_parts.title = logical_parts.title + ('' if len(logical_parts.title) == 0 else ', ') + text\n",
    "                    elif pos_year == i: # Year\n",
    "                        logical_parts.year = logical_parts.year + ('' if len(logical_parts.year) == 0 else ', ') + text\n",
    "                    elif ((pos_pub_num <= i and pos_pub_num != -1) or (pos_pub_place <= i and pos_pub_place != -1) or (pos_transl <= i and pos_transl != -1) or (pos_thome + 1 <= i and pos_thome != -1)) and pos_year > i: # Publication\n",
    "                        logical_parts.publication = logical_parts.publication + ('' if len(logical_parts.publication) == 0 else ', ') + text\n",
    "                    else: # Other\n",
    "                        logical_parts.other = logical_parts.other + ('' if len(logical_parts.other) == 0 else ', ') + text\n",
    "\n",
    "\n",
    "                # Debug section\n",
    "                \"\"\"print('\\n', filename, unit)\n",
    "                print('authors:', logical_parts.authors)\n",
    "                print('title:', logical_parts.title)\n",
    "                print('publication:', logical_parts.publication)\n",
    "                print('year:', logical_parts.year)\n",
    "                print('other:', logical_parts.other)\n",
    "                print(pos_last_auth, pos_last_title, pos_thome, pos_transl, pos_pub_num, pos_pub_place, pos_year)\"\"\"\n",
    "\n",
    "\n",
    "                # Add literature unit\n",
    "                unit = ElementTree.SubElement(literature, \"unit\")\n",
    "                for auth_str in logical_parts.authors:\n",
    "                    author = ElementTree.SubElement(unit, \"author\")\n",
    "                    author.text = auth_str\n",
    "                title = ElementTree.SubElement(unit, \"title\")\n",
    "                title.text = logical_parts.title\n",
    "                publication = ElementTree.SubElement(unit, \"publication\")\n",
    "                publication.text = logical_parts.publication\n",
    "                year = ElementTree.SubElement(unit, \"year\")\n",
    "                year.text = logical_parts.year\n",
    "                other = ElementTree.SubElement(unit, \"other\")\n",
    "                other.text = logical_parts.other\n",
    "\n",
    "\n",
    "            # Write xml\n",
    "            with codecs.open(ARTICLES_DIR + filename, 'w', 'utf-8') as f:\n",
    "                f.write(prettify(article))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Парсер формул\n",
    "\n",
    "Выносит из текстов ранее подготовленных xml-файлов статей сначала выносные, а затем строчные формулы, оставляя на их месте ссылку внутри математического окружения.\n",
    "\n",
    "Минимальная длина в символах, которой должна обладать строчная формула, настраивается."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4ae76e410544b5f9ce3ef91edb3476e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found main formulas: 9959\n",
      "Found auxiliary formulas: 38421\n"
     ]
    }
   ],
   "source": [
    "# 8. Парсер формул\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "MIN_INLINE_LEN = 0\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "\n",
    "n_main = 0\n",
    "n_aux = 0\n",
    "for filename in tqdm(filenames):\n",
    "    article = parse_xml(ARTICLES_DIR + filename)\n",
    "    #print('REMOTES: ' + article.attrib['uri'])\n",
    "    text = get_xml_elem(article, 'text')\n",
    "    formulas_main = get_xml_elem(article, 'formulas_main')\n",
    "    formulas_aux = get_xml_elem(article, 'formulas_aux')\n",
    "\n",
    "    # Get main formulas\n",
    "    pos_find = 0\n",
    "    pos_start = 0\n",
    "    pos_end = 0\n",
    "    n = 1\n",
    "    while text.text is not None and text.text.find('\\\\[', pos_find) != -1:\n",
    "        pos_start = text.text.find('\\\\[', pos_find) + 2\n",
    "        pos_end = text.text.find('\\\\]', pos_start)\n",
    "        while text.text[pos_start] == '\\n':\n",
    "            pos_start += 1\n",
    "        while text.text[pos_end-1] == '\\n':\n",
    "            pos_end -= 1\n",
    "        pos_find = pos_start\n",
    "        uri = URI_PREFIX + 'formula/main' + article.attrib['uri'][article.attrib['uri'].rfind('/', 0, article.attrib['uri'].find('_')):article.attrib['uri'].find('_')+1] + str(n) + article.attrib['uri'][article.attrib['uri'].find('_'):]\n",
    "        n += 1\n",
    "        formula = ElementTree.SubElement(formulas_main, 'formula', {'uri':uri})\n",
    "        formula.text = text.text[pos_start:pos_end]\n",
    "        text.text = text.text[:pos_start] + 'URI[[' + uri + ']]/URI' + text.text[pos_end:]\n",
    "    n_main += n\n",
    "\n",
    "    # Get auxiliary formulas\n",
    "    pos_find = 0\n",
    "    pos_start = 0\n",
    "    pos_end = 0\n",
    "    cnt = 0\n",
    "    n = 1\n",
    "    # Count dollar symbols\n",
    "    while text.text is not None and text.text.find('$', pos_find) != -1:\n",
    "        pos_find = text.text.find('$', pos_find) + 1\n",
    "        cnt += 1\n",
    "    # If cnt is not even assume that first one is garbage from title\n",
    "    pos_find = 0\n",
    "    if cnt % 2:\n",
    "        pos_find = text.text.find('$', pos_find)\n",
    "        text.text = text.text[:pos_find] + '#' + text.text[pos_find+1:]\n",
    "    while text.text is not None and text.text.find('$', pos_find) != -1:\n",
    "        pos_start = text.text.find('$', pos_find) + 1\n",
    "        pos_end = text.text.find('$', pos_start)\n",
    "        if not check_in_uri(text.text, pos_start) and not check_in_uri(text.text, pos_end):\n",
    "            while text.text[pos_start] == '\\n':\n",
    "                pos_start += 1\n",
    "            while text.text[pos_end-1] == '\\n':\n",
    "                pos_end -= 1\n",
    "            pos_find = pos_start\n",
    "            if pos_end - pos_start >= MIN_INLINE_LEN:\n",
    "                uri = URI_PREFIX + 'formula/aux' + article.attrib['uri'][article.attrib['uri'].rfind('/', 0, article.attrib['uri'].find('_')):article.attrib['uri'].find('_')+1] + str(n) + article.attrib['uri'][article.attrib['uri'].find('_'):]\n",
    "                n += 1\n",
    "                formula = ElementTree.SubElement(formulas_aux, 'formula', {'uri':uri})\n",
    "                formula.text = text.text[pos_start:pos_end]\n",
    "                text.text = text.text[:pos_start] + 'URI[[' + uri + ']]/URI' + text.text[pos_end:]\n",
    "            pos_find = text.text.find('$', pos_find) + 1\n",
    "        else:\n",
    "            pos_find = pos_end + 1\n",
    "    n_aux += n\n",
    "\n",
    "    with codecs.open(ARTICLES_DIR + filename, 'w', 'utf-8') as f:\n",
    "        f.write(prettify(article))\n",
    "\n",
    "print(\"Found main formulas:\", n_main)\n",
    "print(\"Found auxiliary formulas:\", n_aux)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.1. Вынос формул\n",
    "\n",
    "Выносит все формулы в отдельный файл с указанием типа для возможной последующей обработки."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f009af1e859d40ccb0743796dcab58a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8.1. Вынос формул\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "EXIT_FILE = \"./results/FMEformulas.xml\"\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "\n",
    "\n",
    "formulas = ElementTree.Element('formulas')\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    root = parse_xml(ARTICLES_DIR + filename)\n",
    "    fmain = get_xml_elem(root, 'formulas_main')\n",
    "    faux = get_xml_elem(root, 'formulas_aux')\n",
    "\n",
    "    for formula in fmain:\n",
    "        formulas.append(formula)\n",
    "    for formula in faux:\n",
    "        formulas.append(formula)\n",
    "\n",
    "with codecs.open(EXIT_FILE, 'w', 'utf-8') as f:\n",
    "    f.write(prettify(formulas))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.2. Проверка формул\n",
    "\n",
    "Случайным образом выбирает 20 случайных формул (из случайных статей) и вставляет их в математическое окружение Markdown для визуальной проверки"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# 8.2. Проверка формул\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "EXIT_FILE = \"./matphys/FMEformulas_check.md\"\n",
    "NUMBER = 20\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "\n",
    "\n",
    "file = ''\n",
    "\n",
    "i = 0\n",
    "while i < NUMBER:\n",
    "    root = parse_xml(ARTICLES_DIR + filenames[randint(0, len(filenames)-1)])\n",
    "\n",
    "    # Get all the info from article\n",
    "    fmain = get_xml_elem(root, 'formulas_main')\n",
    "    start = get_xml_elem(root, 'pages/start').text\n",
    "\n",
    "\n",
    "    # if there's no formulas in the article try another one\n",
    "    total_num = 0\n",
    "    for formula in fmain:\n",
    "        total_num += 1\n",
    "    if not total_num:\n",
    "        continue\n",
    "    i += 1\n",
    "\n",
    "    num = randint(0, 100) % total_num\n",
    "\n",
    "    formula = fmain[num].text\n",
    "\n",
    "    file += f'{i}. Статья: {root.attrib[\"uri\"]}, Начало на стр. {start}, формула {num + 1}:\\n$${formula}$$\\n'\n",
    "\n",
    "with codecs.open(EXIT_FILE, 'w', 'utf-8') as f:\n",
    "    f.write(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Парсер ссылок типа \"смотри также\"\n",
    "\n",
    "Ищет в тексте ссылки начинающиеся на `\"см. [другие опциональные вводные слова]\"` и пытается найти соответствующие им статьи в энциклопедии."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing search base...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb6b5671fe224903bac84a95e232c074"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching relations in articles...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cd25de1cf174378b005d5c7bfe7bd0f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations found in total: 1969\n"
     ]
    }
   ],
   "source": [
    "# 9. Парсер ссылок типа \"смотри также\"\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "STRICT_SEQUENCING = False  # DEFAULT: False; Links with changed order of words will also be found if False\n",
    "BRUTE_FORCE_MODE = False  # Maximum amount of links to find, but takes more time (very slow, use with multiprocessing)\n",
    "USE_MULTIPROCESSING = False  # WARNING: Does not work inside Jupyter!!!; Significantly speeds up scanning process\n",
    "KEEP_FREE = 0  # Make multiprocessing keep free a specified amount of logic processors, if needed.\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "from lib import *\n",
    "\n",
    "number_of_processors = max(1, os.cpu_count() - KEEP_FREE)\n",
    "matches_list = []\n",
    "\n",
    "\n",
    "# Find previous word beginning - 1 from given position\n",
    "def find_prev_space(work_text: str, start_pos: int) -> int:\n",
    "    new_pos = 0\n",
    "    for sym in ' \\n\\r':\n",
    "        new_pos_s = work_text.rfind(sym, 0, start_pos)\n",
    "        new_pos = max(new_pos, (0 if new_pos_s == -1 else new_pos_s))\n",
    "    return new_pos\n",
    "\n",
    "\n",
    "# Find next word ending + 1 from given position\n",
    "def find_next_space(work_text: str, start_pos: int) -> int:\n",
    "    new_pos = len(work_text)\n",
    "    for sym in ' \\n\\r':\n",
    "        new_pos_s = work_text.find(sym, start_pos + 1)\n",
    "        new_pos = min(new_pos, (len(work_text) if new_pos_s == -1 else new_pos_s))\n",
    "    return new_pos\n",
    "\n",
    "\n",
    "# Try to find a matching title to the given one\n",
    "def find_matching_title(sequence_list: list) -> (bool, bool, bool, int):\n",
    "    match_possible_local = False\n",
    "    global matches_list\n",
    "    global titles_list\n",
    "    # Check with titles from matches list\n",
    "    for list_pos in range(len(titles_list)):\n",
    "        if list_pos in matches_list:\n",
    "            match_local = False\n",
    "            title_sel = titles_list[list_pos]\n",
    "            # No match if the title is shorter than the sequence\n",
    "            if len(title_sel) >= len(sequence_list):\n",
    "                title_positions = [p for p in range(len(title_sel))]\n",
    "                len_diff = len(title_positions) - len(sequence_list)\n",
    "                # Check each word in given sequence\n",
    "                for p in range(len(sequence_list)):\n",
    "                    match_current = False\n",
    "                    # Check if any word form title matches the current word from the sequence\n",
    "                    for position in title_positions:\n",
    "                        seq_item = sequence_list[p]\n",
    "                        if STRICT_SEQUENCING:\n",
    "                            position = title_positions[0]  # Force straight title words sequencing\n",
    "                        title_item = title_sel[position]\n",
    "                        # Try to negotiate word endings\n",
    "                        seq_len = len(seq_item)\n",
    "                        title_len = len(title_item)\n",
    "                        max_len = max(len(title_item), len(seq_item))\n",
    "                        if seq_len > 5 and title_len > 5 and abs(seq_len - title_len) <= 1:\n",
    "                            cut_len = min(max_len - 3, 4)\n",
    "                            seq_item = seq_item[:cut_len]\n",
    "                            title_item = title_item[:cut_len]\n",
    "                        # Found matching word\n",
    "                        if seq_item == title_item:\n",
    "                            match_current = True\n",
    "                            title_positions.remove(position)  # Each word from title can be met only once\n",
    "                            break\n",
    "                    # If no matching word found then there is no local match\n",
    "                    if not match_current:\n",
    "                        break\n",
    "                match_local = len(title_positions) == len_diff  # Match found for every word in sequence\n",
    "                match_possible_local = match_possible_local or match_local\n",
    "            # Remove the title position from matches list if no match for sure\n",
    "            if not match_local:\n",
    "                matches_list.remove(list_pos)\n",
    "            # Exact match for the given sequence\n",
    "            if match_local and len(title_sel) == len(sequence_list):\n",
    "                return True, True, True, list_pos\n",
    "\n",
    "    # If only one local match consider possible solid match where title is longer than the link sequence\n",
    "    if len(matches_list) == 1:\n",
    "        return True, True, False, matches_list[0]\n",
    "\n",
    "    # No solid match found\n",
    "    return match_possible_local, False, False, -1\n",
    "\n",
    "\n",
    "# Function for multiprocessing speedup\n",
    "def loop(filenames_loc: list) -> int:\n",
    "    global matches_list\n",
    "    n_loc = 0\n",
    "    for filename_loc in tqdm(filenames_loc):\n",
    "        article_loc = parse_xml(ARTICLES_DIR + filename_loc)\n",
    "        textelem_loc = get_xml_elem(article_loc, 'text')\n",
    "        text_loc = textelem_loc.text\n",
    "        n_loc_loc = 0\n",
    "\n",
    "        if text_loc is not None and len(text_loc):\n",
    "            # Move along the text from right to left to allow easier uri insertion\n",
    "            find_right_loc = len(text_loc)\n",
    "            find_left_loc = find_prev_space(text_loc, find_right_loc)\n",
    "            _border_left_loc = find_right_loc + 1\n",
    "\n",
    "            # Find link starting word\n",
    "            while find_left_loc != -1:\n",
    "                if check_in_uri(text_loc, find_left_loc):\n",
    "                    find_right_loc = find_left_loc\n",
    "                    find_left_loc = find_prev_space(text_loc, find_left_loc) if find_left_loc else -1\n",
    "                    continue\n",
    "                word_loc = title_handle_latin(text_loc[find_left_loc:find_right_loc].strip(' \\n\\r.,;:!?\\\\()[]{}&'),\n",
    "                                              COMBINATIONS_CORR_GLOBAL).upper()\n",
    "                # 'm' can be interpreted both as 'M' and 'T'\n",
    "                if word_loc == 'СМ' or word_loc == 'СТ' or BRUTE_FORCE_MODE:\n",
    "                    border_left_loc = find_left_loc if BRUTE_FORCE_MODE else find_right_loc\n",
    "                    while border_left_loc < len(text_loc) and text_loc[border_left_loc] in ' \\n\\r.,;:!?\\\\()[]{}&':\n",
    "                        border_left_loc += 1\n",
    "                    if _border_left_loc <= border_left_loc:\n",
    "                        # Same position as before\n",
    "                        break\n",
    "                    border_right_f_loc = border_left_loc\n",
    "                    _border_right_loc = border_left_loc\n",
    "                    border_find_allowed_loc = True\n",
    "                    _match_single_loc = False\n",
    "                    _match_pos_loc = -1\n",
    "                    matches_list = [p for p in range(len(titles_list))]\n",
    "                    _event_parts_len = 0\n",
    "                    while border_find_allowed_loc:\n",
    "                        border_right_f_loc = find_next_space(text_loc, border_right_f_loc)\n",
    "                        border_right_loc = border_right_f_loc\n",
    "                        while text_loc[border_right_loc - 1] in ' \\n\\r.,;:!?\\\\()[]{}&':\n",
    "                            border_right_loc -= 1\n",
    "                        if border_right_loc - border_left_loc < 2:\n",
    "                            _border_left_loc = border_left_loc\n",
    "                            break\n",
    "                        border_find_allowed_loc = False if border_right_f_loc == len(text_loc) else True\n",
    "                        event_loc = title_handle_latin(\n",
    "                            text_loc[border_left_loc:border_right_loc], COMBINATIONS_CORR_GLOBAL).upper()\n",
    "                        if event_loc in ['В', 'ПРИ', 'ТАКЖЕ', 'В СТ', 'ПРИ СТ', 'SЕЕАLSО', 'SАMЕАS'] and \\\n",
    "                                not BRUTE_FORCE_MODE:\n",
    "                            # Starting words continuation\n",
    "                            continue\n",
    "\n",
    "                        # Extract word sequence and try to find a matching title from list\n",
    "                        event_parts_loc = event_loc.split(' ')\n",
    "                        # Check for punctuation in event\n",
    "                        if len(event_parts_loc) and len(event_parts_loc[-1]):\n",
    "                            part_loc = event_parts_loc[-1]\n",
    "                            for sym in '\\n\\r\\\\':\n",
    "                                if sym in part_loc:\n",
    "                                    # Severe punctuation\n",
    "                                    _border_left_loc = border_left_loc\n",
    "                                    break\n",
    "                        if len(event_parts_loc) > 1:\n",
    "                            part_loc = event_parts_loc[-1]\n",
    "                            if len(part_loc) and (part_loc[0] in '\\n\\r([{'):\n",
    "                                # Severe punctuation\n",
    "                                _border_left_loc = border_left_loc\n",
    "                                break\n",
    "                            part_loc = event_parts_loc[-2]\n",
    "                            if len(part_loc) and (part_loc[0] in '\\n\\r([{' or part_loc[-1] in ',;:!?\\\\)]}') \\\n",
    "                                    or (len(part_loc) > 2 and part_loc[-1] == '.'):\n",
    "                                # Severe punctuation\n",
    "                                _border_left_loc = border_left_loc\n",
    "                                break\n",
    "                        for part_loc in event_parts_loc:\n",
    "                            if part_loc == '' or part_loc in ' \\n\\r.,;:!?\\\\()[]{}&':\n",
    "                                event_parts_loc.remove(part_loc)\n",
    "                        if _event_parts_len == len(event_parts_loc):\n",
    "                            # Nothing new here\n",
    "                            continue\n",
    "                        _event_parts_len = len(event_parts_loc)\n",
    "                        # Check for junk sequences\n",
    "                        not_junk_loc = 0\n",
    "                        _not_junk_loc = 0\n",
    "                        for part_loc in event_parts_loc:\n",
    "                            _not_junk_loc = not_junk_loc\n",
    "                            not_junk_loc += 1 if len(part_loc.strip(' \\n\\r.,;:!?\\\\()[]{}&')) > 5 else 0\n",
    "                        if not_junk_loc < 2 and len(event_parts_loc) >= 2:\n",
    "                            not_junk_loc = 0\n",
    "                        if _not_junk_loc < 2 and len(event_parts_loc) >= 2:\n",
    "                            _not_junk_loc = 0\n",
    "\n",
    "                        (match_possible_loc, match_single_loc, match_exact_loc, match_pos_loc) = find_matching_title(\n",
    "                            [p.strip(' \\n\\r.,;:!?\\\\()[]{}&') for p in event_parts_loc])\n",
    "                        border_find_allowed_loc = border_find_allowed_loc and match_possible_loc\n",
    "                        # Remember if single match\n",
    "                        if match_single_loc:\n",
    "                            _border_right_loc = border_right_loc\n",
    "                            _match_pos_loc = match_pos_loc\n",
    "                        # Consider last single match as exact\n",
    "                        if (not match_single_loc and _match_single_loc and _not_junk_loc) or (\n",
    "                                not border_find_allowed_loc and match_single_loc and not_junk_loc):\n",
    "                            border_right_loc = _border_right_loc\n",
    "                            match_pos_loc = _match_pos_loc\n",
    "                            match_exact_loc = True\n",
    "\n",
    "                        _match_single_loc = match_single_loc\n",
    "                        # Process exact match\n",
    "                        if match_exact_loc:\n",
    "                            _match_single_loc = False\n",
    "                            # Add an inter-link\n",
    "                            n_loc_loc += 1\n",
    "                            uri_pos_us = article_loc.attrib['uri'].find('_')\n",
    "                            uri_pos_sl = article_loc.attrib['uri'].rfind('/', 0, uri_pos_us)\n",
    "                            uri_loc = URI_PREFIX + 'relation' + \\\n",
    "                                      article_loc.attrib['uri'][uri_pos_sl:uri_pos_us + 1] + \\\n",
    "                                      str(n_loc_loc) + \\\n",
    "                                      article_loc.attrib['uri'][uri_pos_us:]\n",
    "                            relations_loc = get_xml_elem(article_loc, 'relations')\n",
    "                            relation_loc = ElementTree.SubElement(relations_loc, 'relation', {'uri': uri_loc})\n",
    "                            rel_text_loc = ElementTree.SubElement(relation_loc, 'rel_text')\n",
    "                            rel_text_loc.text = text_loc[border_left_loc:border_right_loc]\n",
    "                            rel_tgt_loc = ElementTree.SubElement(relation_loc, 'target')\n",
    "                            related_article_loc = parse_xml(ARTICLES_DIR + articles_list[match_pos_loc])\n",
    "                            rel_tgt_loc.text = related_article_loc.attrib['uri']\n",
    "                            text_loc = text_loc[:border_left_loc] + 'URI[[' + uri_loc + ']]/URI' + text_loc[\n",
    "                                                                                                   border_right_loc:]\n",
    "                            # Continue in case of multilink\n",
    "                            if not BRUTE_FORCE_MODE:\n",
    "                                border_left_loc += len('URI[[' + uri_loc + ']]/URI')\n",
    "                                while border_left_loc < len(text_loc) and not text_loc[border_left_loc] in ' \\n\\r':\n",
    "                                    border_left_loc += 1\n",
    "                                border_right_f_loc = border_left_loc\n",
    "                                matches_list = [p for p in range(len(titles_list))]\n",
    "                            else:\n",
    "                                _border_left_loc = border_left_loc\n",
    "                                break\n",
    "                        elif not border_find_allowed_loc:\n",
    "                            _border_left_loc = border_right_loc\n",
    "\n",
    "                find_right_loc = find_left_loc\n",
    "                find_left_loc = find_prev_space(text_loc, find_left_loc) if find_left_loc else -1\n",
    "\n",
    "        # Write xml\n",
    "        textelem_loc.text = text_loc\n",
    "        xml_write(article_loc, ARTICLES_DIR + filename_loc)\n",
    "        n_loc += n_loc_loc\n",
    "\n",
    "    return n_loc\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "\n",
    "# Get all the titles into a list\n",
    "articles_list = []\n",
    "titles_list = []\n",
    "if __name__ != '__main__' or not USE_MULTIPROCESSING:\n",
    "\n",
    "    # DEBUG                           ###\n",
    "    # filenames = [\"1583_LI.xml\", \"2471_PUASSONA.xml\", \"2472_PUASSONA.xml\"]\n",
    "    #####################################\n",
    "    if __name__ == '__main__':\n",
    "        print(\"Preparing search base...\")\n",
    "    for filename in tqdm(filenames):\n",
    "        article = parse_xml(ARTICLES_DIR + filename)\n",
    "        title = get_xml_elem(article, 'title').text\n",
    "        titles_list.append([title_word.upper().strip(' \\n\\r.,;:!?\\\\()[]{}&') for title_word in title.split(' ')])\n",
    "        articles_list.append(filename)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # DEBUG                           ###\n",
    "    # filenames = ['3576_JaKOBI.xml']    ###\n",
    "    #####################################\n",
    "    print(\"Searching relations in articles...\")\n",
    "    if USE_MULTIPROCESSING:\n",
    "        arrays = []\n",
    "        with Pool(number_of_processors) as pool:\n",
    "            for i in range(number_of_processors):\n",
    "                arr_len = len(filenames)\n",
    "                arr_start = i * (arr_len // number_of_processors)\n",
    "                arr_end = ((i + 1) * (arr_len // number_of_processors)) if i + 1 < number_of_processors else arr_len\n",
    "                arrays.append(filenames[arr_start:arr_end])\n",
    "            n = sum(pool.map(loop, arrays))\n",
    "    else:\n",
    "        n = loop(filenames)\n",
    "    print(\"Relations found in total:\", n)\n",
    "    if USE_MULTIPROCESSING:\n",
    "        input()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}