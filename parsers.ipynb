{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Общий код\n",
    "\n",
    "Ячейка подключает библиотеки и задаёт функции, используемые пунктах 1.1. и далее для сокращения объёма кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Общий код\n",
    "\n",
    "from os import walk\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import re\n",
    "import codecs\n",
    "from transliterate import translit, get_available_language_codes\n",
    "from random import randint\n",
    "\n",
    "\n",
    "############################ VARS ################################\n",
    "# Symbols and combinations that have to be corrected after OCR\n",
    "COMBINATIONS_CORR_GLOBAL = {\n",
    "\t'A':'А', 'a':'а', 'B':'В', 'b':'Ь', 'C':'С', 'c':'с', 'E':'Е', 'e':'е', 'H':'Н', 'K':'К', 'M':'М', 'O':'О', 'P':'Р', 'p':'р', 'T':'Т', 'X':'Х', 'y':'у',\n",
    "\t#'x':'х', # Causes infinite loops for some reason\n",
    "\t'U' : 'И',\n",
    "\t'r' : 'г',\n",
    "\t'n' : 'п',\n",
    "\t'Y' : 'У',\n",
    "\t' -' : '-',\n",
    "\t'- ' : '-',\n",
    "\t'S' : 'Я',\n",
    "\t'0' : 'О',\n",
    "\t'3' : 'З',\n",
    "\t'6' : 'б',\n",
    "\t'І' : 'I'\t\t# This teo are different!\n",
    "}\n",
    "# Symbols excluded in xml have to be converted back\n",
    "XML_EXCLUDES = {\n",
    "\t'&quot;' : '\"',\n",
    "\t'&apos;' : \"'\",\n",
    "\t'&lt;' : '<',\n",
    "\t'&gt;' : '>',\n",
    "\t'&amp;' : '&'\n",
    "}\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Write xml tree to file\n",
    "def prettify(elem):\n",
    "\t# Pretty-printed XML string for the Element.\n",
    "\trough_string = ET.tostring(elem, 'utf-8')\n",
    "\treparsed = minidom.parseString(rough_string)\n",
    "\treturn reparsed.toprettyxml(indent=\"  \")\n",
    "def xml_write(root, filename):\n",
    "\twith codecs.open(filename, 'w', 'utf-8') as f:\n",
    "\t\tf.write(prettify(root))\n",
    "\n",
    "\n",
    "# Convert xml excluded symbols\n",
    "def xml_excluded_convert (text:str) -> str:\n",
    "\tfor key in XML_EXCLUDES.keys():\n",
    "\t\twhile text.find(key) != -1:\n",
    "\t\t\tpos = text.find(key)\n",
    "\t\t\ttext = text[:pos] + XML_EXCLUDES[key] + text[pos+len(key):]\n",
    "\treturn text\n",
    "\n",
    "\n",
    "def remove_xml_spaces(elem, filename):\n",
    "\telem.tail = None\n",
    "\tif elem.text != None:\n",
    "\t\tis_space = True\n",
    "\t\tfor letter in elem.text:\n",
    "\t\t\tis_space = False if letter != ' ' else is_space\n",
    "\t\tif is_space:\n",
    "\t\t\telem.text = None\n",
    "\t\telse:\n",
    "\t\t\tif elem.tag == 'text':\n",
    "\t\t\t\telem.text = get_texts(filename)[0]\n",
    "\t\t\telif elem.tag == 'text_orig':\n",
    "\t\t\t\telem.text = get_texts(filename)[1]\n",
    "\t\t\telem.text = xml_excluded_convert(elem.text)\n",
    "\tfor subelem in elem:\n",
    "\t\tsubelem = remove_xml_spaces(subelem, filename)\n",
    "\treturn elem\n",
    "def parse_xml(filename):\n",
    "\t# Parse existing xml (string parsing is needed to avoid extra newlines appearing)\n",
    "\texit_string = ''\n",
    "\twith codecs.open(filename, 'r', 'utf-8') as f:\n",
    "\t\tfor i in f.readlines():\n",
    "\t\t\texit_string += i[:-1]\n",
    "\troot = ET.fromstring(exit_string)\n",
    "\troot = remove_xml_spaces(root, filename)\n",
    "\treturn root\n",
    "\n",
    "\n",
    "# !!!BUG!!! for some reason newlines diappear in texts in parsed xml, so extract article texts manually and replace\n",
    "def get_texts(filename:str) -> str:\n",
    "\twith codecs.open(filename, 'r', 'utf-8') as f:\n",
    "\t\tfile = f.read()\n",
    "\ttext = file[file.find('<text>')+6:file.find('</text>')]\n",
    "\twith codecs.open(filename, 'r', 'utf-8') as f:\n",
    "\t\tfile = f.read()\n",
    "\ttext_orig = file[file.find('<text_orig>')+11:file.find('</text_orig>')]\n",
    "\treturn (text, text_orig)\n",
    "\n",
    "\n",
    "# Get xml tree element wit sertain tag name\n",
    "def get_xml_elem(root:ET.Element, elem_path:str) -> ET.Element:\n",
    "\ttgt = elem_path.split('/')[0]\n",
    "\tfor elem in root:\n",
    "\t\tif elem.tag == tgt:\n",
    "\t\t\tif elem_path.find('/') != -1:\n",
    "\t\t\t\treturn get_xml_elem(elem, elem_path[elem_path.find('/')+1:])\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn elem\n",
    "\treturn None\n",
    "\n",
    "\n",
    "# Small dictionaries merger\n",
    "def dict_merge(dict1:dict, dict2:dict) -> dict:\n",
    "\tfor key in dict2.keys():\n",
    "\t\tdict1[key] = dict2[key]\n",
    "\treturn dict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Базовый парсер заголовков\n",
    "\n",
    "Вытаскивает из latex-кода заголовки статей и их расположение в файлах.\n",
    "\n",
    "Разбивка происходит в полуручном режиме, т.к. нет уверенности в формате заголовков.\n",
    "\n",
    "В тексте ищутся слова, содержащие в своём составе заглавные буквы на русском и английском языках в отношении, большем или равным заданному (по умолчанию 0.51, при меньших значениях количество вхождений значительно возрастает, например за счёт двухбуквенных предлогов). Предполагается, что таким образом удаётся обнаруживать неправильно машиинно распознанный капс. Слова или цепочки слов, состоящие из одного строчного символа включаются в заголовок, если стоят между слов, определённых как часть заголовка. При этом, одиночные заглавные буквы, а также инициалы не воспринимаются как начало заголовка.\n",
    "\n",
    "## Использование\n",
    "- При удовлетворительном определении заголовка нажать `Enter` без дополнительного ввода.\n",
    "- Если предложенное место заголовком не является ввести `\"n\"`\n",
    "- При неправильном определении границ заголовка ввести два корректировочных числа для сдвига левой и правой границы.\n",
    "  - ЗАМЕЧАНИЕ: сдвиг производится попробельно, т.е. двойной пробел будет распознан как слово нулевой длины.\n",
    "  - ЗАМЕЧАНИЕ: границы отображаемого фрагмента текста будут передвинуты автоматически. Длины левой и правой границ в словах задаются в параметрах.\n",
    "  - ПРИМЕРЫ:\n",
    "    - `out: a [B C] d e f` -> `in: 0 2` -> `out: a [B C D E] f`\n",
    "    - `out: a b c [D E] f` -> `in: 2 -1` -> `out: a [B C D] e f`\n",
    "- Также возможен посимвольный сдвиг правой границы в случае \"сращивания\" заголовка статьи и её текста. Ввести одно число, начиная с точки.\n",
    "  - ПРИМЕРЫ:\n",
    "    - `out: a[BC]def` -> `in: .2` -> `out: a[BCDE]f`\n",
    "    - `out: a[BCDE]f` -> `in: .-1` -> `out: a[BCD]ef`\n",
    "\n",
    "В выводе в терминале переносы строк для удобства заменены на `\"$\"`\n",
    "\n",
    "### Прочее\n",
    "- Для определителя капса достуны исключения, которые никогда не будут рассматриваться, как потенциальные начала заголовков, см. опции. По умолчанию: первые 10 римских цифр, \"МэВ\" и \"ГэВ\".\n",
    "- Использовать системный терминал для взаимодействия оказывается удобнее, чем использовать jupyter, поэтому можно скопировать ячейку с кодом в файл `scripter.py` и запускать его.\n",
    "- При положительном определении заголовка файл дополняется немедленно, прервать процесс можно в любой момент, как и продолжить после -- итоговый файл будет дополяться, а не перезаписываться с нуля при новом запуске программы (главное не забыть предварительно удалить из конца файла дубликаты, если вы начинаете с той страницы, на которой закончили в прошлый раз, а не со следующей)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Базовый парсер заголовков\n",
    "\n",
    "from os import walk\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "\n",
    "############################ VARS ################################\n",
    "PAGES_DIR = \"./matphys/rpages/\"\n",
    "EXIT_DIR = \"./matphys/\"\n",
    "EXIT_FILE = \"FMEv2.xml\"\n",
    "# First and last pages to be parsed\n",
    "START_PAGE = 372\n",
    "END_PAGE = 400\n",
    "# How many words to display before and after a potential title\n",
    "LEAD_WORDS = 5\n",
    "AFT_WORDS = 5\n",
    "# Look in the description\n",
    "CAPS_QUOT = 0.51\n",
    "EXCEPTIONS = ['I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX', 'X', 'МэВ', 'ГэВ']\n",
    "# Symbols excluded in xml have to be converted back\n",
    "XML_EXCLUDES = {'&quot;' : '\"', '&apos;' : \"'\", '&lt;' : '<',\t'&gt;' : '>',\t'&amp;' : '&'}\n",
    "##################################################################\n",
    "\n",
    "\n",
    "\n",
    "class Article:\n",
    "\tstart_title = 0\n",
    "\tend_title = 0\n",
    "\tfilename = ''\n",
    "\n",
    "\n",
    "\n",
    "# Write xml tree to file\n",
    "def prettify_1(elem):\n",
    "\t# Pretty-printed XML string for the Element.\n",
    "\trough_string = ET.tostring(elem, 'utf-8')\n",
    "\treparsed = minidom.parseString(rough_string)\n",
    "\treturn reparsed.toprettyxml(indent=\"  \")\n",
    "def xml_write_1(root):\n",
    "\twith codecs.open(EXIT_DIR + EXIT_FILE, 'w', 'utf-8') as f:\n",
    "\t\tf.write(prettify_1(root))\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames_raw = next(walk(PAGES_DIR), (None, None, []))[2]  # [] if no file\n",
    "filenames = []\n",
    "for i in range(START_PAGE, END_PAGE + 1):\n",
    "\tfor filename in filenames_raw:\n",
    "\t\tbeginning = \"rp-\" + str(i) + \"_\"\n",
    "\t\tif filename[:len(beginning)] == beginning and filename[-4:] == \".mmd\":\n",
    "\t\t\tfilenames.append(filename)\n",
    "\t\t\t\t\t\t\n",
    "\n",
    "# Check for existing xml\n",
    "filenames_raw = next(walk(EXIT_DIR), (None, None, []))[2]  # [] if no file\n",
    "if not(EXIT_FILE in filenames_raw):\n",
    "\troot = ET.Element('data')\n",
    "\txml_write_1(root)\n",
    "\n",
    "\n",
    "# Convert xml excluded symbols\n",
    "def xml_excluded_convert (text:str) -> str:\n",
    "\tfor key in XML_EXCLUDES.keys():\n",
    "\t\twhile text.find(key) != -1:\n",
    "\t\t\tpos = text.find(key)\n",
    "\t\t\ttext = text[:pos] + XML_EXCLUDES[key] + text[pos+len(key):]\n",
    "\treturn text\n",
    "def remove_xml_spaces_1(elem):\n",
    "\telem.tail = None\n",
    "\tif elem.text != None:\n",
    "\t\tis_space = True\n",
    "\t\tfor letter in elem.text:\n",
    "\t\t\tis_space = False if letter != ' ' else is_space\n",
    "\t\telem.text = None if is_space else xml_excluded_convert(elem.text)\n",
    "\tfor subelem in elem:\n",
    "\t\tsubelem = remove_xml_spaces_1(subelem)\n",
    "\treturn elem\n",
    "def parse_xml_1():\n",
    "\t# Parse existing xml (string parsing is needed to avoid extra newlines appearing)\n",
    "\texit_string = ''\n",
    "\twith codecs.open(EXIT_DIR + EXIT_FILE, 'r', 'utf-8') as f:\n",
    "\t\tfor i in f.readlines():\n",
    "\t\t\texit_string += i[:-1]\n",
    "\troot = ET.fromstring(exit_string)\n",
    "\t# Remove empty tails and texts\n",
    "\troot = remove_xml_spaces_1(root)\n",
    "\treturn root\n",
    "root = parse_xml_1()\n",
    "num = len(root) + 1\n",
    "\n",
    "\n",
    "# Add article title and metadata to xml tree\n",
    "def add_artice_1(elem):\n",
    "\t# Update root in case it's been changed\n",
    "\troot = parse_xml_1()\n",
    "\tnum = len(root) + 1\n",
    "\tarticle = ET.SubElement(root, 'article', {'n':str(num)})\n",
    "\ttitle = ET.SubElement(article, 'title')\n",
    "\ttitle.text = file[elem.start_title+1:elem.end_title]\n",
    "\ttitle_meta = ET.SubElement(article, 'title-meta')\n",
    "\ttitle_file = ET.SubElement(title_meta, 'title-file')\n",
    "\ttitle_file.text = elem.filename\n",
    "\ttitle_start = ET.SubElement(title_meta, 'title-start')\n",
    "\ttitle_start.text = str(elem.start_title + 1)\n",
    "\ttitle_end = ET.SubElement(title_meta, 'title-end')\n",
    "\ttitle_end.text = str(elem.end_title)\n",
    "\txml_write_1(root)\n",
    "\treturn num\n",
    "\n",
    "\n",
    "# Count number of alphabetic letters in word\n",
    "def count_letters_1(word):\n",
    "\tnum = 0\n",
    "\tfor letter in word:\n",
    "\t\tnum += 0 if re.match(r\"[A-ZА-Яa-zа-я]\", letter) == None else 1\n",
    "\treturn num\n",
    "\n",
    "# Check if word is written in CAPS\n",
    "def check_caps_1(word):\n",
    "\tnum = 0\n",
    "\tlen_word = 0\n",
    "\tfor letter in word:\n",
    "\t\t#num += 0 if re.match(r\"[A-ZА-Я0-9]|[!#$%&'*+-.^_`|~:]\", letter) == None else 1\t\t\t\t\t# Too many symbols, math formulas are being detected\n",
    "\t\tlen_word += 1 if re.match(r\"[!#$%&'*+-.^_`|~:]\", letter) == None else 0\n",
    "\t\tnum += 0 if re.match(r\"[A-ZА-Я]\", letter) == None else 1\n",
    "\treturn 0 if len_word == 0 or num / len_word < CAPS_QUOT or word in EXCEPTIONS else num\t\t\t\t# Also exclude common roman numbers\n",
    "\n",
    "# Check for initials like \"I.E.\"\n",
    "def check_initials_1(word):\n",
    "\tinitials = True\n",
    "\tfor i in range(len(word) - 1):\n",
    "\t\ttype_1 = 0 if re.match(r\"[A-ZА-Яa-zа-я]\", word[i]) == None else 1\n",
    "\t\ttype_2 = 0 if re.match(r\"[A-ZА-Яa-zа-я]\", word[i + 1]) == None else 1\n",
    "\t\tinitials = False if type_1 and type_2 else initials\n",
    "\treturn initials\n",
    "\n",
    "\n",
    "# Find next ot prev word boundary (space / newline)\n",
    "def prev_from_1(pos, file):\n",
    "\tpos = max(pos, 0)\n",
    "\tprev_space = file.rfind(' ', 0, pos)\n",
    "\tprev_nl = file.rfind('\\n', 0, pos)\n",
    "\tprev_space = -1 if prev_space == -1 else prev_space\n",
    "\tprev_nl = -1 if prev_nl == -1 else prev_nl\n",
    "\treturn max(prev_nl, prev_space)\n",
    "def next_from_1(pos, file, end_replace = True):\n",
    "\tnext_space = file.find(' ', pos + 1)\n",
    "\tnext_nl = file.find('\\n', pos + 1)\n",
    "\tif end_replace:\n",
    "\t\tnext_space = len(file) if next_space == -1 else next_space\n",
    "\t\tnext_nl = len(file) if next_nl == -1 else next_nl\n",
    "\treturn max(next_nl, next_space) if next_space == -1 or next_nl == -1 else min(next_nl, next_space)\n",
    "\n",
    "\n",
    "# Main loop\n",
    "for filename in filenames:\n",
    "\tprint()\n",
    "\tprint(\"################################ \" + filename + \" ################################\")\n",
    "\twith codecs.open(PAGES_DIR + filename, 'r', 'utf-8') as f:\n",
    "\t\tfile = f.read()\n",
    "\t\n",
    "\tword_bound_l = -1\n",
    "\tword_bound_r = next_from_1(word_bound_l, file, end_replace=False)\n",
    "\tEOF_reached = False\n",
    "\n",
    "\twhile not EOF_reached:\n",
    "\t\tif word_bound_r == -1:\n",
    "\t\t\tword_bound_r = len(file)\n",
    "\t\t\tEOF_reached = True\n",
    "\n",
    "\n",
    "\t\tif check_caps_1(file[word_bound_l+1:word_bound_r]) < 2 or check_initials_1(file[word_bound_l+1:word_bound_r]):\n",
    "\t\t\tword_bound_l = word_bound_r\n",
    "\t\t\tword_bound_r = next_from_1(word_bound_l, file, end_replace=False)\n",
    "\t\t\n",
    "\t\telse: # Possibly found a title\n",
    "\t\t\t# Left border of a title is already known\n",
    "\t\t\tstart_title = word_bound_l\n",
    "\n",
    "\t\t\t# Define right border of a title\n",
    "\t\t\tdefined_end = False\n",
    "\t\t\tend_title = word_bound_r\n",
    "\t\t\twhile not defined_end:\n",
    "\t\t\t\tword_bound_l = word_bound_r\n",
    "\t\t\t\tword_bound_r = next_from_1(word_bound_l, file)\n",
    "\n",
    "\t\t\t\tif word_bound_l == len(file):\n",
    "\t\t\t\t\tdefined_end = True\n",
    "\t\t\t\telif not check_caps_1(file[word_bound_l+1:word_bound_r]) and count_letters_1(file[word_bound_l+1:word_bound_r]) < 2:\n",
    "\t\t\t\t\tif re.match(r\"[A-ZА-Яa-zа-я]\", file[word_bound_l+1]) != None:\n",
    "\t\t\t\t\t\t# Most possibly belongs to title\n",
    "\t\t\t\t\t\tend_title = word_bound_r\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t# Most possibly NOT belongs to title\n",
    "\t\t\t\t\t\tpass\n",
    "\t\t\t\telif check_caps_1(file[word_bound_l+1:word_bound_r]):\n",
    "\t\t\t\t\tend_title = word_bound_r\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdefined_end = True\n",
    "\n",
    "\t\t\tnext_title = False\n",
    "\t\t\twhile not next_title:\n",
    "\t\t\t\t# Update root in case it's been changed\n",
    "\t\t\t\troot = parse_xml_1()\n",
    "\t\t\t\tnum = len(root) + 1\n",
    "\n",
    "\t\t\t\t# Console output for further user actions\n",
    "\t\t\t\tsegment_start = start_title\n",
    "\t\t\t\tsegment_end = end_title\n",
    "\t\t\t\tfor i in range(LEAD_WORDS):\n",
    "\t\t\t\t\tsegment_start = prev_from_1(segment_start, file)\n",
    "\t\t\t\tfor i in range(AFT_WORDS):\n",
    "\t\t\t\t\tsegment_end = next_from_1(segment_end, file)\n",
    "\t\t\t\t\n",
    "\t\t\t\tout_str = file[segment_start+1:segment_end]\n",
    "\n",
    "\t\t\t\t# Format\n",
    "\t\t\t\tfor i in range(len(out_str)):\n",
    "\t\t\t\t\tout_str = out_str[:i] + ('$' if out_str[i] == '\\n' else out_str[i]) + out_str[i+1:]\n",
    "\t\t\t\tout_str = f\"{num})\\n\" + out_str + '\\n' + ' ' * (start_title - segment_start) + '^' * (end_title - start_title - 1)\n",
    "\t\t\t\t# Check for \"section\" in the string. This is referred to alphabetic tip at the bottom of the page\n",
    "\t\t\t\t\"\"\"if 'section' in out_str or 'title' in out_str:\n",
    "\t\t\t\t\tout_str += '     ############################### Title or section found! ###############################'\"\"\" # Not Used\n",
    "\t\t\t\tprint(out_str)\n",
    "\n",
    "\t\t\t\t# User actions\n",
    "\t\t\t\tresponse = input()\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tif response == '':\n",
    "\t\t\t\t\t\t# Add article\n",
    "\t\t\t\t\t\tarticle = Article()\n",
    "\t\t\t\t\t\tarticle.start_title = start_title\n",
    "\t\t\t\t\t\tarticle.end_title = end_title\n",
    "\t\t\t\t\t\tarticle.filename = filename\n",
    "\t\t\t\t\t\tnum = add_artice_1(article)\n",
    "\t\t\t\t\t\tnext_title = True\n",
    "\t\t\t\t\t\tword_bound_l = end_title\n",
    "\t\t\t\t\t\tword_bound_r = next_from_1(word_bound_l, file, end_replace=False)\n",
    "\t\t\t\t\t\tprint(f'Adding article, n=\"{num}\", title=\"{file[start_title+1:end_title]}\"\\n\\n')\n",
    "\t\t\t\t\telif response == 'n' or response == 'т':\n",
    "\t\t\t\t\t\t# Do not add this one\n",
    "\t\t\t\t\t\tnext_title = True\n",
    "\t\t\t\t\t\tprint(\"Not an article, skipping\\n\\n\")\n",
    "\t\t\t\t\telif response[0] == '.':\n",
    "\t\t\t\t\t\tend_title += int(response[1:])\n",
    "\t\t\t\t\t\tprint(\"Changing title right border\\n\\n\")\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t# Change title borders\n",
    "\t\t\t\t\t\tcorrections = response.split(' ')\n",
    "\t\t\t\t\t\tcorrections[0] = int(corrections[0])\n",
    "\t\t\t\t\t\tcorrections[1] = int(corrections[1])\n",
    "\t\t\t\t\t\tif corrections[0] > 0:\n",
    "\t\t\t\t\t\t\tfor i in range(abs(corrections[0])):\n",
    "\t\t\t\t\t\t\t\tstart_title = prev_from_1(start_title, file)\n",
    "\t\t\t\t\t\tif corrections[0] < 0:\n",
    "\t\t\t\t\t\t\tfor i in range(abs(corrections[0])):\n",
    "\t\t\t\t\t\t\t\tstart_title = next_from_1(start_title, file)\n",
    "\t\t\t\t\t\tif corrections[1] < 0:\n",
    "\t\t\t\t\t\t\tfor i in range(abs(corrections[1])):\n",
    "\t\t\t\t\t\t\t\tend_title = prev_from_1(end_title, file)\n",
    "\t\t\t\t\t\tif corrections[1] > 0:\n",
    "\t\t\t\t\t\t\tfor i in range(abs(corrections[1])):\n",
    "\t\t\t\t\t\t\t\tend_title = next_from_1(end_title, file)\n",
    "\t\t\t\t\t\tprint(\"Changing title borders\\n\\n\")\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tprint(\"########## !!! Failed on input, try again !!! ##########\\n\\n\")\n",
    "\n",
    "\n",
    "# End reached\n",
    "print('###########################################################################################')\n",
    "print('Last requested page processed. Press \"Enter\" to close this window.')\n",
    "response = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. Добавление заголовков по одному\n",
    "\n",
    "В разделе параметров указать номер страницы и ТОЧНУЮ формулировку заголовка из сырого текста, после чего запустить ячейку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1. Добавление заголовков по одному\n",
    "\n",
    "############################ VARS ################################\n",
    "PAGES_DIR = \"./matphys/rpages/\"\n",
    "EXIT_DIR = \"./matphys/\"\n",
    "EXIT_FILE = \"FMEv2-add.xml\"\n",
    "# First and last pages to be parsed\n",
    "PAGE = 371\n",
    "TITLE = 'МУльтиполь с о с т о я н и я'\n",
    "##################################################################\n",
    "\n",
    "\n",
    "\n",
    "class Article:\n",
    "\tstart_title = 0\n",
    "\tend_title = 0\n",
    "\tfilename = ''\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames_raw = next(walk(PAGES_DIR), (None, None, []))[2]  # [] if no file\n",
    "filenames = []\n",
    "for i in range(PAGE, PAGE + 1):\n",
    "\tfor filename in filenames_raw:\n",
    "\t\tbeginning = \"rp-\" + str(i) + \"_\"\n",
    "\t\tif filename[:len(beginning)] == beginning and filename[-4:] == \".mmd\":\n",
    "\t\t\tfilenames.append(filename)\n",
    "\t\t\t\t\t\t\n",
    "\n",
    "# Check for existing xml\n",
    "filenames_raw = next(walk(EXIT_DIR), (None, None, []))[2]  # [] if no file\n",
    "if not(EXIT_FILE in filenames_raw):\n",
    "\troot = ET.Element('data')\n",
    "\txml_write(root, EXIT_DIR + EXIT_FILE)\n",
    "\n",
    "\n",
    "root = parse_xml(EXIT_DIR + EXIT_FILE)\n",
    "\n",
    "\n",
    "# Add article title and metadata to xml tree\n",
    "def add_artice(elem, root, num):\n",
    "\tarticle = ET.SubElement(root, 'article', {'n':str(num)})\n",
    "\ttitle = ET.SubElement(article, 'title')\n",
    "\ttitle.text = file[elem.start_title+1:elem.end_title]\n",
    "\ttitle_meta = ET.SubElement(article, 'title-meta')\n",
    "\ttitle_file = ET.SubElement(title_meta, 'title-file')\n",
    "\ttitle_file.text = elem.filename\n",
    "\ttitle_start = ET.SubElement(title_meta, 'title-start')\n",
    "\ttitle_start.text = str(elem.start_title + 1)\n",
    "\ttitle_end = ET.SubElement(title_meta, 'title-end')\n",
    "\ttitle_end.text = str(elem.end_title)\n",
    "\txml_write(root, EXIT_DIR + EXIT_FILE)\n",
    "\n",
    "# Read requested file\n",
    "with codecs.open(PAGES_DIR + filenames[0], 'r', 'utf-8') as f:\n",
    "\tfile = f.read()\n",
    "\n",
    "# Find titles and add them\n",
    "start_title = 0\n",
    "end_title = 0\n",
    "num = len(root) + 1\n",
    "while file.find(TITLE, end_title) != -1:\n",
    "\tstart_title = file.find(TITLE, start_title)\n",
    "\tend_title = start_title + len(TITLE)\n",
    "\tstart_title -= 1 # Set on space befor the title\n",
    "\n",
    "\tarticle = Article()\n",
    "\tarticle.start_title = max(start_title, 0)\n",
    "\tarticle.end_title = min(end_title, len(file))\n",
    "\tarticle.filename = filenames[0]\n",
    "\tadd_artice(article, root, num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Исправление ошибок в заголовках\n",
    "\n",
    "Состоит из двух частей: \"составитель пар\" и \"подстановщик\".\n",
    "\n",
    "Сначала \"составитель\" формирует xml-список всех заголовков с возможными автоматическими исправлениями (в формате было / стало):\n",
    "1. замена латиницы на агалогичную кириллицу;\n",
    "2. замена задаванных буквосочетаний (см. параметры)\n",
    "3. удаление обрамляющих знаков препинания;\n",
    "4. замена всех букв на заглавные (в том числе это избавляет дальнейшей необходимости исправлять имена);\n",
    "5. слияние разорванных на отдельные буквы слов (если рядом оказываются несколько таких слов, то они оказываются слиты вместе).\n",
    "\n",
    "Этот список необходимо просмотреть и исправить оставшиеся ошибки.\n",
    "\n",
    "Затем запустить \"подстановщик\", который заменит все заголовки на исправленные.\n",
    "\n",
    "## 2.1. Составитель пар \"оригинальный - исправленный\" для заголовков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1. Составитель пар \"оригинальный - исправленный\" для заголовков:\n",
    "\n",
    "############################ VARS ################################\n",
    "WORK_DIR = \"./matphys/\"\n",
    "INPUT_FILE = \"FMEv2.xml\"\n",
    "CORRECTION_FILE = \"FMEcorr.xml\"\n",
    "COMBINATIONS_CORR = dict_merge(COMBINATIONS_CORR_GLOBAL, {\n",
    "\t'ХК' : 'Ж',\n",
    "\t'ЛАГРАНХ' : 'ЛАГРАНЖ',\n",
    "\t'ЛАТРАНХ' : 'ЛАГРАНЖ'\n",
    "})\n",
    "##################################################################\n",
    "\t\t\t\t\t\t\n",
    "\n",
    "# Check for existing xml\n",
    "filenames_raw = next(walk(WORK_DIR), (None, None, []))[2]  # [] if no file\n",
    "if not(INPUT_FILE in filenames_raw):\n",
    "\troot = ET.Element('data')\n",
    "\txml_write(root, WORK_DIR + CORRECTION_FILE)\n",
    "\n",
    "\n",
    "root = parse_xml(WORK_DIR + INPUT_FILE)\n",
    "\n",
    "\n",
    "# Get all the titles into a dict\n",
    "titles_dict = {}\n",
    "pages_dict = {}\n",
    "for article in root:\n",
    "\ttitle = get_xml_elem(article, 'title').text\n",
    "\ttitles_dict[title] = title\n",
    "\ttitle_file = get_xml_elem(article, 'title-meta/title-file')\n",
    "\tpages_dict[title] = title_file.text[title_file.text.find('-')+1:title_file.text.find('_')]\n",
    "\n",
    "\n",
    "# Correct preferred combinations and latin letters\n",
    "for title in titles_dict.keys():\n",
    "\t#print(title)\n",
    "\ttitle_new = titles_dict[title]\n",
    "\tfor comb in COMBINATIONS_CORR.keys():\n",
    "\t\t#print(comb)\n",
    "\t\twhile title_new.find(comb) != -1:\n",
    "\t\t\t#print(title_new.find(comb))\n",
    "\t\t\ttitle_new = title_new[:title_new.find(comb)] + COMBINATIONS_CORR[comb] + title_new[title_new.find(comb) + len(comb):]\n",
    "\ttitles_dict[title] = title_new\n",
    "\n",
    "# Remove bounding symbols\n",
    "for title in titles_dict.keys():\n",
    "\ttitle_new = titles_dict[title]\n",
    "\twhile re.match(r\"[!#%&'*+-.^_`|~:;]\", title_new[0]) != None:\n",
    "\t\ttitle_new = title_new[1:]\n",
    "\twhile re.match(r\"[!#%&'*+-.^_`|~:;]\", title_new[-1]) != None:\n",
    "\t\ttitle_new = title_new[:-1]\n",
    "\ttitles_dict[title] = title_new\n",
    "\n",
    "# CAPS\n",
    "for title in titles_dict.keys():\n",
    "\ttitles_dict[title] = titles_dict[title].upper()\n",
    "\n",
    "# Merge single-lettered words\n",
    "for title in titles_dict.keys():\n",
    "\ttitle_new = titles_dict[title]\n",
    "\ttitle_new = ' ' + title_new + ' '\n",
    "\tfor i in range(len(title_new) - 4):\n",
    "\t\tif (title_new[i] == ' ' or title_new[i] == '№') and title_new[i + 2] == ' ' and title_new[i + 4] == ' ':\n",
    "\t\t\ttitle_new = title_new[:i+2] + '№' + title_new[i+3:]\n",
    "\ti = 0\n",
    "\twhile i < len(title_new):\n",
    "\t\tif title_new[i] == '№':\n",
    "\t\t\ttitle_new = title_new[:i] + title_new[i+1:]\n",
    "\t\t\ti = 0\n",
    "\t\telse:\n",
    "\t\t\ti += 1\n",
    "\twhile title_new[0] == ' ':\n",
    "\t\ttitle_new = title_new[1:]\n",
    "\twhile title_new[-1] == ' ':\n",
    "\t\ttitle_new = title_new[:-1]\n",
    "\ttitles_dict[title] = title_new\n",
    "\n",
    "\n",
    "# Write corrections xml\n",
    "root = ET.Element('data')\n",
    "for i in titles_dict.items():\n",
    "\tpair = ET.SubElement(root, 'pair')\n",
    "\ttitle_old = ET.SubElement(pair, 'title_old')\n",
    "\ttitle_old.text = i[0]\n",
    "\ttitle_new = ET.SubElement(pair, 'title_new')\n",
    "\ttitle_new.text = i[1]\n",
    "\tpage = ET.SubElement(pair, 'page')\n",
    "\tpage.text = pages_dict[i[0]]\n",
    "xml_write(root, WORK_DIR + CORRECTION_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Подстановщик исправленных заголовков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2. Подстановщик исправленных заголовков:\n",
    "\n",
    "############################ VARS ################################\n",
    "WORK_DIR = \"./matphys/\"\n",
    "INPUT_FILE = \"FMEv2.xml\"\n",
    "CORRECTION_FILE = \"FMEcorr.xml\"\n",
    "EXIT_FILE = \"FMEtitles.xml\"\n",
    "##################################################################\n",
    "\n",
    "\n",
    "\n",
    "root = parse_xml(WORK_DIR + CORRECTION_FILE)\n",
    "\n",
    "\n",
    "# Get all the corrections into a dict\n",
    "titles_dict = {}\n",
    "for pair in root:\n",
    "\ttitles_dict[get_xml_elem(pair, 'title_old').text] = get_xml_elem(pair, 'title_new').text\n",
    "\n",
    "\n",
    "root = parse_xml(WORK_DIR + INPUT_FILE)\n",
    "\n",
    "\n",
    "# Replace titles\n",
    "for article in root:\n",
    "\tget_xml_elem(article, 'title').text = titles_dict[get_xml_elem(article, 'title').text]\n",
    "xml_write(root, WORK_DIR + EXIT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Сортировщик / сливщик файлов с заголовками\n",
    "\n",
    "Сортирует статьи в файлах из данного списка в порядке страница-расположение, т.е. (если не сказано иного) в алфавитном порядке и выводит в один выходной файл. Также порядковый номер заменяется uri формата \"http://libmeta.ru/fme/article/1_Kraevaya\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Сортировщик / сливщик файлов с заголовками\n",
    "\n",
    "############################ VARS ################################\n",
    "WORK_DIR = \"./results/\"\n",
    "INPUT_FILES = [\"FMEtitles-p5-100.xml\", \"FMEtitles-p101-200.xml\", \"FMEtitles-p201-300.xml\", \"FMEtitles-p301-400.xml\", \"FMEtitles-p301-400-add.xml\"]\n",
    "EXIT_FILE = \"FMEtitles-merged.xml\"\n",
    "##################################################################\n",
    "\n",
    "\n",
    "\n",
    "class Article:\n",
    "\ttitle = ''\n",
    "\tstart_title = ''\n",
    "\tend_title = ''\n",
    "\tfilename = ''\n",
    "\n",
    "\n",
    "\n",
    "# Add article title and metadata to xml tree\n",
    "def add_artice(elem, root, num):\n",
    "\tarticle = ET.SubElement(root, 'article', {'uri':\"http://libmeta.ru/fme/article/\"+str(num)+\"_\"+translit(elem.title[:elem.title.find(' ')], 'ru', True)})\n",
    "\ttitle = ET.SubElement(article, 'title')\n",
    "\ttitle.text = elem.title\n",
    "\ttitle_meta = ET.SubElement(article, 'title-meta')\n",
    "\ttitle_file = ET.SubElement(title_meta, 'title-file')\n",
    "\ttitle_file.text = elem.filename\n",
    "\ttitle_start = ET.SubElement(title_meta, 'title-start')\n",
    "\ttitle_start.text = str(int(elem.start_title) + 1)\n",
    "\ttitle_end = ET.SubElement(title_meta, 'title-end')\n",
    "\ttitle_end.text = elem.end_title\n",
    "\n",
    "\n",
    "# Collect all the articles\n",
    "articles_dict = {}\n",
    "for filename in INPUT_FILES:\n",
    "\troot = parse_xml(WORK_DIR + filename)\n",
    "\tfor article in root:\n",
    "\t\tnum = ()\n",
    "\t\ttitle = get_xml_elem(article, 'title').text\n",
    "\t\telem = get_xml_elem(article, 'title-meta/title-file')\n",
    "\t\tpage = elem.text[elem.text.find('-')+1:elem.text.find('_')]\n",
    "\t\tpos = get_xml_elem(article, 'title-meta/title-start').text\n",
    "\t\tstart = get_xml_elem(article, 'title-meta/title-start').text\n",
    "\t\tend = get_xml_elem(article, 'title-meta/title-end').text\n",
    "\t\tfile = get_xml_elem(article, 'title-meta/title-file').text\n",
    "\t\tnum = (int(page), int(pos))\n",
    "\t\tarticles_dict[num] = {'title':title, 'file':file, 'start':start, 'end':end}\n",
    "\n",
    "\n",
    "# Sort keys and wrtite articles accordingly\n",
    "root = ET.Element('data')\n",
    "nums_list = sorted(list(i for i in articles_dict.keys()))\n",
    "for num in range(len(nums_list)):\n",
    "\tarticle = Article()\n",
    "\tarticle.title = articles_dict[nums_list[num]]['title']\n",
    "\tarticle.start_title = articles_dict[nums_list[num]]['start']\n",
    "\tarticle.end_title = articles_dict[nums_list[num]]['end']\n",
    "\tarticle.filename = articles_dict[nums_list[num]]['file']\n",
    "\tadd_artice(article, root, num + 1)\n",
    "xml_write(root, WORK_DIR + EXIT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Парсер текстов статей\n",
    "\n",
    "По информации из указанного файла с заголовками вытаскивает в сыром виде тексты статей. Каждая статья помещается в свой .xml файл, с заголовком, содержащим номер статьи и первое слово из заголовка транслитом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Парсер текстов статей\n",
    "\n",
    "############################ VARS ################################\n",
    "TITLES_FILE = \"./results/FMEtitles-merged.xml\"\n",
    "PAGES_DIR = \"./matphys/rpages/\"\n",
    "EXIT_DIR = \"./results/FMEarticles/\"\n",
    "COMBINATIONS_CORR = {\n",
    "\t'І' : 'I'\t\t# This teo are different!\n",
    "}\n",
    "##################################################################\n",
    "\n",
    "\n",
    "class Article:\n",
    "\tstart_file = ''\n",
    "\tstart_pos = 0\n",
    "\tend_file = ''\n",
    "\tend_pos = 0\n",
    "\ttext = ''\n",
    "\turi = ''\n",
    "\ttitle = ''\n",
    "\txml = ''\n",
    "\n",
    "\tdef get_text(self):\n",
    "\t\tif self.start_file == self.end_file:\n",
    "\t\t\twith codecs.open(PAGES_DIR + self.start_file, 'r', 'utf-8') as f:\n",
    "\t\t\t\tself.text += f.read()[self.start_pos:self.end_pos]\n",
    "\t\telse:\n",
    "\t\t\twith codecs.open(PAGES_DIR + self.start_file, 'r', 'utf-8') as f:\n",
    "\t\t\t\tself.text += f.read()[self.start_pos:]\n",
    "\t\t\tself.text += ' ' # Add a space to prevent word merging\n",
    "\t\t\twith codecs.open(PAGES_DIR + self.end_file, 'r', 'utf-8') as f:\n",
    "\t\t\t\tself.text += f.read()[:self.end_pos]\n",
    "\t\tfor comb in COMBINATIONS_CORR.keys():\n",
    "\t\t\twhile self.text.find(comb) != -1:\n",
    "\t\t\t\tself.text = self.text[:self.text.find(comb)] + COMBINATIONS_CORR[comb] + self.text[self.text.find(comb) + len(comb):]\n",
    "\t\n",
    "\tdef make_xml(self):\n",
    "\t\tself.get_text()\n",
    "\n",
    "\t\tarticle = ET.Element(\"article\", {'uri':self.uri})\n",
    "\t\ttitle = ET.SubElement(article, 'title')\n",
    "\t\ttitle.text = self.title\n",
    "\t\tauthor = ET.SubElement(article, 'authors')\n",
    "\t\ttitle_short = ET.SubElement(article, 'title_short')\n",
    "\t\tpages = ET.SubElement(article, 'pages')\n",
    "\t\tstart = ET.SubElement(pages, 'start')\n",
    "\t\tstart.text = self.start_file[3:self.start_file.find('_', 3)]\n",
    "\t\tend = ET.SubElement(pages, 'end')\n",
    "\t\tend.text = self.end_file[3:self.end_file.find('_', 3)]\n",
    "\t\tliterature = ET.SubElement(article, 'literature')\n",
    "\t\tliterature_orig = ET.SubElement(literature, 'literature_orig')\n",
    "\t\tformulas_remote = ET.SubElement(article, 'formulas_main')\n",
    "\t\tformulas_inline = ET.SubElement(article, 'formulas_aux')\n",
    "\t\trelations = ET.SubElement(article, 'relations')\n",
    "\t\ttext = ET.SubElement(article, 'text')\n",
    "\t\ttext.text = self.text\n",
    "\t\ttext_orig = ET.SubElement(article, 'text_orig')\n",
    "\t\ttext_orig.text = self.text\n",
    "\n",
    "\t\tself.xml = prettify(article)\n",
    "\t\n",
    "\t\n",
    "\n",
    "class Title:\n",
    "\ttext = ''\n",
    "\tfile = ''\n",
    "\tstart_pos = 0\n",
    "\tend_pos = 0\n",
    "\turi = ''\n",
    "\n",
    "\n",
    "def get_title(n, root):\n",
    "\totitle = Title()\n",
    "\tfor title in root:\n",
    "\t\tif int(title.attrib['uri'][30:title.attrib['uri'].find('_', 30)]) == n:\n",
    "\t\t\totitle.uri = title.attrib['uri']\n",
    "\t\t\totitle.text = get_xml_elem(title, 'title').text\n",
    "\t\t\totitle.file = get_xml_elem(title, 'title-meta/title-file').text\n",
    "\t\t\totitle.start_pos = int(get_xml_elem(title, 'title-meta/title-start').text)\n",
    "\t\t\totitle.end_pos = int(get_xml_elem(title, 'title-meta/title-end').text)\n",
    "\treturn otitle\n",
    "\n",
    "\n",
    "root = parse_xml(TITLES_FILE)\n",
    "\n",
    "# Create articles list\n",
    "articles_list = []\n",
    "title = Title()\n",
    "for i in range(len(root)):\n",
    "\ttitle = get_title(i + 1, root)\n",
    "\tif i:\n",
    "\t\tarticles_list[-1].end_file = title.file\n",
    "\t\tarticles_list[-1].end_pos = title.start_pos - 2 # There is a shift for some reason\n",
    "\tarticles_list.append(Article())\n",
    "\tarticles_list[-1].uri = title.uri\n",
    "\tarticles_list[-1].title = title.text\n",
    "\tarticles_list[-1].start_file = title.file\n",
    "\tarticles_list[-1].start_pos = title.end_pos\n",
    "\tarticles_list[-1].end_file = title.file\n",
    "\twith codecs.open(PAGES_DIR + title.file, 'r', 'utf-8') as f:\n",
    "\t\tarticles_list[-1].end_pos = len(f.read())\n",
    "\n",
    "# Parse texts themselves and write\n",
    "for i in range(len(articles_list)):\n",
    "\tarticles_list[i].make_xml()\n",
    "\twith codecs.open(EXIT_DIR + '' + articles_list[i].uri[30:] + '.xml', 'w', 'utf-8') as f:\n",
    "\t\tf.write(articles_list[i].xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Парсинг авторов статьи\n",
    "\n",
    "Ищет в конце текста статей конструкции типа ` [Xxxx]. [Xxxx]. [Xxxx]` или ` [Xxxx].[Xxxx]. [Xxxx]` и итерпретирует её как автора статьи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Парсинг авторов статьи\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "MIN_INLINE_LEN = 0\n",
    "COMBINATIONS_CORR = dict_merge(COMBINATIONS_CORR_GLOBAL, {\n",
    "\t'II' : 'П'\n",
    "})\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = next(walk(ARTICLES_DIR), (None, None, []))[2]  # [] if no file\n",
    "\n",
    "for filename in filenames:\n",
    "\tarticle = parse_xml(ARTICLES_DIR + filename)\n",
    "\ttextelem = get_xml_elem(article, 'text')\n",
    "\ttext = textelem.text\n",
    "\tauthors = get_xml_elem(article, 'authors')\n",
    "\n",
    "\tauth_start = 1\n",
    "\tauth_list = []\n",
    "\twhile auth_start:\n",
    "\t\t# Find first non-space from the end\n",
    "\t\twhile text[-1] == ' ' or text[-1] == '\\n' or text[-1] == '\\r':\n",
    "\t\t\ttext = text[:-1]\n",
    "\n",
    "\t\tauth_start = 0\n",
    "\t\t# Try recognize\n",
    "\t\tfirst_space = max(text.rfind(' ', 0, len(text)), text.rfind('\\n', 0, len(text)), text.rfind('\\r', 0, len(text)))\n",
    "\t\tsecond_space = max(text.rfind(' ', 0, first_space), text.rfind('\\n', 0, first_space), text.rfind('\\r', 0, first_space))\n",
    "\t\tthird_space = max(text.rfind(' ', 0, second_space), text.rfind('\\n', 0, second_space), text.rfind('\\r', 0, second_space))\n",
    "\t\tif first_space >= 0 and text[first_space-1] == '.' and second_space >= 0:\n",
    "\t\t\tif text.find('.', second_space, first_space-1) != -1: # If there's no space between initials\n",
    "\t\t\t\tthird_space = second_space\n",
    "\t\t\t\tsecond_space = first_space\n",
    "\t\t\tif text[second_space-1] == '.' and third_space >= 0:\n",
    "\t\t\t\t# Check if first letters of each word are capitals\n",
    "\t\t\t\tif re.match(r\"[A-ZА-ЯІ]\", text[first_space+1]) != None and re.match(r\"[A-ZА-ЯІ]\", text[second_space+1]) != None and re.match(r\"[A-ZА-ЯІ]\", text[third_space+1]) != None:\n",
    "\t\t\t\t\tauth_start = third_space + 1\n",
    "\n",
    "\t\tif auth_start: # Suggest that an article cannot consist of author only and therefore auth_start should be > 0\n",
    "\t\t\t#print(article.attrib['uri'], author_text)\n",
    "\t\t\tauthor_text = text[auth_start:]\n",
    "\t\t\tif author_text[author_text.find('.')+1] != ' ': # Add space if there's no one between initials\n",
    "\t\t\t\tauthor_text = author_text[:author_text.find('.')+1] + ' ' + author_text[author_text.find('.')+1:]\n",
    "\t\t\tif author_text[-1] == '.' or author_text[-1] == ',':\n",
    "\t\t\t\tauthor_text = author_text[:-1]\n",
    "\t\t\t# convert wrong symbols\n",
    "\t\t\tfor comb in COMBINATIONS_CORR.keys():\n",
    "\t\t\t\twhile author_text.find(comb) != -1:\n",
    "\t\t\t\t\tauthor_text = author_text[:author_text.find(comb)] + COMBINATIONS_CORR[comb] + author_text[author_text.find(comb) + len(comb):]\n",
    "\t\t\t\n",
    "\t\t\tauth_list.append(author_text)\n",
    "\t\t\ttext = text[:auth_start]\n",
    "\n",
    "\t# add authors, reverse their order to alphabetic\n",
    "\tfor auth in reversed(auth_list):\n",
    "\t\tauthor = ET.SubElement(authors, 'author')\n",
    "\t\tauthor.text = auth\n",
    "\n",
    "\ttextelem.text = text\n",
    "\twith codecs.open(ARTICLES_DIR + filename, 'w', 'utf-8') as f:\n",
    "\t\tf.write(prettify(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Парсер формул\n",
    "\n",
    "Выносит из текстов ранее подготовленных xml-файлов статей сначала выносные, а затем строчные формулы, оставляя на их месте ссылку внутри их математического окружения. \n",
    "\n",
    "Минимальная длина в символах, которой должна обладать строчная формула, настраивается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Парсер формул\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "MIN_INLINE_LEN = 0\n",
    "##################################################################\n",
    "\n",
    "\n",
    "def check_in_uri(text:str, pos:int) -> bool:\n",
    "\topen_prev = text.rfind('URI[[', 0, pos)\n",
    "\tclose_prev = text.rfind(']]/URI', 0, pos)\n",
    "\topen_next = text.find('URI[[', pos)\n",
    "\tclose_next = text.find(']]/URI', pos)\n",
    "\tafter_open = True if ((open_prev != -1 and close_prev == -1) or (open_prev > close_prev and open_prev != -1 and close_prev != -1)) else False\n",
    "\tbefore_close = True if ((open_next == -1 and close_next != -1) or (open_next > close_next and open_next != -1 and close_next != -1)) else False\n",
    "\treturn (after_open and before_close)\n",
    "\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = next(walk(ARTICLES_DIR), (None, None, []))[2]  # [] if no file\n",
    "\n",
    "for filename in filenames:\n",
    "\tarticle = parse_xml(ARTICLES_DIR + filename)\n",
    "\t#print('REMOTES: ' + article.attrib['uri'])\n",
    "\ttext = get_xml_elem(article, 'text')\n",
    "\tformulas_main = get_xml_elem(article, 'formulas_main')\n",
    "\tformulas_aux = get_xml_elem(article, 'formulas_aux')\n",
    "\t\t\t\n",
    "# Get main formulas\n",
    "\tpos_find = 0\n",
    "\tpos_start = 0\n",
    "\tpos_end = 0\n",
    "\tn = 1\n",
    "\twhile text.text != None and text.text.find('\\\\[', pos_find) != -1:\n",
    "\t\tpos_start = text.text.find('\\\\[', pos_find) + 2\n",
    "\t\tpos_end = text.text.find('\\\\]', pos_start)\n",
    "\t\twhile text.text[pos_start] == '\\n':\n",
    "\t\t\tpos_start += 1\n",
    "\t\twhile text.text[pos_end-1] == '\\n':\n",
    "\t\t\tpos_end -= 1\n",
    "\t\tpos_find = pos_start\n",
    "\t\turi = 'http://libmeta.ru/fme/formula/main' + article.attrib['uri'][article.attrib['uri'].rfind('/', 0, article.attrib['uri'].find('_')):article.attrib['uri'].find('_')+1] + str(n) + article.attrib['uri'][article.attrib['uri'].find('_'):]\n",
    "\t\tn += 1\n",
    "\t\tformula = ET.SubElement(formulas_main, 'formula', {'uri':uri})\n",
    "\t\tformula.text = text.text[pos_start:pos_end]\n",
    "\t\ttext.text = text.text[:pos_start] + 'URI[[' + uri + ']]/URI' + text.text[pos_end:]\n",
    "\n",
    "# Get auxilary formulas\n",
    "\tpos_find = 0\n",
    "\tpos_start = 0\n",
    "\tpos_end = 0\n",
    "\tcnt = 0\n",
    "\tn = 1\n",
    "\t# Count dollar symbols\n",
    "\twhile text.text != None and text.text.find('$', pos_find) != -1:\n",
    "\t\tpos_find = text.text.find('$', pos_find) + 1\n",
    "\t\tcnt += 1\n",
    "\t# If cnt is not even assume that first one is garbage from title\n",
    "\tpos_find = 0\n",
    "\tif cnt % 2:\n",
    "\t\tpos_find = text.text.find('$', pos_find)\n",
    "\t\ttext.text = text.text[:pos_find] + '#' + text.text[pos_find+1:]\n",
    "\twhile text.text != None and text.text.find('$', pos_find) != -1:\n",
    "\t\tpos_start = text.text.find('$', pos_find) + 1\n",
    "\t\tpos_end = text.text.find('$', pos_start)\n",
    "\t\tif not check_in_uri(text.text, pos_start) and not check_in_uri(text.text, pos_end):\n",
    "\t\t\twhile text.text[pos_start] == '\\n':\n",
    "\t\t\t\tpos_start += 1\n",
    "\t\t\twhile text.text[pos_end-1] == '\\n':\n",
    "\t\t\t\tpos_end -= 1\n",
    "\t\t\tpos_find = pos_start\n",
    "\t\t\tif pos_end - pos_start >= MIN_INLINE_LEN:\n",
    "\t\t\t\turi = 'http://libmeta.ru/fme/formula/aux' + article.attrib['uri'][article.attrib['uri'].rfind('/', 0, article.attrib['uri'].find('_')):article.attrib['uri'].find('_')+1] + str(n) + article.attrib['uri'][article.attrib['uri'].find('_'):]\n",
    "\t\t\t\tn += 1\n",
    "\t\t\t\tformula = ET.SubElement(formulas_aux, 'formula', {'uri':uri})\n",
    "\t\t\t\tformula.text = text.text[pos_start:pos_end]\n",
    "\t\t\t\ttext.text = text.text[:pos_start] + 'URI[[' + uri + ']]/URI' + text.text[pos_end:]\n",
    "\t\t\tpos_find = text.text.find('$', pos_find) + 1\n",
    "\t\telse:\n",
    "\t\t\tpos_find = pos_end + 1\n",
    "\n",
    "\twith codecs.open(ARTICLES_DIR + filename, 'w', 'utf-8') as f:\n",
    "\t\tf.write(prettify(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Вынос формул\n",
    "\n",
    "Выносит все формулы в отдельный файл с указанием типа для возможной последующей обработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1. Вынос формул\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "EXIT_FILE = \"./results/FMEformulas.xml\"\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = next(walk(ARTICLES_DIR), (None, None, []))[2]  # [] if no file\n",
    "\n",
    "\n",
    "formulas = ET.Element('formulas')\n",
    "\n",
    "for filename in filenames:\n",
    "\troot = parse_xml(ARTICLES_DIR + filename)\n",
    "\tfmain = get_xml_elem(root, 'formulas_main')\n",
    "\tfaux = get_xml_elem(root, 'formulas_aux')\n",
    "\t\n",
    "\tfor formula in fmain:\n",
    "\t\tformulas.append(formula)\n",
    "\tfor formula in faux:\n",
    "\t\tformulas.append(formula)\n",
    "\n",
    "with codecs.open(EXIT_FILE, 'w', 'utf-8') as f:\n",
    "\tf.write(prettify(formulas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Проверка формул\n",
    "\n",
    "Случайным образом выбирает 20 случайных формул (из случайных статей) и ставляет их в математическое окружение Markdown для визуальной проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2. Проверка формул\n",
    "\n",
    "############################ VARS ################################\n",
    "ARTICLES_DIR = \"./results/FMEarticles/\"\n",
    "EXIT_FILE = \"./matphys/FMEformulas_check.md\"\n",
    "NUMBER = 20\n",
    "##################################################################\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = next(walk(ARTICLES_DIR), (None, None, []))[2]  # [] if no file\n",
    "\n",
    "\n",
    "file = ''\n",
    "\n",
    "i = 0\n",
    "while i < NUMBER:\n",
    "\troot = parse_xml(ARTICLES_DIR + filenames[randint(0, len(filenames)-1)])\n",
    "\n",
    "\t# Get all the info from article\n",
    "\tfmain = get_xml_elem(root, 'formulas_main')\n",
    "\tstart = get_xml_elem(root, 'pages/start').text\n",
    "\t\n",
    "\n",
    "\t# if there's no formulas in the article try another one\n",
    "\ttotal_num = 0\n",
    "\tfor formula in fmain:\n",
    "\t\ttotal_num += 1\n",
    "\tif not total_num:\n",
    "\t\tcontinue\n",
    "\ti += 1\n",
    "\t\n",
    "\tnum = randint(0, 100) % total_num\n",
    "\n",
    "\tformula = fmain[num].text\n",
    "\n",
    "\tfile += f'{i}. Статья: {root.attrib[\"uri\"]}, Начало на стр. {start}, формула {num + 1}:\\n$${formula}$$\\n'\n",
    "\n",
    "with codecs.open(EXIT_FILE, 'w', 'utf-8') as f:\n",
    "\tf.write(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
