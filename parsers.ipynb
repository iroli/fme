{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Базовый парсер заголовков\n",
    "\n",
    "Вытаскивает из latex-кода заголовки статей и их расположение в файлах.\n",
    "\n",
    "Разбивка происходит в полуручном режиме, т.к. нет уверенности в формате заголовков.\n",
    "\n",
    "В тексте ищутся слова, содержащие в своём составе заглавные буквы на русском и английском языках в отношении, большем или равным заданному (по умолчанию 0.51, при меньших значениях количество вхождений значительно возрастает, например за счёт двухбуквенных предлогов). Предполагается, что таким образом удаётся обнаруживать неправильно машинно распознанный капс. Слова или цепочки слов, состоящие из одного строчного символа включаются в заголовок, если стоят между слов, определённых как часть заголовка. При этом, одиночные заглавные буквы, а также инициалы не воспринимаются как начало заголовка.\n",
    "\n",
    "## Использование\n",
    "- При удовлетворительном определении заголовка нажать `Enter` без дополнительного ввода.\n",
    "- Если предложенное место заголовком не является ввести `\"n\"`\n",
    "- При неправильном определении границ заголовка ввести два корректировочных числа для сдвига левой и правой границы.\n",
    "  - ЗАМЕЧАНИЕ: сдвиг производится попробельно, т.е. двойной пробел будет распознан как слово нулевой длины.\n",
    "  - ЗАМЕЧАНИЕ: границы отображаемого фрагмента текста будут передвинуты автоматически. Длины левой и правой границ в словах задаются в параметрах.\n",
    "  - ПРИМЕРЫ:\n",
    "    - `out: a [B C] d e f` -> `in: 0 2` -> `out: a [B C D E] f`\n",
    "    - `out: a b c [D E] f` -> `in: 2 -1` -> `out: a [B C D] e f`\n",
    "- Также возможен посимвольный сдвиг правой границы в случае \"сращивания\" заголовка статьи и её текста. Ввести одно число, начиная с точки.\n",
    "  - ПРИМЕРЫ:\n",
    "    - `out: a[BC]def` -> `in: .2` -> `out: a[BCDE]f`\n",
    "    - `out: a[BCDE]f` -> `in: .-1` -> `out: a[BCD]ef`\n",
    "\n",
    "В выводе в терминале переносы строк для удобства заменены на `\"$\"`\n",
    "\n",
    "### Прочее\n",
    "- Для определителя капса доступны исключения, которые никогда не будут рассматриваться, как потенциальные начала заголовков, см. опции. По умолчанию: первые 10 римских цифр, \"МэВ\" и \"ГэВ\". Также определитель не реагирует на \"СМ.\", что часто встречается в ссылках сразу после заголовков.\n",
    "- Использовать системный терминал для взаимодействия оказывается удобнее, чем использовать jupyter, поэтому рекомендуется запускать файл `base_titles_parser.py` из терминала или с использованием Python Launcher (но вы всё ещё можете запустить ячейку ниже).\n",
    "- При положительном определении заголовка файл дополняется немедленно, прервать процесс можно в любой момент, как и продолжить после -- итоговый файл будет дополняться, а не перезаписываться с нуля при новом запуске программы (главное не забыть предварительно удалить из конца файла дубликаты, если вы начинаете с той страницы, на которой закончили в прошлый раз, а не со следующей).\n",
    "- В случае пропуска парсером заголовка его можно добавить вручную двумя способами:\n",
    "  1) Сдвинуть границы заголовка назад, как описано в инструкции выше. Подходит, если была пропущена небольшая (обычно ссылочная) статья, примерно 20 слов, плюс-минус. При этом после ввода заголовка поиск продолжится с __его__ конца, поэтому следующий заголовок \"вместо\" которого был введён пропущенный будет определён заново и пропущен не будет.\n",
    "  2) Воспользоваться ячейкой 1.1. Для этого в сыром tex-файле страницы нужно отыскать заголовок, скопировать его и __в точности__ вставить в разделе параметров, а также указать номер страницы. Скрипт парсера при этом можно не закрывать, последующая нумерация подстроится автоматически."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !pip3 uninstall enchant\n",
    "# !pip3 uninstall pyenchant\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import base_titles_parser\n",
    "import importlib\n",
    "\n",
    "importlib.reload(base_titles_parser)\n",
    "base_titles_parser.run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1. Добавление заголовков по одному\n",
    "\n",
    "В разделе параметров указать номер страницы и ТОЧНУЮ формулировку заголовка из сырого latex-текста, а также номер страницы, после чего запустить ячейку.\n",
    "\n",
    "Закрывать скрипт парсера не обязательно, это не вызовет ошибок и его нумерация подстроится автоматически."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'enchant' from 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\enchant\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "# 1.1. Добавление заголовков по одному\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "PAGES_DIR = GLOBAL_WORK_DIR + GLOBAL_PAGES_DIR\n",
    "EXIT_DIR = GLOBAL_WORK_DIR\n",
    "EXIT_FILE = SINGLE_TITLE_PARSER_OUTPUT_FILE\n",
    "# Search parameters\n",
    "PAGE = 146\n",
    "TITLE = 'глюоний'\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "class Article:\n",
    "    start_title = 0\n",
    "    end_title = 0\n",
    "    filename = ''\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames_raw = get_filenames(PAGES_DIR)\n",
    "filenames = []\n",
    "for i in range(PAGE, PAGE + 1):\n",
    "    for filename in filenames_raw:\n",
    "        beginning = \"rp-\" + str(i) + \"_\"\n",
    "        if filename[:len(beginning)] == beginning and filename[-4:] == \".mmd\":\n",
    "            filenames.append(filename)\n",
    "\n",
    "\n",
    "# Check for existing xml\n",
    "filenames_raw = get_filenames(EXIT_DIR)\n",
    "if not(EXIT_FILE in filenames_raw):\n",
    "    root = ElementTree.Element('data')\n",
    "    xml_write(root, EXIT_DIR + EXIT_FILE)\n",
    "\n",
    "\n",
    "root = parse_xml(EXIT_DIR + EXIT_FILE)\n",
    "\n",
    "\n",
    "# Add article title and metadata to xml tree\n",
    "def add_article(article_local:Article, etree_root:ElementTree.Element, number:int):\n",
    "    elem_article = ElementTree.SubElement(etree_root, 'article', {'n':str(number)})\n",
    "    elem_title = ElementTree.SubElement(elem_article, 'title')\n",
    "    elem_title.text = file[article_local.start_title + 1:article_local.end_title]\n",
    "    elem_title_meta = ElementTree.SubElement(elem_article, 'title-meta')\n",
    "    elem_title_file = ElementTree.SubElement(elem_title_meta, 'title-file')\n",
    "    elem_title_file.text = article_local.filename\n",
    "    elem_title_start = ElementTree.SubElement(elem_title_meta, 'title-start')\n",
    "    elem_title_start.text = str(article_local.start_title + 1)\n",
    "    elem_title_end = ElementTree.SubElement(elem_title_meta, 'title-end')\n",
    "    elem_title_end.text = str(article_local.end_title)\n",
    "    xml_write(etree_root, EXIT_DIR + EXIT_FILE)\n",
    "\n",
    "# Read requested file\n",
    "with codecs.open(PAGES_DIR + filenames[0], 'r', 'utf-8') as f:\n",
    "    file = f.read()\n",
    "\n",
    "# Find titles and add them\n",
    "start_title = 0\n",
    "end_title = 0\n",
    "num = len(root) + 1\n",
    "while file.find(TITLE, end_title) != -1:\n",
    "    start_title = file.find(TITLE, start_title)\n",
    "    end_title = start_title + len(TITLE)\n",
    "    start_title -= 1 # Set on space before the title\n",
    "\n",
    "    article = Article()\n",
    "    article.start_title = max(start_title, 0)\n",
    "    article.end_title = min(end_title, len(file))\n",
    "    article.filename = filenames[0]\n",
    "    add_article(article, root, num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Исправление ошибок в заголовках\n",
    "\n",
    "Состоит из двух частей: \"составитель пар\" и \"подстановщик\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1. Составитель пар \"оригинальный - исправленный\" для заголовков\n",
    "\n",
    "Формирует xml-список всех заголовков с возможными автоматическими исправлениями (в формате было / стало):\n",
    "1. замена латиницы на аналогичную кириллицу;\n",
    "2. замена заданных буквосочетаний (см. параметры)\n",
    "3. удаление обрамляющих знаков препинания;\n",
    "4. замена всех букв на заглавные (в том числе это избавляет дальнейшей необходимости исправлять имена);\n",
    "5. слияние разорванных на отдельные буквы слов (если рядом оказываются несколько таких слов, то они оказываются слиты вместе).\n",
    "\n",
    "Этот список необходимо просмотреть и исправить оставшиеся ошибки.\n",
    "\n",
    "Дополнительно, для помощи в поиске орфографических ошибок, формируется строка с изменениями, предложенными спеллчекером. ВНИМАНИЕ: спеллчекер может делать ошибки в именах, специфических терминах и т.п., поэтому следует использовать его результаты лишь для ориентира."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing general corrections...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c85b77ecddf4f33af2083ffa9a28d4e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f180e80efc14be5bb2cdd23f55d1b51"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53b5da4952f64b44988213215feee834"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0190acb85c547418a17111029092f2b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9336913cf5d8445388e815d0bb0ed3fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spellcheck...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38b19e54b207457496752f9eee6428a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2.1. Составитель пар \"оригинальный - исправленный\" для заголовков:\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "WORK_DIR = GLOBAL_WORK_DIR\n",
    "INPUT_FILE = BASE_TITLES_PARSER_OUTPUT_FILE\n",
    "CORRECTION_FILE = TITLES_CHECKER_CORRECTIONS_FILE\n",
    "COMBINATIONS_CORR = dict_merge(COMBINATIONS_CORR_GLOBAL, {\n",
    "    'ХК' : 'Ж',\n",
    "    'ЬI' : 'Ы',\n",
    "    'II' : 'Ш',\n",
    "    'I' : 'П',\n",
    "    'J' : 'Л',\n",
    "    'ЛАГРАНХ' : 'ЛАГРАНЖ',\n",
    "    'ЛАТРАНХ' : 'ЛАГРАНЖ',\n",
    "})\n",
    "SPELLCHECK_ONLY = False # Use if the only thing you need from this script is spellcheck\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Check for existing xml\n",
    "filenames_raw = get_filenames(WORK_DIR)\n",
    "if not(INPUT_FILE in filenames_raw):\n",
    "    root = ElementTree.Element('data')\n",
    "    xml_write(root, WORK_DIR + CORRECTION_FILE)\n",
    "\n",
    "\n",
    "root = parse_xml(WORK_DIR + INPUT_FILE)\n",
    "\n",
    "\n",
    "# Get all the titles into a dict\n",
    "titles_dict = {}\n",
    "pages_dict = {}\n",
    "for article in root:\n",
    "    title = get_xml_elem(article, 'title').text\n",
    "    titles_dict[title] = (title, title)\n",
    "    title_file = get_xml_elem(article, 'title-meta/title-file')\n",
    "    pages_dict[title] = title_file.text[title_file.text.find('-')+1:title_file.text.find('_')]\n",
    "\n",
    "\n",
    "if not SPELLCHECK_ONLY:\n",
    "    print(\"Processing general corrections...\")\n",
    "\n",
    "    # Correct preferred combinations and latin letters\n",
    "    for title in tqdm(titles_dict.keys()):\n",
    "        title_new = title_handle_latin(titles_dict[title][0], COMBINATIONS_CORR)\n",
    "        titles_dict[title] = (title_new, title_new)\n",
    "\n",
    "    # Remove bounding symbols\n",
    "    for title in tqdm(titles_dict.keys()):\n",
    "        title_new = title_handle_bounding(titles_dict[title][0])\n",
    "        titles_dict[title] = (title_new, title_new)\n",
    "\n",
    "    # CAPS\n",
    "    for title in tqdm(titles_dict.keys()):\n",
    "        title_new = titles_dict[title][0].upper()\n",
    "        titles_dict[title] = (title_new, title_new)\n",
    "\n",
    "    # Merge single-lettered words\n",
    "    for title in tqdm(titles_dict.keys()):\n",
    "        title_new = title_handle_merge(titles_dict[title][0])\n",
    "        titles_dict[title] = (title_new, title_new)\n",
    "\n",
    "    # Revert changes for aux formulas in titles\n",
    "    for title in tqdm(titles_dict.keys()):\n",
    "        title_new = title_handle_formulas(titles_dict[title][0], title)\n",
    "        titles_dict[title] = (title_new, title_new)\n",
    "\n",
    "# Try spellcheck on titles\n",
    "print(\"Processing spellcheck...\")\n",
    "spellcheck_dict_update()\n",
    "for title in tqdm(titles_dict.keys()):\n",
    "    title_new = titles_dict[title][0]\n",
    "    title_suggestions = do_spellcheck(title_new)\n",
    "    for i in range(len(title_new)):\n",
    "        title_new = title_new[:i] + ('_' if title_new[i] not in [' ', '\\n', '\\r'] else title_new[i]) + (title_new[i+1:] if i + 1 <= len(title_new) else '')\n",
    "    for pos in sorted(title_suggestions.keys(), reverse=True):\n",
    "        title_new = title_new[:pos] + title_suggestions[pos][1] + title_new[pos+len(title_suggestions[pos][0]):]\n",
    "    titles_dict[title] = (titles_dict[title][0], title_new)\n",
    "\n",
    "\n",
    "# Write corrections xml\n",
    "root = ElementTree.Element('data')\n",
    "for i in titles_dict.items():\n",
    "    pair = ElementTree.SubElement(root, 'pair')\n",
    "    title_old = ElementTree.SubElement(pair, 'title_old')\n",
    "    title_old.text = i[0]\n",
    "    title_new = ElementTree.SubElement(pair, 'title_new')\n",
    "    title_new.text = i[1][0]\n",
    "    title_new = ElementTree.SubElement(pair, 'title__sc')\n",
    "    title_new.text = i[1][1]\n",
    "    page = ElementTree.SubElement(pair, 'page')\n",
    "    page.text = pages_dict[i[0]]\n",
    "xml_write(root, WORK_DIR + CORRECTION_FILE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2. Подстановщик исправленных заголовков\n",
    "\n",
    "Заменяет все заголовки на исправленные согласно списку пар."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b39081ffd31248408d098d61edec1251"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dea1b57800ad480a845f2cc6ffff528c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2.2. Подстановщик исправленных заголовков:\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "WORK_DIR = GLOBAL_WORK_DIR\n",
    "INPUT_FILE = BASE_TITLES_PARSER_OUTPUT_FILE\n",
    "CORRECTION_FILE = TITLES_CHECKER_CORRECTIONS_FILE\n",
    "EXIT_FILE = CHECKED_TITLES_FILE\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "root = parse_xml(WORK_DIR + CORRECTION_FILE)\n",
    "\n",
    "\n",
    "# Get all the corrections into a dict\n",
    "titles_dict = {}\n",
    "for pair in tqdm(root):\n",
    "    titles_dict[get_xml_elem(pair, 'title_old').text] = get_xml_elem(pair, 'title_new').text\n",
    "\n",
    "\n",
    "root = parse_xml(WORK_DIR + INPUT_FILE)\n",
    "\n",
    "\n",
    "# Replace titles\n",
    "for article in tqdm(root):\n",
    "    get_xml_elem(article, 'title').text = titles_dict[get_xml_elem(article, 'title').text]\n",
    "xml_write(root, WORK_DIR + EXIT_FILE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Сортировщик / сливщик файлов с заголовками\n",
    "\n",
    "Сортирует статьи в файлах из основного списка в порядке страница-расположение, т.е. (если не сказано иного) в алфавитном порядке и выводит в один выходной файл. Также порядковый номер заменяется uri формата \"http://libmeta.ru/me/article/1_Kraevaya\". (Созданные uri кешируются по номеру страницы и позиции заголовка в тексте и при последующих запусках остаются неизменными, если включен `URI_SAFER`).\n",
    "\n",
    "Также в конец выходного файл добавляются заголовки из \"ручного\" файла, в том же формате, но без сортировки, что позволяет добавлять случайно забытые статьи без изменения uri и имён файлов всех остальных статей."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'enchant' from 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\enchant\\\\__init__.py'>\n",
      "Parsing main input files...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "424c2be804ba42d2903e0bcf6845a398"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing \"manual\" file...\n",
      "Writing articles...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3584 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "883e2265923b40c6937ce58314d45a9f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b7a42b026a84135b1452b71538ecce0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Сортировщик / сливщик файлов с заголовками\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "WORK_DIR = GLOBAL_RESULTS_DIR\n",
    "TITLES_DIR = GLOBAL_TITLES_DIR\n",
    "MANUALLY_ADDED_FILE = SINGLE_TITLE_PARSER_OUTPUT_FILE\n",
    "URI_CACHE = GLOBAL_URI_CACHE\n",
    "INPUT_FILES = get_filenames(WORK_DIR + TITLES_DIR)\n",
    "INPUT_FILES.remove(URI_CACHE)\n",
    "INPUT_FILES.remove(MANUALLY_ADDED_FILE)\n",
    "EXIT_FILE = GLOBAL_MERGED_TITLES_FILE\n",
    "# Uri safer prevents already existing uri from being changed. Set to False ONLY IF you need to update an existing uris.\n",
    "URI_SAFER = True\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "class Article:\n",
    "    title = ''\n",
    "    start_title = ''\n",
    "    end_title = ''\n",
    "    filename = ''\n",
    "\n",
    "\n",
    "\n",
    "# Try to get uri from the cache for title with given page and pos\n",
    "def get_uri(title_page:str, title_pos:str) -> str:\n",
    "    global cache_root\n",
    "    for elem_uri in cache_root:\n",
    "        if elem_uri.tag == 'uri' and elem_uri.attrib['page'] == title_page and elem_uri.attrib['pos'] == title_pos:\n",
    "            return elem_uri.text\n",
    "    return ''\n",
    "# Cache given uri\n",
    "def cache_uri(title_page:str, title_pos:str, uri_str:str):\n",
    "    global cache_root\n",
    "    elem_uri = ElementTree.SubElement(cache_root, 'uri', {'page':title_page, 'pos':title_pos})\n",
    "    elem_uri.text = uri_str\n",
    "    xml_write(cache_root, WORK_DIR + TITLES_DIR + URI_CACHE)\n",
    "\n",
    "\n",
    "# Add article title and metadata to xml tree\n",
    "def add_article(article_local:Article, etree_root:ElementTree.Element, number:int):\n",
    "    page_str = article_local.filename[article_local.filename.find('-') + 1: article_local.filename.find('_')]\n",
    "    uri_cached = get_uri(page_str, article_local.start_title)\n",
    "    translitted = translit(article_local.title[:article_local.title.find(' ')], 'ru', True)\n",
    "    while translitted.find('/') != -1:\n",
    "        translitted = translitted[:translitted.find('/')] + '_' + translitted[translitted.find('/')+1:]\t\t# Prevent slash being counted as subfolder in further\n",
    "    uri_str = URI_PREFIX + \"article/\" + str(number) + \"_\" + translitted\n",
    "    if URI_SAFER and uri_cached != '':\n",
    "        uri_str = uri_cached\n",
    "    else:\n",
    "        cache_uri(page_str, article_local.start_title, uri_str)\n",
    "    elem_article = ElementTree.SubElement(etree_root, 'article', {'uri':uri_str})\n",
    "    elem_title = ElementTree.SubElement(elem_article, 'title')\n",
    "    elem_title.text = article_local.title\n",
    "    elem_title_meta = ElementTree.SubElement(elem_article, 'title-meta')\n",
    "    elem_title_file = ElementTree.SubElement(elem_title_meta, 'title-file')\n",
    "    elem_title_file.text = article_local.filename\n",
    "    elem_title_start = ElementTree.SubElement(elem_title_meta, 'title-start')\n",
    "    elem_title_start.text = str(int(article_local.start_title) + 1)\n",
    "    elem_title_end = ElementTree.SubElement(elem_title_meta, 'title-end')\n",
    "    elem_title_end.text = article_local.end_title\n",
    "\n",
    "\n",
    "# Check for existing uri list\n",
    "filenames_raw = get_filenames(WORK_DIR + TITLES_DIR)\n",
    "if not(URI_CACHE in filenames_raw):\n",
    "    root = ElementTree.Element('data')\n",
    "    xml_write(root, WORK_DIR + TITLES_DIR + URI_CACHE)\n",
    "cache_root = parse_xml(WORK_DIR + TITLES_DIR + URI_CACHE)\n",
    "\n",
    "\n",
    "# Collect all the articles\n",
    "print(\"Parsing main input files...\")\n",
    "articles_dict = {}\n",
    "for filename in tqdm(INPUT_FILES):\n",
    "    root = parse_xml(WORK_DIR + TITLES_DIR + filename)\n",
    "    for article in root:\n",
    "        title = get_xml_elem(article, 'title').text\n",
    "        elem = get_xml_elem(article, 'title-meta/title-file')\n",
    "        page = elem.text[elem.text.find('-')+1:elem.text.find('_')]\n",
    "        pos = get_xml_elem(article, 'title-meta/title-start').text\n",
    "        start = get_xml_elem(article, 'title-meta/title-start').text\n",
    "        end = get_xml_elem(article, 'title-meta/title-end').text\n",
    "        file = get_xml_elem(article, 'title-meta/title-file').text\n",
    "        num = (int(page), int(pos))\n",
    "        articles_dict[num] = {'title':title, 'file':file, 'start':start, 'end':end}\n",
    "\n",
    "# Same for manually added articles\n",
    "articles_dict_man = {}\n",
    "nums_list_man = []\n",
    "if len(MANUALLY_ADDED_FILE):\n",
    "    print(\"Parsing \\\"manual\\\" file...\")\n",
    "    root = parse_xml(WORK_DIR + TITLES_DIR + MANUALLY_ADDED_FILE)\n",
    "    for article in root:\n",
    "        title = get_xml_elem(article, 'title').text\n",
    "        elem = get_xml_elem(article, 'title-meta/title-file')\n",
    "        page = elem.text[elem.text.find('-')+1:elem.text.find('_')]\n",
    "        pos = get_xml_elem(article, 'title-meta/title-start').text\n",
    "        start = get_xml_elem(article, 'title-meta/title-start').text\n",
    "        end = get_xml_elem(article, 'title-meta/title-end').text\n",
    "        file = get_xml_elem(article, 'title-meta/title-file').text\n",
    "        num = (int(page), int(pos))\n",
    "        articles_dict_man[num] = {'title':title, 'file':file, 'start':start, 'end':end}\n",
    "        nums_list_man.append(num)\n",
    "\n",
    "\n",
    "# Sort keys and write articles accordingly\n",
    "root = ElementTree.Element('data')\n",
    "nums_list = sorted(list(i for i in articles_dict.keys()))\n",
    "print(\"Writing articles...\")\n",
    "for num in tqdm(range(len(nums_list))):\n",
    "    article = Article()\n",
    "    article.title = articles_dict[nums_list[num]]['title']\n",
    "    article.start_title = articles_dict[nums_list[num]]['start']\n",
    "    article.end_title = articles_dict[nums_list[num]]['end']\n",
    "    article.filename = articles_dict[nums_list[num]]['file']\n",
    "    add_article(article, root, num + 1)\n",
    "if len(MANUALLY_ADDED_FILE):\n",
    "    for num in tqdm(range(len(nums_list_man))):\n",
    "        article = Article()\n",
    "        article.title = articles_dict_man[nums_list_man[num]]['title']\n",
    "        article.start_title = articles_dict_man[nums_list_man[num]]['start']\n",
    "        article.end_title = articles_dict_man[nums_list_man[num]]['end']\n",
    "        article.filename = articles_dict_man[nums_list_man[num]]['file']\n",
    "        add_article(article, root, num + 1 + len(nums_list))\n",
    "xml_write(root, WORK_DIR + EXIT_FILE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Парсер текстов статей\n",
    "\n",
    "По информации из указанного файла с заголовками вытаскивает в сыром виде тексты статей. Каждая статья помещается в свой .xml файл, с именем, содержащим номер статьи и первое слово из заголовка транслитом."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting articles info...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1501cf7d47f4465da98e226d5f859041"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing articles...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7a42edcb56c4ef0a91819abc2ea04f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Парсер текстов статей\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "TITLES_FILE = GLOBAL_RESULTS_DIR + GLOBAL_MERGED_TITLES_FILE\n",
    "PAGES_DIR = GLOBAL_WORK_DIR + GLOBAL_PAGES_DIR\n",
    "EXIT_DIR = GLOBAL_RESULTS_DIR + GLOBAL_ARTICLES_DIR\n",
    "COMBINATIONS_CORR = {\n",
    "    'І' : 'I'\t\t# These two are different!\n",
    "}\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Article:\n",
    "    start_file = ''\n",
    "    start_pos = 0\n",
    "    end_file = ''\n",
    "    end_pos = 0\n",
    "    text = ''\n",
    "    text_orig = ''\n",
    "    uri = ''\n",
    "    num = ''\n",
    "    title = ''\n",
    "    xml = ''\n",
    "\n",
    "    def get_text(self):\n",
    "        # Get filenames\n",
    "        filenames_raw_local = get_filenames(PAGES_DIR)\n",
    "        filenames_local = []\n",
    "        for filename_local in filenames_raw_local:\n",
    "            if filename_local[-4:] == \".mmd\":\n",
    "                filenames_local.append(filename_local)\n",
    "        if self.start_file == self.end_file:\n",
    "            with codecs.open(PAGES_DIR + self.start_file, 'r', 'utf-8') as f_in:\n",
    "                self.text += f_in.read()[self.start_pos:self.end_pos]\n",
    "        else:\n",
    "            with codecs.open(PAGES_DIR + self.start_file, 'r', 'utf-8') as f_in:\n",
    "                self.text += f_in.read()[self.start_pos:]\n",
    "            for page_local in range(int(self.start_file[3:self.start_file.find('_')]) + 1, int(self.end_file[3:self.end_file.find('_')])):\n",
    "                for filename_local in filenames_local:\n",
    "                    if int(filename_local[3:filename_local.find('_')]) == page_local:\n",
    "                        self.text += ' ' # Add a space to prevent word merging\n",
    "                        with codecs.open(PAGES_DIR + filename_local, 'r', 'utf-8') as f_in:\n",
    "                            self.text += f_in.read()\n",
    "            self.text += ' ' # Add a space to prevent word merging\n",
    "            with codecs.open(PAGES_DIR + self.end_file, 'r', 'utf-8') as f_in:\n",
    "                self.text += f_in.read()[:self.end_pos]\n",
    "        for comb_local in COMBINATIONS_CORR.keys():\n",
    "            while self.text.find(comb_local) != -1:\n",
    "                self.text = self.text[:self.text.find(comb_local)] + COMBINATIONS_CORR[comb_local] + self.text[self.text.find(comb_local) + len(comb_local):]\n",
    "        while self.text is not None and len(self.text) and self.text[0] in [' ', ',', '.', ':', ';', '-', '\\n', '\\r']:\n",
    "            self.text = self.text[1:]\n",
    "        while self.text is not None and len(self.text) and self.text[-1] in [' ', '\\n', '\\r']:\n",
    "            self.text = self.text[:-1]\n",
    "        self.text_orig = self.text\n",
    "        # Fix several capital symbols per word\n",
    "        word_left = 0\n",
    "        while word_left < len(self.text):\n",
    "            word_right = min(len(self.text), self.text.find(' ', word_left) if self.text.find(' ', word_left) != -1 else len(self.text))\n",
    "            word_right = min(word_right, self.text.find('\\n', word_left) if self.text.find('\\n', word_left) != -1 else len(self.text))\n",
    "            word_right = min(word_right, self.text.find('\\r', word_left) if self.text.find('\\r', word_left) != -1 else len(self.text))\n",
    "            word_right = min(word_right, self.text.find('-', word_left) if self.text.find('-', word_left) != -1 else len(self.text))\n",
    "            word_right = min(word_right, self.text.find('.', word_left) if self.text.find('.', word_left) != -1 else len(self.text))\n",
    "            word_str = self.text[word_left:word_right]\n",
    "            if word_str is not None and len(word_str) > 1 and not check_in_uri(self.text, word_left) and not check_in_formula(self.text, word_left) and not check_in_link(self.text, word_left):\n",
    "                word_str = word_str[0] + word_str[1:len(word_str)].lower()\n",
    "                self.text = self.text[:word_left] + word_str + self.text[word_right:]\n",
    "            word_left = word_right + 1\n",
    "\n",
    "    def make_xml(self):\n",
    "        self.get_text()\n",
    "\n",
    "        elem_article = ElementTree.Element(\"article\", {'uri':self.uri, 'alphabetic_pos':self.num})\n",
    "        elem_title = ElementTree.SubElement(elem_article, 'title')\n",
    "        elem_title.text = self.title\n",
    "        elem_author = ElementTree.SubElement(elem_article, 'authors')\n",
    "        elem_author.text = None\n",
    "        #elem_title_short = ElementTree.SubElement(elem_article, 'title_short')\n",
    "        #elem_title_short.text = None\n",
    "        elem_pages = ElementTree.SubElement(elem_article, 'pages')\n",
    "        elem_start = ElementTree.SubElement(elem_pages, 'start')\n",
    "        elem_start.text = self.start_file[3:self.start_file.find('_', 3)]\n",
    "        elem_end = ElementTree.SubElement(elem_pages, 'end')\n",
    "        elem_end.text = self.end_file[3:self.end_file.find('_', 3)]\n",
    "        elem_literature = ElementTree.SubElement(elem_article, 'literature')\n",
    "        elem_literature_orig = ElementTree.SubElement(elem_literature, 'literature_orig')\n",
    "        elem_literature_orig.text = None\n",
    "        elem_formulas_remote = ElementTree.SubElement(elem_article, 'formulas_main')\n",
    "        elem_formulas_remote.text = None\n",
    "        elem_formulas_inline = ElementTree.SubElement(elem_article, 'formulas_aux')\n",
    "        elem_formulas_inline.text = None\n",
    "        elem_relations = ElementTree.SubElement(elem_article, 'relations', {'n': '0'})\n",
    "        elem_relations.text = None\n",
    "        elem_text = ElementTree.SubElement(elem_article, 'text')\n",
    "        elem_text.text = self.text\n",
    "        elem_text_orig = ElementTree.SubElement(elem_article, 'text_orig')\n",
    "        elem_text_orig.text = self.text_orig\n",
    "\n",
    "        self.xml = prettify(elem_article)\n",
    "\n",
    "\n",
    "\n",
    "class Title:\n",
    "    text = ''\n",
    "    file = ''\n",
    "    start_pos = 0\n",
    "    end_pos = 0\n",
    "    uri = ''\n",
    "\n",
    "\n",
    "def get_titles_dict(etree_root:ElementTree.Element) -> dict:\n",
    "    titles_dict_local = {}\n",
    "    for elem_title in etree_root:\n",
    "        elem_uri = elem_title.attrib['uri']\n",
    "        elem_text = get_xml_elem(elem_title, 'title').text\n",
    "        elem_file = get_xml_elem(elem_title, 'title-meta/title-file').text\n",
    "        elem_page = int(elem_file[elem_file.find('-') + 1 : elem_file.find('_')])\n",
    "        elem_start_pos = int(get_xml_elem(elem_title, 'title-meta/title-start').text)\n",
    "        elem_end_pos = int(get_xml_elem(elem_title, 'title-meta/title-end').text)\n",
    "        titles_dict_local[(elem_page, elem_start_pos)] = Title()\n",
    "        titles_dict_local[(elem_page, elem_start_pos)].uri = elem_uri\n",
    "        titles_dict_local[(elem_page, elem_start_pos)].text = elem_text\n",
    "        titles_dict_local[(elem_page, elem_start_pos)].file = elem_file\n",
    "        titles_dict_local[(elem_page, elem_start_pos)].start_pos = elem_start_pos\n",
    "        titles_dict_local[(elem_page, elem_start_pos)].end_pos = elem_end_pos\n",
    "    return titles_dict_local\n",
    "\n",
    "\n",
    "def get_title(number:int, dict_with_titles:dict) -> Title:\n",
    "    out_title = Title()\n",
    "    titles_dict_keys = sorted(dict_with_titles.keys())\n",
    "    for p in range(len(titles_dict_keys)):\n",
    "        if p == number:\n",
    "            out_title = dict_with_titles[titles_dict_keys[p]]\n",
    "    return out_title\n",
    "\n",
    "\n",
    "root = parse_xml(TITLES_FILE)\n",
    "\n",
    "# Create articles list\n",
    "articles_list = []\n",
    "title = Title()\n",
    "titles_dict = get_titles_dict(root)\n",
    "print(\"Getting articles info...\")\n",
    "for i in tqdm(range(len(root))):\n",
    "    title = get_title(i, titles_dict)\n",
    "    if i:\n",
    "        articles_list[-1].end_file = title.file\n",
    "        articles_list[-1].end_pos = max(title.start_pos - 2, 0) # There is a shift for some reason\n",
    "    articles_list.append(Article())\n",
    "    articles_list[-1].uri = title.uri\n",
    "    articles_list[-1].num = str(i + 1)\n",
    "    articles_list[-1].title = title.text\n",
    "    articles_list[-1].start_file = title.file\n",
    "    articles_list[-1].start_pos = title.end_pos\n",
    "    articles_list[-1].end_file = title.file\n",
    "    with codecs.open(PAGES_DIR + title.file, 'r', 'utf-8') as f:\n",
    "        articles_list[-1].end_pos = len(f.read())\n",
    "\n",
    "# Parse texts themselves and write\n",
    "print(\"Parsing articles...\")\n",
    "for i in tqdm(range(len(articles_list))):\n",
    "    articles_list[i].make_xml()\n",
    "    with codecs.open(EXIT_DIR + '' + articles_list[i].uri[len(URI_PREFIX) + 8:] + '.xml', 'w', 'utf-8') as f:\n",
    "        f.write(articles_list[i].xml)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Проверка правописания в текстах\n",
    "\n",
    "## 5.1. Сканер\n",
    "\n",
    "Сканирует тексты из указанного диапазона статей и выносит все показавшиеся подозрительными слова в отдельный xml следующего формата:\n",
    "- Статья (имя файла в аттрибутах)\n",
    "  - Слово (позиция в тексте и флаги в аттрибутах)\n",
    "    - Исходный вариант\n",
    "    - Контекстная строка (размер задаётся в разделе параметров скрипта)\n",
    "    - Предложенная замена\n",
    "\n",
    "Предлагается два флага для определения дальнейшей \"судьбы\" слова: \"результат\" (0 -- исходное, 1 -- предложенное) и \"добавление в словарь\" (0 -- не добавлять, 1 -- добавить как есть, 2 -- перевести в нижний регистр и добавить (для первого слова в предложении), 3 -- сделать первую букву заглавной и добавить (для имён, случайно распознанных без заглавной; применяется к выбранному результату)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7bccab7e805468da5403708e7fd1ce1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases found: 0\n"
     ]
    }
   ],
   "source": [
    "# 5.1. Проверка правописания в текстах. Сканер.\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "ARTICLES_DIR = GLOBAL_RESULTS_DIR + GLOBAL_ARTICLES_DIR\n",
    "EXIT_DIR = GLOBAL_WORK_DIR\n",
    "CONTEXT_SIZE = 20\n",
    "START_ARTICLE = 3587\n",
    "END_ARTICLE = 3586\n",
    "# Flags for usual cases\n",
    "DEFAULT_RESULT_FLAG = '1'\n",
    "DEFAULT_ADD_TO_PWL_FLAG = '0'\n",
    "# Flags if name is detected\n",
    "\"\"\"NAME_RESULT_FLAG = '0'\n",
    "NAME_ADD_TO_PWL_FLAG = '1'\"\"\"\n",
    "# Cases that have to be overriden\n",
    "OVERRIDE_FORCE_CYRILLIC = {\n",
    "    'Ссср' : 'СССР',\n",
    "    'Усср' : 'УССР',\n",
    "    'Церн' : 'ЦЕРН'\n",
    "}\n",
    "OVERRIDE_AS_IS = {\n",
    "}\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "spellcheck_dict_update()\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "#filenames = ['4_ABELEVA.xml']\n",
    "\n",
    "root = ElementTree.Element('data')\n",
    "\n",
    "total_wois = 0\n",
    "for filename in tqdm(filenames):\n",
    "    article_number = int(filename[:filename.find('_')])\n",
    "    if article_number < START_ARTICLE or article_number > END_ARTICLE:\n",
    "        continue\n",
    "\n",
    "    #print(f'{filename}: found ', end='')\n",
    "    article = parse_xml(ARTICLES_DIR + filename)\n",
    "    text = get_xml_elem(article, 'text')\n",
    "    if text.text is None:\n",
    "        continue\n",
    "    elif not len(text.text):\n",
    "        continue\n",
    "\n",
    "    #add_to_pwl(filename[filename.find('_')+1:filename.find('.xml')])\n",
    "\n",
    "    text_suggestions = do_spellcheck(text.text)\n",
    "    #print(len(text_suggestions.keys()))\n",
    "    total_wois += len(text_suggestions.keys())\n",
    "    if len(text_suggestions.keys()):\n",
    "        article = ElementTree.SubElement(root, 'article', {'filename': filename})\n",
    "        for pos in text_suggestions.keys():\n",
    "            #print(f'{pos}: {text_suggestions[pos][0]} -> {text_suggestions[pos][1]}')\n",
    "            local_result_flag = DEFAULT_RESULT_FLAG\n",
    "            local_add_to_pwl_flag = DEFAULT_ADD_TO_PWL_FLAG\n",
    "            # Process possible name case\n",
    "            '''if len(text_suggestions[pos][0]) >= 2 and len(text_suggestions[pos][1]) >= 2:\n",
    "                is_name_orig = re.match(r\"[А-ЯA-Z]\", text_suggestions[pos][0][0]) is not None and re.match(r\"[а-яa-z]\", text_suggestions[pos][0][1]) is not None\n",
    "                is_name_sugg = re.match(r\"[А-ЯA-Z]\", text_suggestions[pos][1][0]) is not None and re.match(r\"[а-яa-z]\", text_suggestions[pos][1][1]) is not None\n",
    "                if is_name_orig and is_name_sugg:\n",
    "                    local_result_flag = NAME_RESULT_FLAG\n",
    "                    local_add_to_pwl_flag = NAME_ADD_TO_PWL_FLAG'''\n",
    "            # Override specific cases\n",
    "            suggestion_text = text_suggestions[pos][1]\n",
    "            if title_handle_latin(text_suggestions[pos][0], COMBINATIONS_CORR_GLOBAL) in OVERRIDE_FORCE_CYRILLIC.keys():\n",
    "                suggestion_text = OVERRIDE_FORCE_CYRILLIC[title_handle_latin(text_suggestions[pos][0], COMBINATIONS_CORR_GLOBAL)]\n",
    "                local_result_flag = DEFAULT_RESULT_FLAG\n",
    "                local_add_to_pwl_flag = DEFAULT_ADD_TO_PWL_FLAG\n",
    "            if text_suggestions[pos][0] in OVERRIDE_AS_IS.keys():\n",
    "                suggestion_text = OVERRIDE_AS_IS[text_suggestions[pos][0]]\n",
    "                local_result_flag = DEFAULT_RESULT_FLAG\n",
    "                local_add_to_pwl_flag = DEFAULT_ADD_TO_PWL_FLAG\n",
    "            word = ElementTree.SubElement(article, 'word', {'pos': str(pos), 'result': local_result_flag, 'add_to_pwl': local_add_to_pwl_flag})\n",
    "            source = ElementTree.SubElement(word, 'source')\n",
    "            source.text = text_suggestions[pos][0]\n",
    "            context = ElementTree.SubElement(word, 'context')\n",
    "            context_string = text.text[max(0, pos - CONTEXT_SIZE):min(len(text.text), pos + len(text_suggestions[pos][0]) + CONTEXT_SIZE)]\n",
    "            while context_string.find('\\n') != -1:\n",
    "                context_string = context_string[:context_string.find('\\n')] + '\\\\n' + context_string[context_string.find('\\n')+1:]\n",
    "            while context_string.find('\\r') != -1:\n",
    "                context_string = context_string[:context_string.find('\\r')] + '\\\\r' + context_string[context_string.find('\\r')+1:]\n",
    "            context.text = context_string\n",
    "            suggestion = ElementTree.SubElement(word, 'suggestion')\n",
    "            suggestion.text = suggestion_text\n",
    "\n",
    "print(\"Cases found:\", total_wois)\n",
    "\n",
    "\n",
    "with codecs.open(EXIT_DIR + f'MEspellcheck-a{START_ARTICLE}-{END_ARTICLE}.xml', 'w', 'utf-8') as f:\n",
    "    f.write(prettify(root))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2. Пополнение словаря\n",
    "\n",
    "Добавляет отмеченные флагом \"добавление в словарь\" слова из всех файлов в директории спеллчека\n",
    "- Учитывается, было ли выбрано оригинальное слово или исправленное флагом \"результат\".\n",
    "- Словарь сортируется по алфавиту при каждом запуске.\n",
    "- Дубликаты удаляются при каждом запуске (символы разного регистра одинаковыми не считаются).\n",
    "- Слова добавленные вручную при запуске не удаляются.\n",
    "\n",
    "Чтобы объединить ваш словарь с другим, скопируйте и вставьте всё содержимое нового словаря в ваш, после чего запустите скрипт. Дубликаты будут удалены, итоговый словарь будет отсортирован."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for PWL additions...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/69 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eec4182b6d9a4206939b688d80e24252"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PWL...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8420 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51b7fb85f589493e93d7d35bdf203bce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing PWL...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4508 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27205b40c8a5498cbd14744c68ad3c92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5.2. Проверка правописания в текстах. Пополнение словаря.\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "SPELLCHECK_DIR = GLOBAL_RESULTS_DIR + GLOBAL_SPELLCHECK_DIR\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Read PWL and form word list\n",
    "with codecs.open(PERSONAL_WORD_LIST, 'r', 'utf-8') as f:\n",
    "    PWL_text = f.read()\n",
    "additions = [i.strip() for i in PWL_text.split('\\n')]\n",
    "while '' in additions:\n",
    "    additions.remove('')\n",
    "PWL_text = ''\n",
    "\n",
    "# Read all spellcheck outputs and create additions list\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(SPELLCHECK_DIR)\n",
    "\n",
    "print(\"Scanning for PWL additions...\")\n",
    "for filename in tqdm(filenames):\n",
    "    root = parse_xml(SPELLCHECK_DIR + filename)\n",
    "    for article in root:\n",
    "        if article.tag == \"article\":\n",
    "            for word in article:\n",
    "                if word.tag == \"word\" and word.attrib[\"add_to_pwl\"] != '0':\n",
    "                    word_text = get_xml_elem(word, 'suggestion').text.strip() if word.attrib[\"result\"] == '1' else get_xml_elem(word, 'source').text.strip()\n",
    "                    # Check (and correct) that the word has no latin and cyrillic letters at the same time\n",
    "                    if word_text is not None and len(word_text):\n",
    "                        exist_from_comb = False\n",
    "                        exist_rus = False\n",
    "                        for i in range(len(word_text)):\n",
    "                            exist_from_comb = True if word_text[i] in COMBINATIONS_CORR_GLOBAL.keys() else exist_from_comb\n",
    "                            exist_rus = True if re.match(r\"[А-Яа-я]\", word_text[i]) is not None else exist_rus\n",
    "                        if exist_from_comb and exist_rus:\n",
    "                            for i in range(len(word_text)):\n",
    "                                word_text = word_text[:i] + (COMBINATIONS_CORR_GLOBAL[word_text[i]] if word_text[i] in COMBINATIONS_CORR_GLOBAL.keys() else word_text[i]) + (word_text[i+1:] if (i + 1) <= len(word_text) else '')\n",
    "                    if word.attrib[\"add_to_pwl\"] == '2':\n",
    "                        additions.append(word_text.lower())\n",
    "                    elif word.attrib[\"add_to_pwl\"] == '3' and len(word_text):\n",
    "                        additions.append(word_text[0].upper() + word_text[1:] if len(word_text) > 1 else '')\n",
    "                    else:\n",
    "                        additions.append(word_text)\n",
    "\n",
    "# Make new PWL list and sort it\n",
    "print(\"Processing PWL...\")\n",
    "PWL_list_new = []\n",
    "for word in tqdm(additions):\n",
    "    # Append word to the list if not present yet\n",
    "    if word is not None and len(word) and not word in PWL_list_new:\n",
    "        PWL_list_new.append(word)\n",
    "PWL_list_new.sort()\n",
    "\n",
    "# Write PWL\n",
    "print(\"Writing PWL...\")\n",
    "for word in tqdm(PWL_list_new):\n",
    "    PWL_text = PWL_text + word + '\\n'\n",
    "with codecs.open(PERSONAL_WORD_LIST, 'w', 'utf-8') as f:\n",
    "    f.write(PWL_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3. Подстановка исправленной орфографии\n",
    "\n",
    "Подставляет в исходный текст исправленные слова или оригиналы, в зависимости от установленного флага \"результат\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting corrections...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/69 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43951e4674754184ba529a9963278c89"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting articles...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1412 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48c7d1081bd8498f95cdf6ab4ba77c4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5.3. Проверка правописания в текстах. Подстановка исправленной орфографии.\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "SPELLCHECK_DIR = GLOBAL_RESULTS_DIR + GLOBAL_SPELLCHECK_DIR\n",
    "ARTICLES_DIR = GLOBAL_RESULTS_DIR + GLOBAL_ARTICLES_DIR\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "corrections = {}\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(SPELLCHECK_DIR)\n",
    "\n",
    "print('Getting corrections...')\n",
    "for filename_data in tqdm(filenames):\n",
    "    # Parse articles corrections file\n",
    "    root_data = parse_xml(SPELLCHECK_DIR + filename_data)\n",
    "    for article in root_data:\n",
    "        if article.tag == 'article':\n",
    "            filename_article = article.attrib['filename']\n",
    "            if filename_article not in corrections.keys():\n",
    "                corrections[filename_article] = []\n",
    "            # Parse corrections in one article\n",
    "            for word in article:\n",
    "                if word.tag == 'word':\n",
    "                    pos = int(word.attrib['pos'])\n",
    "                    len_src = len(get_xml_elem(word, 'source').text)\n",
    "                    word_text = get_xml_elem(word, 'suggestion').text if word.attrib['result'] == '1' else get_xml_elem(word, 'source').text\n",
    "                    if word.attrib['add_to_pwl'] == '3' and len(word_text):\n",
    "                        word_text = word_text[0].upper() + word_text[1:]\n",
    "                    corrections[filename_article].append((pos, word_text, len_src))\n",
    "\n",
    "print('Correcting articles...')\n",
    "for filename_article in tqdm(corrections.keys()):\n",
    "    # Apply corrections\n",
    "    root_article = parse_xml(ARTICLES_DIR + filename_article)\n",
    "    text = get_xml_elem(root_article, 'text')\n",
    "    corrections[filename_article].sort(reverse=True)\n",
    "    for word in corrections[filename_article]:\n",
    "        pos = word[0]\n",
    "        len_src = word[2]\n",
    "        word_text = word[1]\n",
    "        text.text = text.text[:pos] + word_text + (text.text[pos+len_src:] if pos+len_src <= len(text.text) else '')\n",
    "    # Write corrected article xml\n",
    "    with codecs.open(ARTICLES_DIR + filename_article, 'w', 'utf-8') as f:\n",
    "        f.write(prettify(root_article))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Парсер авторов статьи\n",
    "\n",
    "Ищет в конце текста статей конструкции типа ` [Xxxx]. [Xxxx]. [Xxxx]` или ` [Xxxx].[Xxxx]. [Xxxx]` и интерпретирует её как автора статьи."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e5f6a755f6a4070ba54f7a0b4fbb2cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors found in total: 1287\n"
     ]
    }
   ],
   "source": [
    "# 6. Парсинг авторов статьи\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "ARTICLES_DIR = GLOBAL_RESULTS_DIR + GLOBAL_ARTICLES_DIR\n",
    "COMBINATIONS_CORR = dict_merge(COMBINATIONS_CORR_UNICODE, {\n",
    "    'II' : 'П'\n",
    "})\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "n = 0\n",
    "LOCAL_DICT = {'0':'О', '3':'З', '6':'б'}\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    article = parse_xml(ARTICLES_DIR + filename)\n",
    "    textelem = get_xml_elem(article, 'text')\n",
    "    text = textelem.text\n",
    "    authors = get_xml_elem(article, 'authors')\n",
    "\n",
    "    auth_start = 1\n",
    "    auth_list = []\n",
    "    while auth_start and text is not None:\n",
    "        # Find first non-space from the end\n",
    "        while text[-1] == ' ' or text[-1] == '\\n' or text[-1] == '\\r':\n",
    "            text = text[:-1]\n",
    "\n",
    "        auth_start = 0\n",
    "        # Try recognize\n",
    "        first_space = max(text.rfind(' ', 0, len(text)), text.rfind('\\n', 0, len(text)), text.rfind('\\r', 0, len(text)))\n",
    "        second_space = max(text.rfind(' ', 0, first_space), text.rfind('\\n', 0, first_space), text.rfind('\\r', 0, first_space))\n",
    "        third_space = max(text.rfind(' ', 0, second_space), text.rfind('\\n', 0, second_space), text.rfind('\\r', 0, second_space))\n",
    "        if first_space >= 0 and text[first_space-1] == '.' and second_space >= 0:\n",
    "            if text.find('.', second_space, first_space-1) != -1: # If there's no space between initials\n",
    "                third_space = second_space\n",
    "                second_space = first_space\n",
    "            if text[second_space-1] == '.' and third_space >= 0:\n",
    "                # Check if first letters of each word are capitals\n",
    "                keep = text\n",
    "                for comb in LOCAL_DICT.keys():\n",
    "                    while text[third_space+1:].find(comb) != -1:\n",
    "                        text = text[:third_space+1+text[third_space+1:].find(comb)] + LOCAL_DICT[comb] + text[third_space+2+text[third_space+1:].find(comb):]\n",
    "                if re.match(r\"[A-ZА-ЯІ]\", text[first_space+1]) is not None and re.match(r\"[A-ZА-ЯІ]\", text[second_space+1]) is not None and re.match(r\"[A-ZА-ЯІ]\", text[third_space+1]) is not None:\n",
    "                    auth_start = third_space + 1\n",
    "                text = keep\n",
    "\n",
    "        if auth_start: # Suggest that an article cannot consist of author only and therefore auth_start should be > 0\n",
    "            #print(article.attrib['uri'], author_text)\n",
    "            author_text = text[auth_start:]\n",
    "            if author_text[author_text.find('.')+1] != ' ': # Add space if there's no one between initials\n",
    "                author_text = author_text[:author_text.find('.')+1] + ' ' + author_text[author_text.find('.')+1:]\n",
    "            if author_text[-1] == '.' or author_text[-1] == ',':\n",
    "                author_text = author_text[:-1]\n",
    "            # convert wrong symbols\n",
    "            for comb in dict_merge(COMBINATIONS_CORR, LOCAL_DICT).keys():\n",
    "                while author_text.find(comb) != -1:\n",
    "                    author_text = author_text[:author_text.find(comb)] + dict_merge(COMBINATIONS_CORR, LOCAL_DICT)[comb] + author_text[author_text.find(comb) + len(comb):]\n",
    "\n",
    "            auth_list.append(author_text)\n",
    "            text = text[:auth_start]\n",
    "\n",
    "    # add authors, reverse their order to alphabetic\n",
    "    for auth in reversed(auth_list):\n",
    "        n += 1\n",
    "        author = ElementTree.SubElement(authors, 'author')\n",
    "        author.text = auth\n",
    "\n",
    "    textelem.text = text\n",
    "    with codecs.open(ARTICLES_DIR + filename, 'w', 'utf-8') as f:\n",
    "        f.write(prettify(article))\n",
    "\n",
    "print(\"Authors found in total:\", n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Парсер литературы\n",
    "\n",
    "После извлечения авторов статьи в конце за текстом статьи присутствует только строчка литературы, если вообще присутствует. Поэтому ищется и извлекается фрагмент начиная с \"`Лит.:`\". Он разделяется на сегменты по \"`[num]`\", а сегменты на подфрагменты по запятым. Общий вид сегмента полагается следующим: \"`[Авторы (возможно несколько, определяются по наличию инициалов в конце подфрагмента)], Название (возможно содержит запятые), Номер тома (может отсутствовать), [Информация об издании (может частично или полностью отсутствовать)], Год, [Прочее (главы, страницы и прочее, может отсутствовать)];`\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23f03ff92cae4f3f82c300a95b7c6e34"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3768\\2339465218.py:89: FutureWarning: Possible nested set at position 1\n",
      "  if text[-1] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-2]) is not None and text[-3] == ' ' and text[-4] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-5]) is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Literature found in 1040 articles\n",
      "Publications found in total: 3214\n"
     ]
    }
   ],
   "source": [
    "# 7. Парсинг литературы\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "ARTICLES_DIR = GLOBAL_RESULTS_DIR + GLOBAL_ARTICLES_DIR\n",
    "COMBINATIONS_CORR_LOCAL = dict_merge(dict_merge(COMBINATIONS_CORR_ALPHABET, COMBINATIONS_CORR_UNICODE), {'J':'Л'})\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Unit:\n",
    "    authors = []\n",
    "    title = \"\"\n",
    "    publication = \"\"\n",
    "    year = \"\"\n",
    "    other = \"\"\n",
    "\n",
    "\n",
    "n_lit = 0\n",
    "n_pub = 0\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    article = parse_xml(ARTICLES_DIR + filename)\n",
    "    textelem = get_xml_elem(article, 'text')\n",
    "    text = textelem.text\n",
    "    literature = get_xml_elem(article, 'literature')\n",
    "    literature_orig = get_xml_elem(literature, 'literature_orig')\n",
    "\n",
    "    if textelem.text is not None and len(textelem.text):\n",
    "        #Find literature start position and extract if present\n",
    "        for key in COMBINATIONS_CORR_LOCAL.keys():\n",
    "            while text.find(key) != -1:\n",
    "                text = text[:text.find(key)] + COMBINATIONS_CORR_LOCAL[key] + text[text.find(key)+1:]\n",
    "        text = text.upper()\n",
    "        lit_pos = text.rfind('\\nЛИТ.: ')\n",
    "        lit_pos = text.rfind('\\rЛИТ.: ') if lit_pos == -1 else lit_pos\n",
    "        lit_pos = text.rfind(' ЛИТ.: ') if lit_pos == -1 else lit_pos\n",
    "        if lit_pos != -1:\n",
    "            n_lit += 1\n",
    "            literature_orig.text = textelem.text[lit_pos:]\n",
    "            while literature_orig.text[0] in [' ', '\\n', '\\r']:\n",
    "                literature_orig.text = literature_orig.text[1:]\n",
    "            textelem.text = textelem.text[:lit_pos]\n",
    "            while textelem.text[-1] in [' ', '\\n', '\\r']:\n",
    "                textelem.text = textelem.text[:-1]\n",
    "\n",
    "\n",
    "            # Parse literature string\n",
    "            text = literature_orig.text\n",
    "            units = []\n",
    "            num = 1\n",
    "            while text.find('['+str(num)+']') != -1:\n",
    "                units.append(text[text.find('['+str(num)+']')+len('['+str(num)+']'):(text.find('['+str(num+1)+']') if text.find('['+str(num+1)+']') != -1 else len(text))])\n",
    "                n_pub += 1\n",
    "                num += 1\n",
    "            for unit in units:\n",
    "                logical_parts = Unit()\n",
    "                logical_parts.authors.clear()\n",
    "                subunits = unit.split(',')\n",
    "                while '' in subunits:\n",
    "                    subunits.remove('')\n",
    "                pos_last_auth = -1\n",
    "                pos_last_title = -1\n",
    "                pos_thome = -1\n",
    "                pos_transl = -1\n",
    "                pos_pub_num = -1\n",
    "                pos_pub_place = -1\n",
    "                pos_year = -1\n",
    "\n",
    "\n",
    "                # Define positions of most common pats of literature string\n",
    "                for i in range(len(subunits)):\n",
    "                    text = subunits[i]\n",
    "                    while text[-1] in [' ', '\\n', '\\r', ';']:\n",
    "                        text = text[:-1]\n",
    "                    while text[0] in [' ', '\\n', '\\r']:\n",
    "                        text = text[1:]\n",
    "                    subunits[i] = text\n",
    "\n",
    "                    if pos_last_auth + 1 == i: # Recognize authors\n",
    "                        keep = text\n",
    "                        pos_initials = 0\n",
    "                        for j in range(len(text)):\n",
    "                            if text[j] in COMBINATIONS_CORR_UNICODE:\n",
    "                                text = text[:j] + COMBINATIONS_CORR_UNICODE[text[j]] + text[j+1:]\n",
    "                        if text[-1] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-2]) is not None and text[-3] == ' ' and text[-4] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-5]) is not None:\n",
    "                            # \"X. X.\"\n",
    "                            pos_last_auth = i\n",
    "                            pos_initials = -5\n",
    "                        elif text[-1] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-2]) is not None and text[-3] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-4]) is not None:\n",
    "                            # \"X.X.\"\n",
    "                            pos_last_auth = i\n",
    "                            text = text[:-2] + ' ' + text[-2:]\n",
    "                            pos_initials = -5\n",
    "                        elif text[-1] == '.' and re.match(r\"[[А-ЯA-Z]\", text[-2]) is not None:\n",
    "                            # \"X.\"\n",
    "                            pos_last_auth = i\n",
    "                            pos_initials = -2\n",
    "                        else: # Title starts\n",
    "                            text = keep\n",
    "                        # If correct\n",
    "                        if pos_last_auth == i:\n",
    "                            surname = text[:pos_initials]\n",
    "                            while surname.find(' ') != -1:\n",
    "                                surname = surname[:surname.find(' ')] + surname[surname.find(' ')+1:]\n",
    "                            text = surname + ' ' + text[pos_initials:]\n",
    "                            j = 1\n",
    "                            while j < len(text):\n",
    "                                if re.match(r\"[А-ЯA-Z]\", text[j]) is not None and re.match(r\"[а-яa-z]\", text[j-1]) is not None:\n",
    "                                    text = text[:j] + ' ' + text[j:]\n",
    "                                    j = 1\n",
    "                                else:\n",
    "                                    j += 1\n",
    "                            subunits[i] = text\n",
    "                    else:\n",
    "                        if pos_thome == -1: # Recognize thome\n",
    "                            keep = text\n",
    "                            for j in range(len(text)):\n",
    "                                if text[j] in COMBINATIONS_CORR_GLOBAL:\n",
    "                                    text = text[:j] + COMBINATIONS_CORR_GLOBAL[text[j]] + text[j+1:]\n",
    "                            if text.upper().find('Т.') != -1:\n",
    "                                pos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "                                pos_thome = i\n",
    "                            text = keep\n",
    "                        if pos_transl == -1: # Recognize publication number\n",
    "                            keep = text\n",
    "                            for j in range(len(text)):\n",
    "                                if text[j] in COMBINATIONS_CORR_GLOBAL:\n",
    "                                    text = text[:j] + COMBINATIONS_CORR_GLOBAL[text[j]] + text[j+1:]\n",
    "                            if text.upper().find('ПЕР.') != -1:\n",
    "                                pos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "                                pos_transl = i\n",
    "                            text = keep\n",
    "                        if pos_pub_num == -1: # Recognize publication number\n",
    "                            keep = text\n",
    "                            for j in range(len(text)):\n",
    "                                if text[j] in COMBINATIONS_CORR_GLOBAL:\n",
    "                                    text = text[:j] + COMBINATIONS_CORR_GLOBAL[text[j]] + text[j+1:]\n",
    "                            if text.upper().find('ИЗД.') != -1:\n",
    "                                pos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "                                pos_pub_num = i\n",
    "                            text = keep\n",
    "                        if pos_pub_place == -1: # Recognize publication place\n",
    "                            keep = text\n",
    "                            for j in range(len(text)):\n",
    "                                if text[j] in COMBINATIONS_CORR_GLOBAL:\n",
    "                                    text = text[:j] + COMBINATIONS_CORR_GLOBAL[text[j]] + text[j+1:]\n",
    "                            if text.upper() in ['М.', 'Л.', 'СПБ.', 'М.Л.', 'Л.М.', 'М.СПБ.', 'СПБ.М.']:\n",
    "                                pos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "                                pos_pub_place = i\n",
    "                            text = keep\n",
    "                        # If correct\n",
    "                        if pos_last_auth != i and (pos_thome == i or pos_pub_num == i or pos_pub_place == i):\n",
    "                            for j in range(len(text)):\n",
    "                                if text[j] in COMBINATIONS_CORR_UNICODE:\n",
    "                                    subunits[i] = text[:j] + COMBINATIONS_CORR_UNICODE[text[j]] + text[j+1:]\n",
    "\n",
    "                        if pos_year == -1 and len(text) >= 4: # Recognize year\n",
    "                            numbers = ['0','1','2','3','4','5','6','7','8','9']\n",
    "                            j = 0\n",
    "                            for j in range(len(text) - 3):\n",
    "                                if text[j] in numbers and text[j+1] in numbers and text[j+2] in numbers and text[j+3] in numbers:\n",
    "                                    pos_last_title = (i - 1) if pos_last_title == -1 else pos_last_title\n",
    "                                    pos_year = i\n",
    "                                    break\n",
    "                            # if correct\n",
    "                            if pos_year == i:\n",
    "                                subunits[i] = text[j:j+4]\n",
    "\n",
    "\n",
    "                # Extract info from literature string using positions defined above\n",
    "                for i in range(len(subunits)):\n",
    "                    text = subunits[i]\n",
    "                    if pos_last_auth >= i: # Author\n",
    "                        logical_parts.authors.append(text)\n",
    "                    elif pos_last_auth < i <= pos_last_title: # Title\n",
    "                        logical_parts.title = logical_parts.title + ('' if len(logical_parts.title) == 0 else ', ') + text\n",
    "                    elif pos_year == i: # Year\n",
    "                        logical_parts.year = logical_parts.year + ('' if len(logical_parts.year) == 0 else ', ') + text\n",
    "                    elif ((pos_pub_num <= i and pos_pub_num != -1) or (pos_pub_place <= i and pos_pub_place != -1) or (pos_transl <= i and pos_transl != -1) or (pos_thome + 1 <= i and pos_thome != -1)) and pos_year > i: # Publication\n",
    "                        logical_parts.publication = logical_parts.publication + ('' if len(logical_parts.publication) == 0 else ', ') + text\n",
    "                    else: # Other\n",
    "                        logical_parts.other = logical_parts.other + ('' if len(logical_parts.other) == 0 else ', ') + text\n",
    "\n",
    "\n",
    "                # Debug section\n",
    "                \"\"\"print('\\n', filename, unit)\n",
    "                print('authors:', logical_parts.authors)\n",
    "                print('title:', logical_parts.title)\n",
    "                print('publication:', logical_parts.publication)\n",
    "                print('year:', logical_parts.year)\n",
    "                print('other:', logical_parts.other)\n",
    "                print(pos_last_auth, pos_last_title, pos_thome, pos_transl, pos_pub_num, pos_pub_place, pos_year)\"\"\"\n",
    "\n",
    "\n",
    "                # Add literature unit\n",
    "                unit = ElementTree.SubElement(literature, \"unit\")\n",
    "                for auth_str in logical_parts.authors:\n",
    "                    author = ElementTree.SubElement(unit, \"author\")\n",
    "                    author.text = auth_str\n",
    "                title = ElementTree.SubElement(unit, \"title\")\n",
    "                title.text = logical_parts.title\n",
    "                publication = ElementTree.SubElement(unit, \"publication\")\n",
    "                publication.text = logical_parts.publication\n",
    "                year = ElementTree.SubElement(unit, \"year\")\n",
    "                year.text = logical_parts.year\n",
    "                other = ElementTree.SubElement(unit, \"other\")\n",
    "                other.text = logical_parts.other\n",
    "\n",
    "\n",
    "            # Write xml\n",
    "            with codecs.open(ARTICLES_DIR + filename, 'w', 'utf-8') as f:\n",
    "                f.write(prettify(article))\n",
    "\n",
    "print(\"Literature found in\", n_lit, \"articles\")\n",
    "print(\"Publications found in total:\", n_pub)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Парсер формул\n",
    "\n",
    "Выносит из текстов ранее подготовленных xml-файлов статей сначала выносные, а затем строчные формулы, оставляя на их месте ссылку внутри математического окружения.\n",
    "\n",
    "Минимальная длина в символах, которой должна обладать строчная формула, настраивается."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e04af22a9ed74b5e9631393e9468d1f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found main formulas: 9959\n",
      "Found auxiliary formulas: 38421\n"
     ]
    }
   ],
   "source": [
    "# 8. Парсер формул\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "ARTICLES_DIR = GLOBAL_RESULTS_DIR + GLOBAL_ARTICLES_DIR\n",
    "MIN_INLINE_LEN = 0\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "\n",
    "n_main = 0\n",
    "n_aux = 0\n",
    "for filename in tqdm(filenames):\n",
    "    article = parse_xml(ARTICLES_DIR + filename)\n",
    "    #print('REMOTES: ' + article.attrib['uri'])\n",
    "    text = get_xml_elem(article, 'text')\n",
    "    formulas_main = get_xml_elem(article, 'formulas_main')\n",
    "    formulas_aux = get_xml_elem(article, 'formulas_aux')\n",
    "\n",
    "    # Get main formulas\n",
    "    pos_find = 0\n",
    "    pos_start = 0\n",
    "    pos_end = 0\n",
    "    n = 1\n",
    "    while text.text is not None and text.text.find('\\\\[', pos_find) != -1:\n",
    "        pos_start = text.text.find('\\\\[', pos_find) + 2\n",
    "        pos_end = text.text.find('\\\\]', pos_start)\n",
    "        while text.text[pos_start] == '\\n':\n",
    "            pos_start += 1\n",
    "        while text.text[pos_end-1] == '\\n':\n",
    "            pos_end -= 1\n",
    "        pos_find = pos_start\n",
    "        uri = URI_PREFIX + 'formula/main' + article.attrib['uri'][article.attrib['uri'].rfind('/', 0, article.attrib['uri'].find('_')):article.attrib['uri'].find('_')+1] + str(n) + article.attrib['uri'][article.attrib['uri'].find('_'):]\n",
    "        n += 1\n",
    "        formula = ElementTree.SubElement(formulas_main, 'formula', {'uri':uri})\n",
    "        formula.text = text.text[pos_start:pos_end]\n",
    "        text.text = text.text[:pos_start] + 'URI[[' + uri + ']]/URI' + text.text[pos_end:]\n",
    "    n_main += n\n",
    "\n",
    "    # Get auxiliary formulas\n",
    "    pos_find = 0\n",
    "    pos_start = 0\n",
    "    pos_end = 0\n",
    "    cnt = 0\n",
    "    n = 1\n",
    "    # Count dollar symbols\n",
    "    while text.text is not None and text.text.find('$', pos_find) != -1:\n",
    "        pos_find = text.text.find('$', pos_find) + 1\n",
    "        cnt += 1\n",
    "    # If cnt is not even assume that first one is garbage from title\n",
    "    pos_find = 0\n",
    "    if cnt % 2:\n",
    "        pos_find = text.text.find('$', pos_find)\n",
    "        text.text = text.text[:pos_find] + '#' + text.text[pos_find+1:]\n",
    "    while text.text is not None and text.text.find('$', pos_find) != -1:\n",
    "        pos_start = text.text.find('$', pos_find) + 1\n",
    "        pos_end = text.text.find('$', pos_start)\n",
    "        if not check_in_uri(text.text, pos_start) and not check_in_uri(text.text, pos_end):\n",
    "            while text.text[pos_start] == '\\n':\n",
    "                pos_start += 1\n",
    "            while text.text[pos_end-1] == '\\n':\n",
    "                pos_end -= 1\n",
    "            pos_find = pos_start\n",
    "            if pos_end - pos_start >= MIN_INLINE_LEN:\n",
    "                uri = URI_PREFIX + 'formula/aux' + article.attrib['uri'][article.attrib['uri'].rfind('/', 0, article.attrib['uri'].find('_')):article.attrib['uri'].find('_')+1] + str(n) + article.attrib['uri'][article.attrib['uri'].find('_'):]\n",
    "                n += 1\n",
    "                formula = ElementTree.SubElement(formulas_aux, 'formula', {'uri':uri})\n",
    "                formula.text = text.text[pos_start:pos_end]\n",
    "                text.text = text.text[:pos_start] + 'URI[[' + uri + ']]/URI' + text.text[pos_end:]\n",
    "            pos_find = text.text.find('$', pos_find) + 1\n",
    "        else:\n",
    "            pos_find = pos_end + 1\n",
    "    n_aux += n\n",
    "\n",
    "    with codecs.open(ARTICLES_DIR + filename, 'w', 'utf-8') as f:\n",
    "        f.write(prettify(article))\n",
    "\n",
    "print(\"Found main formulas:\", n_main)\n",
    "print(\"Found auxiliary formulas:\", n_aux)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.1. Вынос формул\n",
    "\n",
    "Выносит все формулы в отдельный файл с указанием типа для возможной последующей обработки."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a8c05e3dfc34dd58af3996c6b31caeb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8.1. Вынос формул\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "ARTICLES_DIR = GLOBAL_RESULTS_DIR + GLOBAL_ARTICLES_DIR\n",
    "EXIT_FILE = GLOBAL_RESULTS_DIR + GLOBAL_EXTRACTED_FORMULAS_FILE\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "\n",
    "\n",
    "formulas = ElementTree.Element('formulas')\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    root = parse_xml(ARTICLES_DIR + filename)\n",
    "    fmain = get_xml_elem(root, 'formulas_main')\n",
    "    faux = get_xml_elem(root, 'formulas_aux')\n",
    "\n",
    "    for formula in fmain:\n",
    "        formulas.append(formula)\n",
    "    for formula in faux:\n",
    "        formulas.append(formula)\n",
    "\n",
    "with codecs.open(EXIT_FILE, 'w', 'utf-8') as f:\n",
    "    f.write(prettify(formulas))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.2. Проверка формул\n",
    "\n",
    "Случайным образом выбирает 20 случайных формул (из случайных статей) и вставляет их в математическое окружение Markdown для визуальной проверки"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# 8.2. Проверка формул\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "ARTICLES_DIR = GLOBAL_RESULTS_DIR + GLOBAL_ARTICLES_DIR\n",
    "EXIT_FILE = GLOBAL_WORK_DIR + GLOBAL_FORMULAS_CHECK_FILE\n",
    "NUMBER = 20\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Get filenames needed\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "\n",
    "\n",
    "file = ''\n",
    "\n",
    "i = 0\n",
    "while i < NUMBER:\n",
    "    root = parse_xml(ARTICLES_DIR + filenames[randint(0, len(filenames)-1)])\n",
    "\n",
    "    # Get all the info from article\n",
    "    fmain = get_xml_elem(root, 'formulas_main')\n",
    "    start = get_xml_elem(root, 'pages/start').text\n",
    "\n",
    "\n",
    "    # if there's no formulas in the article try another one\n",
    "    total_num = 0\n",
    "    for formula in fmain:\n",
    "        total_num += 1\n",
    "    if not total_num:\n",
    "        continue\n",
    "    i += 1\n",
    "\n",
    "    num = randint(0, 100) % total_num\n",
    "\n",
    "    formula = fmain[num].text\n",
    "\n",
    "    file += f'{i}. Статья: {root.attrib[\"uri\"]}, Начало на стр. {start}, формула {num + 1}:\\n$${formula}$$\\n'\n",
    "\n",
    "with codecs.open(EXIT_FILE, 'w', 'utf-8') as f:\n",
    "    f.write(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 9. Парсер ссылок типа \"смотри также\"\n",
    "\n",
    "Ищет в тексте ссылки начинающиеся на `\"см. [другие опциональные вводные слова]\"` и пытается найти соответствующие им статьи в энциклопедии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing search base...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "039f435d886a48a6bdaa4ec7f8c42182"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1972f2e637454910a67a7838e922d288"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching relations in articles...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de253e730ac54493845dabd994f30a6a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations found in total: 1983\n"
     ]
    }
   ],
   "source": [
    "# 9. Парсер ссылок типа \"смотри также\"\n",
    "\n",
    "print(\"Preparing search base...\")\n",
    "\n",
    "import relations as r\n",
    "import importlib\n",
    "importlib.reload(r)\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "r.BRUTE_FORCE_MODE = False  # Maximum amount of links to find, but takes more time (very slow)\n",
    "r.USE_MULTIPROCESSING = False  # WARNING: Does not work inside Jupyter!!!; Significantly speeds up scanning process\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "r.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 10. RDF конвертер\n",
    "\n",
    "Преобразует полученные \"проприетарные\" xml-файлы статей в формат RDF, пригодный для загрузки в базу данных."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning articles...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ead5faa6a8704a77bdfdbf3b1fe96494"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total objects: 26482\n",
      "Person duplicates found: 4030\n",
      "Publication duplicates found: 583\n",
      "Formula duplicates found: 18818\n",
      "\n",
      "\n",
      "Writing objects...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/26482 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13438817b0074f849ecc627f0bd796c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing concepts...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3586 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "297236b218874052ae0423736910b5d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 10. RDF конвертер\n",
    "\n",
    "from lib import *\n",
    "\n",
    "# -------------------------- VARS --------------------------------\n",
    "ARTICLES_DIR = GLOBAL_RESULTS_DIR + GLOBAL_ARTICLES_DIR\n",
    "EXIT_DIR = GLOBAL_RESULTS_DIR + GLOBAL_RDF_DIR\n",
    "# Resources links\n",
    "RESOURCE_CONCEPT = RDF_RESOURCE_CONCEPT\n",
    "RESOURCE_PERSON = RDF_RESOURCE_PERSON\n",
    "RESOURCE_PUBLICATION = RDF_RESOURCE_PUBLICATION\n",
    "RESOURCE_FORMULA = RDF_RESOURCE_FORMULA\n",
    "# Uri prefixes\n",
    "CORE_URL = RDF_CORE_URL\n",
    "CONCEPTS_URI_POSTPREFIX = RDF_CONCEPTS_URI_POSTPREFIX\n",
    "CONCEPTS_URI_PREFIX = RDF_CONCEPTS_URI_PREFIX\n",
    "PERSONS_URI_PREFIX = RDF_PERSONS_URI_PREFIX\n",
    "PUBLICATIONS_URI_PREFIX = RDF_PUBLICATIONS_URI_PREFIX\n",
    "FORMULAS_URI_PREFIX = RDF_FORMULAS_URI_PREFIX\n",
    "# Filename and uri ranges\n",
    "CONCEPTS_NUM_RANGE = RDF_CONCEPTS_NUM_RANGE\n",
    "OBJECTS_NUM_RANGE = RDF_OBJECTS_NUM_RANGE\n",
    "# Option that adds \".xml\" file type for automatic highlighting in text editors, `False` by default\n",
    "XML_FILETYPE = False\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Node:\n",
    "    attrib = {}\n",
    "    type = ''\n",
    "    text = ''\n",
    "    file = ''\n",
    "    link = ''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.contents = []\n",
    "        self.attrib = {}\n",
    "        self.type = ''\n",
    "        self.text = ''\n",
    "        self.file = ''\n",
    "        self.link = ''\n",
    "\n",
    "\n",
    "def add_person(name: str, src_type: str) -> str:\n",
    "    global objects\n",
    "    global objects_index\n",
    "    global doubles_person\n",
    "    # Split name into parts\n",
    "    last = ''\n",
    "    first = ''\n",
    "    middle = ''\n",
    "    num = len(name.split(' '))\n",
    "    if src_type == \"art\":\n",
    "        last = name.split(' ')[-1] if num >= 1 else ''\n",
    "        first = name.split(' ')[0] if num >= 2 else ''\n",
    "        middle = name.split(' ')[1] if num >= 3 else ''\n",
    "    if src_type == \"lit\":\n",
    "        last = name.split(' ')[0] if num >= 1 else ''\n",
    "        first = name.split(' ')[1] if num >= 2 else ''\n",
    "        middle = name.split(' ')[2] if num >= 3 else ''\n",
    "    # Try to find an existing one\n",
    "    for index in objects.keys():\n",
    "        if objects[index].type == \"person\":\n",
    "            match = 0\n",
    "            match += 1 if objects[index].attrib[\"last\"] == last else 0\n",
    "            match += 1 if objects[index].attrib[\"first\"] == first else 0\n",
    "            match += 1 if objects[index].attrib[\"middle\"] == middle else 0\n",
    "            if match >= num:\n",
    "                doubles_person += 1\n",
    "                return objects[index].link\n",
    "    # If not found create a new one\n",
    "    index = str(objects_index)\n",
    "    objects_index += 1\n",
    "    objects[index] = Node()\n",
    "    objects[index].type = 'person'\n",
    "    objects[index].attrib[\"last\"] = last\n",
    "    objects[index].attrib[\"first\"] = first\n",
    "    objects[index].attrib[\"middle\"] = middle\n",
    "    objects[index].link = PERSONS_URI_PREFIX + index\n",
    "    return objects[index].link\n",
    "\n",
    "\n",
    "def add_publication(node: Node) -> Node:\n",
    "    global objects\n",
    "    global objects_index\n",
    "    global doubles_publication\n",
    "    # Try to find an existing one\n",
    "    for index in objects.keys():\n",
    "        if objects[index].type == \"publication\":\n",
    "            match = True\n",
    "            for author_in in node.attrib[\"authors\"]:\n",
    "                exist = False\n",
    "                for author_ref in objects[index].attrib[\"authors\"]:\n",
    "                    exist = True if author_in.link == author_ref.link else exist\n",
    "                match = match and exist\n",
    "            match = False if node.attrib['title'] != objects[index].attrib['title'] else match\n",
    "            match = False if node.attrib['publication'] != objects[index].attrib['publication'] else match\n",
    "            match = False if node.attrib['year'] != objects[index].attrib['year'] else match\n",
    "            match = False if node.attrib['other'] != objects[index].attrib['other'] else match\n",
    "            if match:\n",
    "                doubles_publication += 1\n",
    "                return objects[index]\n",
    "    # If not found create a new one\n",
    "    index = str(objects_index)\n",
    "    objects_index += 1\n",
    "    node.type = 'publication'\n",
    "    node.link = PUBLICATIONS_URI_PREFIX + index\n",
    "    objects[index] = node\n",
    "    return objects[index]\n",
    "\n",
    "\n",
    "def add_formula(node: Node) -> Node:\n",
    "    global objects\n",
    "    global objects_index\n",
    "    global doubles_formula\n",
    "    # Try to find an existing one\n",
    "    for index in objects.keys():\n",
    "        if objects[index].type == \"formula\":\n",
    "            if node.text == objects[index].text:\n",
    "                doubles_formula += 1\n",
    "                return objects[index]\n",
    "    # If not found create a new one\n",
    "    index = str(objects_index)\n",
    "    objects_index += 1\n",
    "    node.type = \"formula\"\n",
    "    node.link = FORMULAS_URI_PREFIX + index\n",
    "    objects[index] = node\n",
    "    return objects[index]\n",
    "\n",
    "\n",
    "def get_ct() -> str:\n",
    "    ct = datetime.datetime.now(datetime.timezone.utc)\n",
    "    return f' {ct.day}-{ct.month}-{ct.year} {ct.hour}:{ct.minute} '\n",
    "\n",
    "\n",
    "def make_person(node: Node) -> ElementTree.Element:\n",
    "    person_root = ElementTree.Element('rdf:RDF', {'xmlns:rdf': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',\n",
    "                                      'xmlns:lbm': 'http://libmeta.ru/'})\n",
    "    subroot = ElementTree.SubElement(person_root, 'lbm:InformationObject', {'rdf:about': node.link})\n",
    "    ElementTree.SubElement(subroot, 'lbm:type', {'rdf:resource': RESOURCE_PERSON})\n",
    "    ElementTree.SubElement(subroot, 'lbm:description')\n",
    "    person_elem = ElementTree.SubElement(subroot, 'lbm:dateCreated')\n",
    "    person_elem.text = get_ct()\n",
    "    person_elem = ElementTree.SubElement(subroot, 'lbm:dateUpdated')\n",
    "    person_elem.text = get_ct()\n",
    "\n",
    "    subroot = ElementTree.SubElement(subroot, 'lbm:properties')\n",
    "\n",
    "    subsubroot = ElementTree.SubElement(subroot, 'lbm:property')\n",
    "    ElementTree.SubElement(subsubroot, 'lbm:type', {'rdf:resource': 'http://libmeta.ru/attribute#first'})\n",
    "    person_elem = ElementTree.SubElement(subsubroot, 'lbm:value')\n",
    "    person_elem.text = node.attrib['first']\n",
    "\n",
    "    subsubroot = ElementTree.SubElement(subroot, 'lbm:property')\n",
    "    ElementTree.SubElement(subsubroot, 'lbm:type', {'rdf:resource': 'http://libmeta.ru/attribute#last'})\n",
    "    person_elem = ElementTree.SubElement(subsubroot, 'lbm:value')\n",
    "    person_elem.text = node.attrib['last']\n",
    "\n",
    "    subsubroot = ElementTree.SubElement(subroot, 'lbm:property')\n",
    "    ElementTree.SubElement(subsubroot, 'lbm:type', {'rdf:resource': 'http://libmeta.ru/attribute#middle'})\n",
    "    person_elem = ElementTree.SubElement(subsubroot, 'lbm:value')\n",
    "    person_elem.text = node.attrib['middle']\n",
    "\n",
    "    \"\"\"subsubroot = ElementTree.SubElement(subroot, 'lbm:property')\n",
    "    elem = ElementTree.SubElement(subsubroot, 'lbm:type', {'rdf:resource':'http://libmeta.ru/attribute#email'})\n",
    "    elem = ElementTree.SubElement(subsubroot, 'lbm:value')\n",
    "    elem.text = ''\"\"\"\n",
    "\n",
    "    return person_root\n",
    "\n",
    "\n",
    "def make_publication(node: Node) -> ElementTree.Element:\n",
    "    publication_root = ElementTree.Element('rdf:RDF', {'xmlns:rdf': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',\n",
    "                                           'xmlns:lbm': 'http://libmeta.ru/'})\n",
    "    subroot = ElementTree.SubElement(publication_root, 'lbm:InformationObject', {'rdf:about': node.link})\n",
    "    ElementTree.SubElement(subroot, 'lbm:type', {'rdf:resource': RESOURCE_PUBLICATION})\n",
    "    ElementTree.SubElement(subroot, 'lbm:description')\n",
    "    publication_elem = ElementTree.SubElement(subroot, 'lbm:dateCreated')\n",
    "    publication_elem.text = get_ct()\n",
    "    publication_elem = ElementTree.SubElement(subroot, 'lbm:dateUpdated')\n",
    "    publication_elem.text = get_ct()\n",
    "\n",
    "    subroot = ElementTree.SubElement(subroot, 'lbm:properties')\n",
    "\n",
    "    for auth in node.attrib['authors']:\n",
    "        subsubroot = ElementTree.SubElement(subroot, 'lbm:property')\n",
    "        ElementTree.SubElement(subsubroot, 'lbm:type', {'rdf:resource': 'http://libmeta.ru/attribute#auth'})\n",
    "        ElementTree.SubElement(subsubroot, 'lbm:value', {'rdf:resource': auth.link})\n",
    "\n",
    "    \"\"\"subsubroot = ElementTree.SubElement(subroot, 'lbm:property')\n",
    "    ElementTree.SubElement(subsubroot, 'lbm:type', {'rdf:resource': 'http://libmeta.ru/attribute/doi'})\n",
    "    pub_elem = ElementTree.SubElement(subsubroot, 'lbm:value')\n",
    "    pub_elem.text = ''\"\"\"\n",
    "\n",
    "    \"\"\"subsubroot = ElementTree.SubElement(subroot, 'lbm:property')\n",
    "    ElementTree.SubElement(subsubroot, 'lbm:type', {'rdf:resource': 'http://libmeta.ru/attribute#keywords'})\n",
    "    pub_elem = ElementTree.SubElement(subsubroot, 'lbm:value')\n",
    "    pub_elem.text = ''\"\"\"\n",
    "\n",
    "    \"\"\"subsubroot = ElementTree.SubElement(subroot, 'lbm:property')\n",
    "    ElementTree.SubElement(subsubroot, 'lbm:type', {'rdf:resource': 'http://libmeta.ru/attribute#lang'})\n",
    "    pub_elem = ElementTree.SubElement(subsubroot, 'lbm:value')\n",
    "    pub_elem.text = ''\"\"\"\n",
    "\n",
    "    subsubroot = ElementTree.SubElement(subroot, 'lbm:property')\n",
    "    ElementTree.SubElement(subsubroot, 'lbm:type', {'rdf:resource': 'http://libmeta.ru/attribute#originalTitle'})\n",
    "    pub_elem = ElementTree.SubElement(subsubroot, 'lbm:value')\n",
    "    pub_elem.text = node.attrib['title']\n",
    "\n",
    "    \"\"\"subsubroot = ElementTree.SubElement(subroot, 'lbm:property')\n",
    "    ElementTree.SubElement(subsubroot, 'lbm:type', {'rdf:resource': 'http://libmeta.ru/attribute#udc'})\n",
    "    pub_elem = ElementTree.SubElement(subsubroot, 'lbm:value')\n",
    "    pub_elem.text = ''\"\"\"\n",
    "\n",
    "    return publication_root\n",
    "\n",
    "\n",
    "def make_formula(node: Node) -> (ElementTree.Element, str):\n",
    "    formula_root = ElementTree.Element('rdf:RDF', {'xmlns:rdf': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',\n",
    "                                       'xmlns:lbm': 'http://libmeta.ru/'})\n",
    "    subroot = ElementTree.SubElement(formula_root, 'lbm:InformationObject', {'rdf:about': node.link})\n",
    "    ElementTree.SubElement(subroot, 'lbm:type', {'rdf:resource': RESOURCE_FORMULA})\n",
    "    ElementTree.SubElement(subroot, 'lbm:description')\n",
    "    formula_elem = ElementTree.SubElement(subroot, 'lbm:dateCreated')\n",
    "    formula_elem.text = get_ct()\n",
    "    formula_elem = ElementTree.SubElement(subroot, 'lbm:dateUpdated')\n",
    "    formula_elem.text = get_ct()\n",
    "\n",
    "    subroot = ElementTree.SubElement(subroot, 'lbm:properties')\n",
    "\n",
    "    \"\"\"subsubroot = ElementTree.SubElement(subroot, 'lbm:property')\n",
    "    ElementTree.SubElement(subsubroot, 'lbm:type', {'rdf:resource': 'http://libmeta.ru/attribute/simplemathml'})\n",
    "    formula_elem = ElementTree.SubElement(subsubroot, 'lbm:value')\n",
    "    formula_elem.text = ''\"\"\"\n",
    "\n",
    "    subsubroot = ElementTree.SubElement(subroot, 'lbm:property')\n",
    "    ElementTree.SubElement(subsubroot, 'lbm:type', {'rdf:resource': 'http://libmeta.ru/attribute/mathml'})\n",
    "    ElementTree.SubElement(subsubroot, 'lbm:value')\n",
    "\n",
    "    subsubroot = ElementTree.SubElement(subroot, 'lbm:property')\n",
    "    ElementTree.SubElement(subsubroot, 'lbm:type', {'rdf:resource': 'http://libmeta.ru/attribute/tex'})\n",
    "    formula_elem = ElementTree.SubElement(subsubroot, 'lbm:value')\n",
    "    formula_elem.text = '$$' + (node.text if node.text is not None else '') + '$$'\n",
    "\n",
    "    converted2mathml = ''\n",
    "    # noinspection PyBroadException\n",
    "    try:\n",
    "        converted2mathml = tex2mml(node.text)\n",
    "    except:\n",
    "        pass\n",
    "    if converted2mathml is None:\n",
    "        converted2mathml = ''\n",
    "    return formula_root, converted2mathml\n",
    "\n",
    "\n",
    "def make_concept(node: Node, index) -> ElementTree.Element:\n",
    "    concept_root = ElementTree.Element('rdf:RDF', {'xmlns:rdf': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',\n",
    "                                       'xmlns:lbm': 'http://libmeta.ru/', 'xmlns:core': 'http://libmeta.ru/core'})\n",
    "    subroot = ElementTree.SubElement(concept_root, 'lbm:Concept', {'rdf:about': node.link})\n",
    "    ElementTree.SubElement(subroot, 'lbm:thesaurus', {'rdf:resource': RESOURCE_CONCEPT})\n",
    "    concept_elem = ElementTree.SubElement(subroot, 'lbm:code')\n",
    "    concept_elem.text = node.link[len(CONCEPTS_URI_PREFIX):]\n",
    "    concept_elem = ElementTree.SubElement(subroot, 'core:url')\n",
    "    concept_elem.text = CORE_URL + index\n",
    "    ElementTree.SubElement(subroot, 'lbm:descriptor')\n",
    "    ElementTree.SubElement(subroot, 'lbm:comment')\n",
    "\n",
    "    properties_root = ElementTree.SubElement(subroot, 'lbm:properties')\n",
    "\n",
    "    for auth in node.attrib[\"authors\"]:\n",
    "        property_elem = ElementTree.SubElement(properties_root, 'lbm:property',\n",
    "                                               {'rdf:resource':\n",
    "                                                    'http://libmeta.ru/thesaurus/attribute/author_of_the_article'})\n",
    "        ElementTree.SubElement(property_elem, 'lbm:value', {'rdf:resource': auth.link})\n",
    "\n",
    "    \"\"\"property_elem = ElementTree.SubElement(properties_root, 'lbm:property',\n",
    "                                           {'rdf:resource': 'http://libmeta.ru/thesaurus/attribute/msc'})\n",
    "    ElementTree.SubElement(property_elem, 'lbm:value', {'rdf:resource': ''})\"\"\"\n",
    "\n",
    "    \"\"\"property_elem = ElementTree.SubElement(properties_root, 'lbm:property',\n",
    "                                           {'rdf:resource': 'http://libmeta.ru/thesaurus/attribute/theme_odu'})\n",
    "    ElementTree.SubElement(property_elem, 'lbm:value', {'rdf:resource': ''})\"\"\"\n",
    "\n",
    "    for pub in node.attrib[\"lit\"]:\n",
    "        property_elem = ElementTree.SubElement(properties_root, 'lbm:property',\n",
    "                                               {'rdf:resource': 'http://libmeta.ru/thesaurus/attribute/lit'})\n",
    "        ElementTree.SubElement(property_elem, 'lbm:value', {'rdf:resource': pub.link})\n",
    "\n",
    "    formulas_added = []\n",
    "    for form in node.attrib[\"f_main\"]:\n",
    "        if form.link not in formulas_added:\n",
    "            formulas_added.append(form.link)\n",
    "            property_elem = ElementTree.SubElement(properties_root, 'lbm:property',\n",
    "                                                   {'rdf:resource':\n",
    "                                                        'http://libmeta.ru/thesaurus/attribute/mainFormula'})\n",
    "            ElementTree.SubElement(property_elem, 'lbm:value', {'rdf:resource': form.link})\n",
    "    for form in node.attrib[\"f_aux\"]:\n",
    "        if form.link not in formulas_added:\n",
    "            formulas_added.append(form.link)\n",
    "            property_elem = ElementTree.SubElement(properties_root, 'lbm:property',\n",
    "                                                   {'rdf:resource':\n",
    "                                                        'http://libmeta.ru/thesaurus/attribute/additonalFormula'})\n",
    "            ElementTree.SubElement(property_elem, 'lbm:value', {'rdf:resource': form.link})\n",
    "\n",
    "    property_elem = ElementTree.SubElement(properties_root, 'lbm:property',\n",
    "                                           {'rdf:resource': 'http://libmeta.ru/thesaurus/attribute/article'})\n",
    "    ElementTree.SubElement(property_elem, 'lbm:value')\n",
    "\n",
    "    property_elem = ElementTree.SubElement(properties_root, 'lbm:property',\n",
    "                                           {'rdf:resource': 'http://libmeta.ru/thesaurus/attribute/original_text'})\n",
    "    ElementTree.SubElement(property_elem, 'lbm:value')\n",
    "\n",
    "    relations_added = []\n",
    "    for relat in node.attrib[\"relations\"]:\n",
    "        if relat.link not in relations_added:\n",
    "            relation_elem = ElementTree.SubElement(subroot, 'lbm:familyRelation',\n",
    "                                                   {'type': 'http://libmeta.ru/relation/family#related'})\n",
    "            ElementTree.SubElement(relation_elem, 'lbm:value', {'rdf:resource': relat.link})\n",
    "\n",
    "    return concept_root\n",
    "\n",
    "\n",
    "def make_link(text: str, link: str, link_type: str) -> str:\n",
    "    return f'<a href=\"/{link_type}/show?uri={link}\">{text}</a>'\n",
    "\n",
    "\n",
    "def make_concept_uri(uri: str) -> str:\n",
    "    return CONCEPTS_URI_PREFIX + CONCEPTS_URI_POSTPREFIX + uri[len(URI_PREFIX) + len(\"article/\"):]\n",
    "\n",
    "\n",
    "def prepare_texts(node: Node) -> Node:\n",
    "    text = node.attrib[\"text\"] if node.attrib[\"text\"] is not None else ''\n",
    "\n",
    "    # Process publications\n",
    "    if len(node.attrib[\"lit\"]):\n",
    "        text += '\\n\\n<i>Лит.</i>: '\n",
    "    for pub_idx in range(1, len(node.attrib[\"lit\"]) + 1):\n",
    "        pub = node.attrib[\"lit\"][pub_idx - 1]\n",
    "        link = make_link(f'[{pub_idx}]', pub.link, 'object')\n",
    "        text_pos = 0\n",
    "        while text.find(f'[{pub_idx}]', text_pos) != -1:\n",
    "            text = text[:text.find(f'[{pub_idx}]', text_pos)] + link +\\\n",
    "                   text[text.find(f'[{pub_idx}]', text_pos)+len(f'[{pub_idx}]'):]\n",
    "            text_pos = text.find(f'[{pub_idx}]', text_pos)+len(f'[{pub_idx}]')\n",
    "        text += f'{link} '\n",
    "        pub_lst = []\n",
    "        for auth in pub.attrib[\"authors\"]:\n",
    "            if auth.text != '':\n",
    "                pub_lst.append(make_link(auth.text, auth.link, 'object'))\n",
    "        text += str.join(', ', pub_lst) + ', '\n",
    "        pub_lst = []\n",
    "        if pub.attrib[\"title\"] is not None and pub.attrib[\"title\"] != '':\n",
    "            pub_lst.append(pub.attrib[\"title\"])\n",
    "        if pub.attrib[\"publication\"] is not None and pub.attrib[\"publication\"] != '':\n",
    "            pub_lst.append(pub.attrib[\"publication\"])\n",
    "        if pub.attrib[\"year\"] is not None and pub.attrib[\"year\"] != '':\n",
    "            pub_lst.append(pub.attrib[\"year\"])\n",
    "        if pub.attrib[\"other\"] is not None and pub.attrib[\"other\"] != '':\n",
    "            pub_lst.append(pub.attrib[\"other\"])\n",
    "        text += make_link(str.join(', ', pub_lst), pub.link, 'object') +\\\n",
    "                ('; ' if pub_idx < len(node.attrib[\"lit\"]) else '.')\n",
    "\n",
    "    # Process authors\n",
    "    if len(node.attrib[\"authors\"]):\n",
    "        text += '\\n\\n'\n",
    "        auth_lst = []\n",
    "        for auth in node.attrib[\"authors\"]:\n",
    "            if auth.text is not None and auth.text != '':\n",
    "                auth_lst.append(make_link(auth.text, auth.link, 'object'))\n",
    "        text += '<i>' + str.join(', ', auth_lst) + '.</i>'\n",
    "\n",
    "    # Process main formulas\n",
    "    for form in node.attrib[\"f_main\"]:\n",
    "        link = make_link(f'$${form.text}$$', form.link, 'object')\n",
    "        pos_start = text.find(form.attrib[\"uri\"])\n",
    "        if pos_start >= 0:\n",
    "            pos_end = pos_start\n",
    "            while text[pos_start:pos_start+2] != '\\\\[':\n",
    "                if pos_start == 0:\n",
    "                    break\n",
    "                pos_start -= 1\n",
    "            while text[pos_end-2:pos_end] != '\\\\]':\n",
    "                if pos_end == len(text):\n",
    "                    break\n",
    "                pos_end += 1\n",
    "            text = (text[:pos_start] if pos_start > 0 else '') + link + (text[pos_end:] if pos_end < len(text) else '')\n",
    "    # Process auxiliary formulas\n",
    "    for form in node.attrib[\"f_aux\"]:\n",
    "        link = make_link(f'$${form.text}$$', form.link, 'object')\n",
    "        pos_start = text.find(form.attrib[\"uri\"])\n",
    "        if pos_start >= 0:\n",
    "            pos_end = pos_start\n",
    "            while text[pos_start:pos_start+1] != '$':\n",
    "                if pos_start == 0:\n",
    "                    break\n",
    "                pos_start -= 1\n",
    "            while text[pos_end-1:pos_end] != '$':\n",
    "                if pos_end == len(text):\n",
    "                    break\n",
    "                pos_end += 1\n",
    "            text = (text[:pos_start] if pos_start > 0 else '') + link + (text[pos_end:] if pos_end < len(text) else '')\n",
    "\n",
    "    # Process relations\n",
    "    for relat in node.attrib[\"relations\"]:\n",
    "        link = make_link(relat.text, relat.link, 'concept')\n",
    "        pos_start = text.find(relat.attrib[\"uri\"])\n",
    "        if pos_start >= 0:\n",
    "            pos_end = pos_start\n",
    "            while text[pos_start:pos_start+5] != 'URI[[':\n",
    "                if pos_start == 0:\n",
    "                    break\n",
    "                pos_start -= 1\n",
    "            while text[pos_end-6:pos_end] != ']]/URI':\n",
    "                if pos_end == len(text):\n",
    "                    break\n",
    "                pos_end += 1\n",
    "            text = (text[:pos_start] if pos_start > 0 else '') + link + (text[pos_end:] if pos_end < len(text) else '')\n",
    "\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    for p in range(len(paragraphs)):\n",
    "        paragraphs[p] = f'<P>{paragraphs[p]}<BR/></P>'\n",
    "    text = str.join('', paragraphs)\n",
    "\n",
    "    node.attrib[\"text\"] = text\n",
    "    return node\n",
    "\n",
    "\n",
    "concepts = {}\n",
    "concepts_index = CONCEPTS_NUM_RANGE[0]\n",
    "objects = {}\n",
    "objects_index = OBJECTS_NUM_RANGE[0]\n",
    "doubles_person = 0\n",
    "doubles_publication = 0\n",
    "doubles_formula = 0\n",
    "\n",
    "\n",
    "# Prepare directories\n",
    "deep = 0\n",
    "for next_dir in EXIT_DIR.split(\"/\"):\n",
    "    if next_dir not in ('.', ''):\n",
    "        deep += 1\n",
    "        os.chdir(next_dir)\n",
    "try:\n",
    "    os.chdir(\"concept\")\n",
    "    os.chdir(\"..\")\n",
    "except FileNotFoundError:\n",
    "    os.mkdir(\"concept\")\n",
    "try:\n",
    "    os.chdir(\"object\")\n",
    "    os.chdir(\"..\")\n",
    "except FileNotFoundError:\n",
    "    os.mkdir(\"object\")\n",
    "for i in range(deep):\n",
    "    os.chdir('..')\n",
    "\n",
    "# Convert articles into a node tree\n",
    "filenames = get_filenames(ARTICLES_DIR)\n",
    "# filenames = filenames[:5]\n",
    "# filenames = [\"13_AVTOKOLEBANI.xml\", \"29_ADAMARA-PERRONA.xml\"]\n",
    "\n",
    "print(\"\\nScanning articles...\")\n",
    "for filename in tqdm(filenames):\n",
    "    root = parse_xml(ARTICLES_DIR + filename)\n",
    "    article = Node()\n",
    "    article.attrib[\"uri\"] = root.attrib[\"uri\"]\n",
    "    article.link = make_concept_uri(root.attrib[\"uri\"])\n",
    "    article.text = get_xml_elem(root, \"title\").text\n",
    "\n",
    "    # Extract article authors\n",
    "    article.attrib[\"authors\"] = []\n",
    "    elem = get_xml_elem(root, \"authors\")\n",
    "    for subelem in elem:\n",
    "        if subelem.tag == 'author':\n",
    "            author = Node()\n",
    "            author.text = subelem.text\n",
    "            # Check for duplicates\n",
    "            author.link = add_person(author.text, 'art')\n",
    "            article.attrib[\"authors\"].append(author)\n",
    "\n",
    "    # Extract literature\n",
    "    article.attrib[\"lit\"] = []\n",
    "    elem = get_xml_elem(root, \"literature\")\n",
    "    for subelem in elem:\n",
    "        if subelem.tag == 'unit':\n",
    "            unit = Node()\n",
    "            unit.attrib[\"authors\"] = []\n",
    "            # Extract authors\n",
    "            for subsubelem in subelem:\n",
    "                if subsubelem.tag == 'author':\n",
    "                    author = Node()\n",
    "                    author.text = subsubelem.text\n",
    "                    author.link = add_person(author.text, 'lit')\n",
    "                    unit.attrib[\"authors\"].append(author)\n",
    "            # Extract other attributes\n",
    "            unit.attrib['title'] = get_xml_elem(subelem, 'title').text\n",
    "            unit.attrib['publication'] = get_xml_elem(subelem, 'publication').text\n",
    "            unit.attrib['year'] = get_xml_elem(subelem, 'year').text\n",
    "            unit.attrib['other'] = get_xml_elem(subelem, 'other').text\n",
    "            # Check for duplicates and add\n",
    "            article.attrib[\"lit\"].append(add_publication(unit))\n",
    "\n",
    "    # Extract formulas\n",
    "    article.attrib[\"f_main\"] = []\n",
    "    elem = get_xml_elem(root, \"formulas_main\")\n",
    "    for subelem in elem:\n",
    "        if subelem.tag == 'formula':\n",
    "            formula = Node()\n",
    "            formula.text = subelem.text\n",
    "            # Check for duplicates and add\n",
    "            formula = add_formula(formula)\n",
    "            formula.attrib[\"uri\"] = subelem.attrib[\"uri\"]\n",
    "            article.attrib[\"f_main\"].append(formula)\n",
    "    article.attrib[\"f_aux\"] = []\n",
    "    elem = get_xml_elem(root, \"formulas_aux\")\n",
    "    for subelem in elem:\n",
    "        if subelem.tag == 'formula':\n",
    "            formula = Node()\n",
    "            formula.text = subelem.text\n",
    "            # Check for duplicates and add\n",
    "            formula = add_formula(formula)\n",
    "            formula.attrib[\"uri\"] = subelem.attrib[\"uri\"]\n",
    "            article.attrib[\"f_aux\"].append(formula)\n",
    "\n",
    "    # Extract relations\n",
    "    article.attrib[\"relations\"] = []\n",
    "    elem = get_xml_elem(root, \"relations\")\n",
    "    for subelem in elem:\n",
    "        if subelem.tag == 'relation':\n",
    "            relation = Node()\n",
    "            relation.text = get_xml_elem(subelem, \"rel_text\").text\n",
    "            relation.attrib['uri'] = subelem.attrib['uri']\n",
    "            relation.attrib['tgt'] = get_xml_elem(subelem, 'target').text\n",
    "            relation.link = make_concept_uri(relation.attrib['tgt'])\n",
    "            article.attrib[\"relations\"].append(relation)\n",
    "\n",
    "    # Extract modified text\n",
    "    article.attrib['text'] = get_xml_elem(root, 'text').text\n",
    "    # Extract original text\n",
    "    article.attrib['text_orig'] = get_xml_elem(root, 'text_orig').text\n",
    "\n",
    "    # Create concepts\n",
    "    idx = str(concepts_index)\n",
    "    concepts_index += 1\n",
    "    concepts[idx] = article\n",
    "\n",
    "print(f\"Total objects: {len(objects)}\\n\"\n",
    "      f\"Person duplicates found: {doubles_person}\\n\"\n",
    "      f\"Publication duplicates found: {doubles_publication}\\n\"\n",
    "      f\"Formula duplicates found: {doubles_formula}\\n\")\n",
    "\n",
    "print(\"\\nWriting objects...\")\n",
    "for idx in tqdm(objects.keys()):\n",
    "    obj = ElementTree.Element('_')\n",
    "    mml = ''\n",
    "    if objects[idx].type == 'person':\n",
    "        obj = make_person(objects[idx])\n",
    "    elif objects[idx].type == 'publication':\n",
    "        obj = make_publication(objects[idx])\n",
    "    elif objects[idx].type == 'formula':\n",
    "        obj, mml = make_formula(objects[idx])\n",
    "\n",
    "    xml_out = prettify(obj)\n",
    "    if objects[idx].type == 'formula':\n",
    "        xml_out = insert_texts(xml=xml_out,\n",
    "                               fragment='mathml\"/>\\n        <lbm:value/>',\n",
    "                               left_scope='mathml\"/>\\n        <lbm:value>',\n",
    "                               right_scope='</lbm:value>',\n",
    "                               text=mml)\n",
    "    with codecs.open(EXIT_DIR + 'object/' + idx + ('.xml' if XML_FILETYPE else ''), 'w', 'utf-8') as f:\n",
    "        f.write(xml_out)\n",
    "\n",
    "print(\"\\nWriting concepts...\")\n",
    "for idx in tqdm(concepts.keys()):\n",
    "    concept_node = prepare_texts(concepts[idx])\n",
    "    obj = make_concept(concept_node, idx)\n",
    "    xml_out = prettify(obj)\n",
    "\n",
    "    xml_out = insert_texts(xml=xml_out,\n",
    "                           fragment='<lbm:descriptor/>',\n",
    "                           left_scope='<lbm:descriptor><![CDATA[',\n",
    "                           right_scope=']]></lbm:descriptor>',\n",
    "                           text=concept_node.text)\n",
    "\n",
    "    xml_out = insert_texts(xml=xml_out,\n",
    "                           fragment='attribute/article\">\\n        <lbm:value/>',\n",
    "                           left_scope='attribute/article\">\\n        <lbm:value><![CDATA[',\n",
    "                           right_scope=']]></lbm:value>',\n",
    "                           text=concept_node.attrib['text'])\n",
    "\n",
    "    xml_out = insert_texts(xml=xml_out,\n",
    "                           fragment='attribute/original_text\">\\n        <lbm:value/>',\n",
    "                           left_scope='attribute/original_text\">\\n        <lbm:value><![CDATA[',\n",
    "                           right_scope=']]></lbm:value>',\n",
    "                           text=concept_node.attrib['text_orig'])\n",
    "\n",
    "    with codecs.open(EXIT_DIR + 'concept/' + idx + ('.xml' if XML_FILETYPE else ''), 'w', 'utf-8') as f:\n",
    "        f.write(xml_out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}