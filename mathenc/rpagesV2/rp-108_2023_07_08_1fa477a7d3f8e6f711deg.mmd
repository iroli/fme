кривых (например, $x=0$ для уравнения $y^{\prime 2}-x^{2}=0$, рис. 5).

Рассматривается также уравнение $F=0$ в комплексной области, когда $F$ - многочлен от $y^{\prime}$ (см., напр., [2], гл. II).

Jum.: [1] С а н с о н е Д ж., Обыкновенные диффференциальные уравнения, пер. с итал., т. 2, М., 1954; [2] Г о л у б е в
В. В., Јекции по аналитической теории дифференциальных уравнений, 2 изд., М.-Ј., 1950.

ДИСКРИМИНАНТНАЯ ФУНКЦИЯ - статистика, служащая для построения правила различения в задачах дискрияинантного анализа с двумя распределениями. Задача различения (дискриминации) для двух распределений состоит в следующем. Пусть наблюденный объект с вектором измерений $x=\left(x_{1}, \ldots, x_{p}\right)$ принадлежит одной из совокупностей $\pi_{i}, i=1,2$, причем неизвестно какой. Требуется построить правило, согласно к-рому по значению наблюденного вектора $x$ объект относят к $\pi_{i}$ (правило различения). Построение такого правила основывается на разбиении выборочного пространства вектора $x$ на такие области $R_{i}, i=1,2$, чтобы при попадании $x$ в $R_{i}$ было разумно (с точки зрения выбранного принципа оптпмальности решения) отнести $x$ к $\pi_{i}$. Если правило діскриминации основывается на разбиенил: $R_{1}=\{x: T(x)<a\}, R_{2}=\{x: T(x) \geqslant b\}$, где $a$ и $b$ - константы, $a<b$, то статистику $T(x)$ наз. Д. ф., а область, где $a \leqslant T(x)<b,-3$ он о й с о м не ния. Особую роль, из-за простоты применений, играют линейные Д. ф. В частном случае, когда распределения нормальны и пмеют одінаковые матрицы ковариаций, Д. ф. оказывается линейиой при разумных требованиях оптімальности к указанному правилу. В задачах дискриминантного анализа со многими распределениями при бейесовском подходе (см. Бейесовский подход к статистическім задачам) вводится понятие дискриминантного инфорланта. Н. М. Митрофанова, А. П. Хусу.

ДИСКРИМИНАНТНБІИ АНАЛИЗ - раздел математич. статистики, содержанием к-рого является разработка и исследование статистич. методов решения следующей 3 а д ач і р а з л и ч е н и я (дискриминации): основываясь на результатах наблюдений, определить, какой пз нескольких возможных совокупностей принадлељит объект, случайно извлеченный из одной из них. На практике задача различения возникает, напр., в тех случаях, когда наблюдение признака, полностью определяющего принадлежность объекта к той или иной совокупности, невозможно или требует чрезмерных затрат средств или времени; в случаях, когда информация о таком признаке утеряна, и ее нужно восстановить, а также, когда речь идет о предсказании будущих событıй на основе имеющихся данных. Ситуации первого типа встречаются в медицинской практике, напр. при установлении диагноза по комплексу неспецифических проявлений заболевания. Пример ситуации второго типа - определение пола давно умершего человека по останкам, найденным при археологич. раскопках. Ситуация третьего типа возникает, напр., при статістич. прогнозе отдаленных результатов лечения. Методом Д. а. является многомерный статистический анализ, служащый для количественного выражения и обработки имеющейся информации в соответствии с выбранным критерием оптимальности решения.

В общем виде задача различения ставится следующим образом. Пусть результатом наблюдения над случайным объектом является реализация $p$-мерного случайного вектора $x^{\prime}=\left(x_{1}, \ldots, x_{p}\right)$ (IIтрих означает транспонирование) значений $p$ признаков объекта. Требуется установить правило, согласно к-рому по значению вектора $x$ объект относят к одной из возможных совокупностей $\pi_{i}, i=1, \ldots, k$. Построение п р а в и ла д и с кр и м и н а и и состоит в том, что все выборочное проcmpанство $\boldsymbol{R}$ значений вектора $x$ разбивается на области $R_{i}, i=1, \ldots, k$, так что при попадании $x$ в $R_{i}$ объект относят к совокупности $\pi_{i}$. Выбор правила дискриминации среди всех возможных производится в соответствии с установленным принципом оптимальности на основе априорной информации о совокупностях $\pi_{i}$ и вероятностях $q_{i}$ извлечения объекта из $\pi_{i}$ П При этом учитывается размер убытка от неправильной дискриминации. Априорная информация о совокупностях $\pi_{i}$ может состоять в том, что известны функции распределения вектора признаков объекта в каждой из этих совокупностей, она может быть представлена также и в виде выборок из каждой из этих совокупностей, при этом априорные вероятности $q_{i}$ совокупностей могут быть либо известны, либо нет. Очевидно, чем полнее исходная информация, тем точнее могут быть рекомендации.

Пусть рассматривается случай двух совокупностей $\pi_{1}$ и $\pi_{2}$ в ситуации, когда имеется полная исходная информация: известны функции распределения вектора Признаков в каждой из совокупностей и априорные вероятности (бейесовский подход). Пусть $P_{1}(x)$ и $P_{2}(x)-$ функции распределения вектора признаков соответственно в $\pi_{1}$ и $\pi_{2}, p_{1}(x)$ и $p_{2}(x)$ - плотности распределения, а $C(i \mid i), i, j=1,2,-$ убыток вследствие отнесения объекта из i-й совокупности к $i$-й. Тогда вероятності неправильной дискриминации объектов из $\pi_{1}$ и $\pi_{2}$ соответственно равны:

\[
P(2 \mid 1 ; R)=\int_{R_{2}} p_{1}(x) d x, \quad P(1 \mid 2 ; R)=\int_{R_{1}} p_{2}(x) d x
\]

(символом $P(i \mid j ; R)$ обозначена вероятность приписывания объекта из $\pi_{j}$ к совокупности $\pi_{i}$ при использовании правила $R$ ), а математич. ожидание потерь, связанных с неверной дискриминацией, равно

\[
C(2 \mid 1) P(2 \mid 1 ; R) q_{1}+C(1 \mid 2) P(1 \mid 2 ; R) q_{2} \text {. }
\]

Естественным в рассматриваемой ситуации принцигом оптимальности является принцип минимизации әтой величины, к-рый приводит в этом случае к следующему разбиению пространства выборок [1]:

\[
\left.\begin{array}{l}
R_{1}: \frac{p_{1}(x)}{p_{2}(x)} \geqslant \frac{C(1 \mid 2) q_{2}}{C(2 \mid 1) q_{1}}, \\
R_{2}: \frac{p_{1}(x)}{p_{2}(x)}<\frac{C(1 \mid 2) q_{2}}{C(2 \mid 1) q_{1}} .
\end{array}\right\}
\]

Если выполнены условия

\[
\mathrm{P}\left\{\frac{p_{1}(x)}{p_{2}(x)}=\frac{C(1 \mid 2) q_{2}}{C(2 \mid 1) q_{1}} \mid \pi_{i}\right\}=0, \quad i=1,2,
\]

то такое разбиение единственно с точностью до множества нулевой вероятности. К аналогичному правилу различения в рассмотренном случае можно прийти ІІ другими путями, напр. с помощью Неймана - Пирсона леммы из теории проверки статистич. гипотез.

При выбранном критерии оптимальности о качестве правила различения судят по величине математич. ожидания потерь, и из двух правил лучшим считается то, к-рое приводит к меньшему значению этой величины.

Если в задаче различения априорные вероятности $q_{i}$ неизвестны, то естественно искать решение в классе допустимых правил, выбирая среди них правило, минимизирующее максимум по всем $q_{i}$ математич. ожидания потерь (такое правило наз. м и н и м а к с н ы м). Математич. ожидания потерь при условии, что наблюдения производились соответственно над объектами из $\pi_{1}$ или $\pi_{2}$, равны

$C(2 \mid 1) P(2 \mid 1 ; R)=r(1, R), C(1 \mid 2) P(1 \mid 2 ; R)=r(2, R)$.

Справедливо утверждение (см. [1]): если выполнены условия

$\mathrm{P}\left\{\frac{p_{1}(x)}{p_{2}(x)}=k \mid \pi_{i}\right\}=0, \quad i=1,2, \quad 0 \leqslant k \leqslant \infty$,

то класс бейесовских методов является минимальным полным классом. Минимаксное правило $R^{*}$ из этого