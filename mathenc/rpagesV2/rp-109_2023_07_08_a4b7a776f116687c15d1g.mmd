класса получается при значении $q_{1}$, для к-рого выполнено условие $P\left(2 \mid 1 ; R^{*}\right)=P\left(1 \mid 2 ; R^{*}\right)$. В важном случае, когда $P_{1}$ и $P_{2}$ - многомерные нормальные распределевия с векторами средних $\mu^{(1)}$ и $\mu^{(2)}$ и общей ковариационной матрицей $\Sigma$, правило дискриминации (1) принимает вид:

\[
\left.\begin{array}{c}
R_{1}: x^{\prime} \sum^{-1}\left(\mu^{(1)}-\mu^{(2)}\right)- \\
-\frac{1}{2}\left(\mu^{(1)}+\mu^{(2)}\right)^{\prime} \sum^{-1}\left(\mu^{(1)}-\mu^{(2)}\right) \geqslant \ln k, \\
R_{2}: x^{\prime} \sum^{-1}\left(\mu^{(1)}-\mu^{(2)}\right)- \\
-\frac{1}{2}\left(\mu^{(1)}+\mu^{(2)}\right)^{\prime} \sum^{-1}\left(\mu^{(1)}-\mu^{(2)}\right)<\ln k,
\end{array}\right\}
\]

где $k=\frac{q_{2} C(1 \mid 2)}{q_{1} C(2 \mid 1)}$. Если $C(1 \mid 2)=C(2 \mid 1)$ и $q_{2}=q_{1}$, то $\ln k=0$

\[
\begin{gathered}
\text { и } R_{1}: D(x)=x^{\prime} \sum^{-1}\left(\mu^{(1)}-\mu^{(2)}\right) \geqslant \\
\geqslant \frac{1}{2}\left(\mu^{(1)}+\mu^{(2)}\right)^{\prime} \sum^{-1}\left(\mu^{(1)}-\mu^{(2)}\right) .
\end{gathered}
\]

Если априорные вероятности неизвестны, то можно выбрать $\ln k=c$, напр. из условия минимальности ошибки неверной дискриминации или из условия обращения в нуль математич. ожидания потерь от неверной дискриминации. Вообще говоря, выбор критерия оптимальности, как правило, определяется характером самой задачи. Выражение в левой части (3) наз. дискриминантной функцией данной задачи; ее можно толковать как поверхность в выборочном пространстве, разделяющую совокупности $\pi_{1}$ и $\pi_{2}$. В приведенном примере дискриминантная функция линейна, т. е. такая поверхность есть гиперплоскость. Если в приведенном примере матрицы ковариации неодинаковы, то дискриминантная функция будет квадратичной функцией от $x$. В целях упрощения вычислений найден минимальный полный класс линейных процедур различения для этого случая (см. [3]).

С точки зрения применений Д. а. наиболее важной является ситуация, когда исходная информация о распределениях представлена выборками из них. В этом случае задача дискриминации ставится следующим образом. Пусть $x_{1}^{(i)}, x_{2}^{(i)}, \ldots, x_{n_{i}}^{(i)}$ - выборка из совокупностиІ $\pi_{i}, \quad i=1, \ldots, k ; x_{j}^{(i)}=\left(x_{j}^{(i)}, \ldots, x_{j}^{(i)}\right)-$ вектор признаков $j$-го объекта выборки из $i$-й совокупности, и произведено дополнительное наблюдение $x^{\prime}=\left(x_{1}, \ldots\right.$, $\left.x_{p}\right)$ над объектом, принадлежащим одной из совокупностей $\pi_{i}$. Требуется построить правило приписывания наблюдения $x$ к одной из әтих совокупностей. Первый подход к решению этой задачи в случае двух совокупностей принадлежит Р. А. Фишеру - основоположнику Д. а. [4]. Используя в задаче различения вместо вектора признаков, характеризующих объект, их линейную комбинацию -- гиперплоскость, в нек-ром смысле наилучшим образом разделяющую совокупность выборочных точек, - он пришел к дискриминантной функции (3).

Наиболее изученным является случай, когда известно, что распределения векторов признаков в каждой совокупности нормальны, но нет информации о параметрах этих распределений. Здесь самым естественным является подход, состоящий в замене неизвестных параметров распределений в дискриминантной функции (3) их наилучшими оценками (см. [5], [6]). Как и в случае известных распределений, правило дискриминации можно основывать на отношении правдоподобия (см. [7], [8]).

Подавляющая часть результатов Д. а. получена в предположении нормальности распределений. Изучаются вопросы применимости оптимальных в нормальном случае методов в ситуациях, где предположение о нормальности носит лишь приближенный характер [9]. В этих работах задачи Д. а. рассматриваются в рамках общей теории решающих функций и изучаются свойства правил дискриминации по отношению к так наз. принципу $Q$-оптимальности, естественным образом охватывающему как бейесовский, так и минимаксный подходы. Именно, пусть $\boldsymbol{R}(\xi, \delta)$ - вероятность ошибки при применении правила діскриминации $\delta$, когда вектор априорных вероятностей есть $\xi$. Пусть известно, что $\xi \in Q$, где $Q$ - некоторое множество в пространстве векторов $\xi$. Правило $\delta^{*}$ наз. $Q-0$ п т и м а л ь н ы м, если

\[
\sup _{\xi \in Q} R\left(\xi, \delta^{*}\right)=\inf _{\delta \in D} \sup _{\xi \in Q} R(\xi, \delta)=R_{Q} \text {, }
\]

где $D$ - множество всех возможных правил дискриминации. Пусть известен функциональный вид $P_{i}\left(x, \lambda_{i}\right)$, зависящих от параметра распределений вектора признаков в каждой из совокупностей, $i=1,2$, но параметр $\lambda$ неизвестен и оценивается по выборке. Тогда если $P_{i}\left(x, \lambda_{i}\right)$ таковы, что существует $Q$-оптимальное правило $\delta^{*}\left(\lambda_{1}, \lambda_{2}\right)$ дискриминации для распределений $P_{i}\left(x, \lambda_{i}\right), i=1,2$, когда значение параметра $\lambda=\left(\lambda_{1}, \lambda_{2}\right)$ известно, и $\left\{\lambda_{i}^{\left(n_{i}\right)}\right\}$ - сильно состоятельная оценка параметра $\lambda_{i}$ по выборке объема $n_{i}$, то при нек-рых дополнительных условиях последовательность правил $\left\{\delta^{*}\left(\lambda_{1}^{\left(n_{1}\right)}, \lambda_{2}^{\left(n_{2}\right)}\right)\right\}$ при $n_{1}, n_{2} \rightarrow \infty$ является асимптотически $Q$-оптимальной, то есть с вероятностью 1

\[
\lim _{n_{1}, n_{2} \rightarrow \infty} \sup _{\xi \in Q} R\left(\xi, \delta^{*}\left(\lambda_{1}^{\left(n_{1}\right)}, \lambda_{2}^{\left(n_{2}\right)}\right)\right)=R_{Q},
\]

где риск $R$ в левой части (5) может быть вычислен как при истинном значении параметров, так и при замене истинных значений их оценками $\lambda_{i}^{\left(n_{i}\right)}$. Если потребовать лишь состоятельности оценки, то имеет место несколько более слабое утверждение.

Непараметрич. методы дискриминации, не требующие знаний о точном функциональном виде распределений и позволяющие решать задачи дискриминации на основе малой априорной информаціи о совокупностях, являются особо ценными для практических применений $[2],[10]$.

В задачах Д. а. приходится иметь дело со случайными наблюдениями как над количественными, так и над качественными признаками (возможен и смешанный случай). Между этими случаями нет принципиальной разницы. Если признаки качественные, то вводится понятие многомерного состояния объекта и рассматривается распределение по нему. От природы наблюдений зависит способ оценки функции распределений вектора признаков. В соответствующих ситуациях снова применимы бейесовский и минимаксный подходы и можно строить процедуру различения, основываясь на отношении правдоподобия. Иногда целесообразно переходить от количественных величин к качественным путем разбиения функции частот, и наоборот, от качественных к количественным, вводя фикктивные переменные, преобразующие качественную информацию в количественную. При этом, разумеется, нужно исследовать вопрос о том, не происходит ли существенного ухудшения качества правила.

Выше рассматривались задачи Д. а. при фонсированной размерности пространства значений вектора признаков. Однако практич. ситуациі чаще всего таковы, что выбор размерности осуществляется исследователем. На первый взгляд кажется, что добавление каждого нового признака в дискриминантной функции по крайней мере не ухудшит ее качества. Однако многие факторы могут при этом вести к потере эфффективности различения (достаточно вспомнить, что вместо истинных значений параметров распределеній часто используются их оценки). К тому же увеличение числа признаков ведет к быстрому возрастанию трудностей счета. Имеет-