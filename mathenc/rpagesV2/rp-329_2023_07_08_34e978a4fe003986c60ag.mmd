Попытка определить границы И. т., исходя из ее общепринятых определений, и включить в нее все разделы математики, имеющие дело с понятием информации в его общелексической трактовке, привела бы к неоправданной, по крайней мере на современном этапе, расширенной трактовке понятия И. т. Так, вся математич. статистика имеет дело с проблемами извлечения информации, теория алгоритмов - с проблемами переработки информации, теория формальных языков - с проблемами записи информации и т. Д.

Понятие информации и его приложения весьма многообразны, и этим можно объяснить то, что в настоящее время (к 1978) комплекс наук об информации представляет собой совокупность довольно разрозненных научных дисциплин, каждая из к-рых связана с изучением одного из аспектов этого понятия. Несмотря на интенсивные усилия ученых процесс сближения этих научных дисциплин идет довольно медленно, и создание единой и всеохватывающей И. т. представляется делом не слишком близкого будущего.

Лum.: [1] Г а л л a $\mathbf{r}$ е p Р., Теория информации и надежная связь, пер. с англ., М., 1974 ; [2] К о л м о г о р о в А. Н., "Пробл. передачи информ.», 1965, т. 1, № 1, c. 3-11; [3] X a pк е в и ч А. А., Борьба с помехами, 2 изд., М., 1965 ; [4] Б р и лл ю 2 ч Ј., Наука и теория информации, пер. с англ., М., $1960 ;$ [5] ч் е p p и К., Человек и информация, пер. с англ. М., 1972; [6] Я г л о м А. М., Я г л о м И. М., Веронтность и информация, 3 изд., М., 1973; [7] В о з е н к р а ф т Д ж., Д же к о б с И., Теоретические основы техники связи, пер. с англ., М., 1969 ; [8] Л е в и н Б. Р., Теоретические основы статистической радиотехники, 2 изд., М., 1974.

И. Р. Добрушин, В. В. Прелов.

![](https://cdn.mathpix.com/cropped/2023_07_08_34e978a4fe003986c60ag-1.jpg?height=42&width=833&top_left_y=1069&top_left_x=122)
манта. Для доминированного семейства распределений вероятностей $P^{t}(d \omega)$ с плотностями $p(\omega ; t)$, достаточно гладко зависящими от векторного (в частности, числового) параметра $t=\left(t_{1}, \ldots, t_{m}\right) \in \Theta$, элементы И. м. при $t=\theta$ определяются как

\[
I_{j k}(\theta)=\left.\int_{\Omega} \frac{\partial \ln p(\omega ; t)}{\partial t_{j}} \cdot \frac{\partial \ln p(\omega ; t)}{\partial t_{k}}\right|_{t=\theta} p(\omega ; \theta) d \mu
\]

где $j, k=1, \ldots, m$. При скалярном параметре $t$ И. м. описывается единственным числом - дисперсией информанта.

И. м. $I(\theta)$ определяет неотрицательную дифференциальную квадратичную форму

\[
\sum_{j, k} I_{j k}(\theta) d t_{j} d t_{k}=\Delta_{\theta}
\]

снабжающую семейство $\left\{p^{t}\right\}$ римановой метрикой. Когда пространство $\Omega$ исходов $\omega$ конечно,

\[
\Delta_{P}=\sum_{j}\left(d p_{j}\right)^{2} / p_{j} ; \quad p_{j}=P\left(\omega_{j}\right), \quad \forall \omega_{j} \in \Omega .
\]

Диффференциальная квадратичная форма Фишера (2) является единственной (с точностью до постоянного множителя) дифференциальной квадратичной формой, инвариантной относительно категории статистических решающих правил. Ввиду этого она возникает в формулировке многих статистич. закономерностей.

Любое измеримое отображение $f$ пространства $\Omega$ исходов порождает новое гладкое семейство распределений $Q^{t}=P^{t_{f}^{-1}}$ с И. м. $I Q(\theta)$. При этом И. м. монотонно не возрастает

\[
\sum_{j, k} I_{j k}^{Q} z_{j} z_{k} \leqslant \sum_{j, k} I_{j, k}^{P} z_{j} z_{k},
\]

каковы бы ни были $z_{1}, \ldots, z_{m}$. И. м. обладает также свойством аддитивности. Если $I^{(i)}(\theta)$ - И. м. для семейства с плотностью $P_{i}\left(\omega^{i} ; t\right)$, то для семейства

\[
p\left(\omega^{(1)}, \ldots, \omega^{(N)} ; t\right)=\prod_{i=1}^{N} p_{i}\left(\omega^{(i)} ; t\right)
\]

будет $I_{N}(\theta)=\sum_{i} I^{(i)}(\theta)$. В частности, $I_{N}(\theta)=N I(\theta)$ при $N$ независимых одинаково распределенных испытаниях. И. м. позволяет охарактеризовать статистич. точность решающих правил в задаче оценки параметра закона распределения. Для дисперсии любой несмещенной оценки $\tau(\omega)=\tau\left(\omega^{(1)}, \ldots, \omega^{(N)}\right)$ скалярного параметра $t$ справедливо

\[
D_{\theta} \tau \geqslant[N I(\theta)]^{-1} \text {. }
\]

Аналогичное матричное неравенство информации выполняется для оценок векторного параметра. Его скалярное следствие

$\mathbf{E}_{\theta} \sum_{j, k=1}^{m}\left[\tau_{j}(\omega)-\theta_{j}\right]\left[\tau_{k}(\omega)-\theta_{k}\right] I_{j k}(\theta) \geqslant m N^{-1}(3)$ показывает, что несмещенное оценивание нигде не может быть слишком точным. Для произвольных оценок последнее неверно. Однако остаются ограничения, напр., на среднюю точность

\[
\mathfrak{M}_{\Theta^{\prime}} \mathrm{E}_{\theta}\langle\tau-\theta|I(\theta)| \tau-\theta\rangle \geqslant m N^{-1}+o\left(N^{-1}\right),
\]

где усреднение $\mathfrak{M}$ левой части (3) проведено по инвариантному объему $V$ любой компактной подобласти $\Theta^{\prime} \subset \Theta$

\[
d V(\theta)=\sqrt{\operatorname{det} I(\theta)} d \theta_{1} \ldots d \theta_{m} ;
\]

остаточный член зависит от размеров $\Theta^{\prime}$. Неравенства (4) асимптотически точны, и асимптотически оптимальной в этом смысле оказывается оценка максимума правдоподобия.

В точках вырождения, $\operatorname{det} I(\theta)=0$, совместная оценка параметров затруднена; если $\operatorname{det} I(\theta)=0$ в нек-рой области, то совместная оценка вообще невозможна. Таким образом, следуя Р. Фишеру [1], с известной осторожностью можно сказать, что И. м. описывает среднее количество информации о параметрах закона распределения, содержащееся в случайной выборке.

Jum.: [1] F i s h e r R. A., "Proc. Cambr. Phil. Soc.", 1925 , v. 22, p. $700-25$; [2] Б a p p a Ж.-Р., Основные понятия математической статистики, пер. с франц., М., 1974; [3] Ч е нц о в Н. Н., Статистические решающие правила и оптимальИНФОРМАЦИОННОЕ МНОЖЕСТВО (в теориИ игр) - известная игроку в данный момент совокупность возможных состояний (позиций) игры, среди к-рых находится ее действительное состояние. И. м. характеризует знания игрока о проплых состояния и выборах как своих (память), так и чужих (информация). Находясь в И. м., игрок должен принять одно репение (выбрать а л ь т е р н а т и в у И. м., т. е. по одной альтернативе сразу для всех позиций из И. м.), к-рое лишь конкретизируется как альтернатива позиции при реальном разыгрывании игры. И. Н. Врублевская.

ИНФОРМАЦИОННОЕ РАССТОЯ НИЕ - метрика или псевдометрика на совокупности распределений вероятностей, характеризующая «непохожесты» описываемых этими распределениями случайных явлений. Наиболее интересны И. р. $\rho(P, Q)$, связанные с мерами информативности әксперимента в задаче различения $P$ и $Q$ по наблюдениям.

В любой конкретной задаче статистик, обработав материалы наблюдений, должен сделать выводы о наблюденном явлении. Эти выводы не будут, вообще говоря, совершенно точными, поскольку исходы наблюдений случайны. Интуитивно понятно, что каждая выборка несет какое-то количество полезной информации, причем: А) при обработке инфорормация может только теряться, Б) информация, доставляемая независимыми источниками, напр. независимыми выборками, суммируется. Таким образом, если ввести и нфо р м а т и ность эк с пе р и мент а как среднее количество информации в наблюдении, то для информативности выполнены аксиомы А и Б. И хотя понятие информации остается интуитивным, иногда можно указать величину $I$, удовлетворяющую аксиомам А и Б, к-рая описывает асимптотику средней точ-