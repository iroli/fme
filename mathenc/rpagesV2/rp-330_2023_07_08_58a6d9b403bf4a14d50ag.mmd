ности выводов в задаче с ростом числа наблюдений и к-рую потому естественно принять за информативность. Информативность может быть как числовой, так и матричной величиной. Важный пример - информационная матрица в задаче оценки параметра закона распределения.

Согласно аксиоме Б, информативности складываются как квадраты длин (катетов), т. е. квадрат разумного И. р. должен обладать свойством аддитивности. Простейшие И. р.- р а с с т о я н и е п о в а р и а ц и и:

\[
\rho_{V}(P, Q)=\int|P(d \omega)-Q(d \omega)| \text {, }
\]

и расстояние в ин ва ри антно й римано в о й ме т р и к е Ф и іIі е р a:

\[
\rho_{F}(P, Q)=2 \arccos \int \sqrt{P(d \omega) Q(d \omega)},
\]

последним свойством не обладают и собственного статистич. смысла не имеют.

По теории Неймана - Пирсона вся полезная информация о различении распределений вероятностей $P(d \omega)$ и $Q(d \omega)$ на общем пространстве $\Omega$ исходов $\omega$ содержится в отнопении правдоподобия или его логариф্фе

\[
\ln p(\omega)-\ln q(\omega)=\ln \frac{d P}{d Q}(\omega),
\]

определенном с точностью до значений на множестве исходов вероятности нуль. Математическое ожидание

\[
\begin{gathered}
I(P: Q)=\int_{\Omega}\left[\ln \frac{p(\omega)}{q(\omega)}\right] P(d \omega)= \\
=\int_{\Omega}\left[\frac{P(d \omega)}{Q(d \omega)} \ln \frac{P(d \omega)}{Q(d \omega)}\right] Q(d \omega)
\end{gathered}
\]

наз. (с р е дн е й) ин фо р м а ц и й р а з лич е-

![](https://cdn.mathpix.com/cropped/2023_07_08_58a6d9b403bf4a14d50ag-1.jpg?height=37&width=831&top_left_y=1212&top_left_x=120)
т и в $Q$, а также о т носит е л ь но й э н т р опи е й, инфо р м а и инным уклоне ни ем. Неотрицательная (может быть, бесконечная) величина $I(P: Q)$ удовлетворяет аксиомам А и Б. Она характеризует точность одностороннего различения $P$ от $Q$, определяя максимальный порядок убывания вероятности $\beta_{N}$ опибки второго рода (т. е. ошіибочного принятия гипотезы $P$, когда она неверна), при росте числа $N$ независимых наблюдений:

\[
\beta_{N} \sim \exp [-N \cdot I(P: Q)]
\]

при фиксированном уровне значимости - вероятности $\alpha_{N}$ ошіибки первого рода, $0<a_{0} \leqslant \alpha_{N} \leqslant a_{1}<1$.

Аналогичная величина $I(Q: P)$ определяет максимальный порядок убывания $\alpha_{N}$ при $0<b_{0} \leqslant \beta_{N} \leqslant b_{1}<1$. Отношение "сходства", в частности «сходства" случайных явлений, не симметрично и, как правило, $I(P: Q) \neq$ $\neq I(Q: P)$. Геометрич. интерпретация $I(P: Q)$ как половины квадрата несимметричного расстояния от $Q$ до $P$ оказалась естественной в ряде вопросов статистики. Для такого И. р. неравенство треугольника неверно, но справедлив несимметричный аналог теоремы Пифагора:

\[
I(R: P)=I(R: Q)+I(Q: P),
\]

если

\[
\int_{\Omega}\left[\ln \frac{q(\omega)}{p(\omega)}\right] Q(d \omega)=\int_{\Omega}\left[\ln \frac{q(\omega)}{p(\omega)}\right] R(d \omega) .
\]

Симметричная характеристика непохожести $P$ и $Q$ возникает при их минимаксном тестировании. Для оптимального теста

\[
\alpha_{N}=\beta_{N} \sim \exp \left[-N I_{P Q}\right]
\]\[
I_{P Q}=-\ln \min _{0<u<1} \int_{\Omega}|P(d \omega)|^{u}|Q(d \omega)|^{1-u}=
\]\[
=\underset{R}{\min } \max \{I(R: P), I(R: Q)\} .
\]

C информационным уклонением связаны также нек-рые другие И. р. (см. [1], [2]). Для бесконечно близких $P$ и $Q$ главная часть информационного уклонения, равно как и квадрата любого разумного И. р., задается, с точностью до постоянного множителя $c(I)$, квадратичной формой Фишера. Для информационного уклонения

\[
c(I)=\frac{1}{2} \text {. }
\]

Лum.: [1] К у л ь б а к С., Теория информации и статистика, пер. с англ., М., 1967; [2] Ч е н ц о в Н. Н., Статистические решающие правила и оптимальные выводы, М. М. 1972.

ИНФОРМАЦИОННЫИ КОЭФФИЦИЕНТ КОРРЕЛЯЦИИ - мера зависимости между двумя случайными величинами $X$ и $Y$, определяемая как функция от величины количества информации в одной случайной величине относительно другой:

\[
\boldsymbol{R}(X, Y)=\sqrt{1-e^{-2 I(X, Y)}}
\]

где $I(X, Y)$ - инбормации количество.

Свойства И. к. к. $R(X, Y)$ как меры зависимости полностью определяются свойствами величины $I(X, Y)$, к-рая сама служит характеристикой зависимости случайных величин $X$ и $Y$. Однако использование И. к. к. $\boldsymbol{R}$ в качестве самостоятельной меры зависимости как информационного аналога обычного коәффициента корреляции $\rho$ оправдано тем, что для произвольных случайных величин И. к. к. имеет преимущество перед $\rho$, так как в силу свойств информации $R=0$ тогда и только тогда, когда $X$ и $Y$ н е за в и с и м ы. Е.сли $X$ и $Y$ имеют совместное нормальное распределение, то эти два коэфффициента совпадают, так как в этом случае

\[
I=-\frac{1}{2} \ln \left(1-\rho^{2}\right) .
\]

Практическое исследование зависимости с помощью И. к. к. равносильно анализу количества информации в таблицах типа таблиц сопряженности признаков. Выборочным аналогом $\boldsymbol{R}$ служит коәфффициент

\[
\hat{R}=\sqrt{1-e^{-2 \hat{I}}}
\]

вычисляемый через информационную статистику $\hat{I}$ :

\[
\hat{I}=\sum_{i=1}^{s} \sum_{j=1}^{t} \frac{n_{i j}}{n} \ln \frac{n n_{i j}}{n_{i} \cdot n_{j}},
\]

где $n$ - число наблюдений, $s$ и $t$ - числа классов группировки по двум признакам, $n_{i j}$ - число наблюдений в классе $(i, j), n_{i}=\sum_{j=1}^{t} n_{i j}, n_{j}=\sum_{i=1}^{s} n_{i j}$. Таким образом, вопрос о распределении выборочного И. к. к. сводится к вопросу о распределении выборочной информации. Анализ выборочной информации как меры зависимости затрудняется тем, что $\widehat{I}$ сильно зависит от группировки наблюдений.

Jum.: [1] L i n f o o t E., "Information and Control», 1957, v. 1, № 1, 85-89; [2] К к у л ь б а K С., Теория информации и статистика, пер. с англ., М., $1967 . \quad$ А. В. Прохоров.

ИНФОРМАЦИЯ - основное понятие кибернетики. Кибернетика изучает машины и живые организмы исключительно с точки зрения их способности воспринимать определенную И., сохранять эту И. в «памяти», передавать ее по каналам связи и перерабатывать ее в «сигналы», направляющие их деятельность в соответствующую сторону. Интуитивное представление об И. относительно каких-либо величин или явлений, содержащейся в нек-рых данных, в кибернетике ограничивается и уточняется.

В нек-рых случаях возможность сравнения различных групп данных по содержащейся в них И. столь же естественна, как возможность сравнения плоских фи-