гур по их «площади»: независимо от способа измерения площадей можно сказать, что фигура $A$ имеет не бо́льшую площадь, чем $B$, если $A$ может быть целиком помещена в $B$ (ср. примеры $1-3$ ниже). Более глубокий факт - возможность выразить площадь числом и на этой основе сравнивать между собой фигуры произвольной формы - является результатом развитой математич. теории. Подобно этому фундаментальным результатом теории И. является утверждение о том, что в определенных, весьма широких, условиях можно пренебречь качественными особенностями И. и выразить ее количество числом. Только этим числом определяются возможности передачи И. по каналам связи и ее хранения в запоминающих устройствах.

$\Pi \mathrm{p}$ и м е р 1. Знание положения и скорости частицы, движущейся в силовом поле, дает И. о ее положении в любой будущий момент времени, притом полную, т. к. это положение может быть предсказано точно. Знание энергии частицы также дает И., но, очевидно, неполную.

П р и м е р 2. Равенство

\[
a=b
\]

дает И. относительно переменных $a$ и $b$. Равенство

\[
a^{2}=b^{2}
\]

дает меньшую И. [т. к. из (1) следует (2), но эти равенства не равносильны]. Наконец, равенство

\[
a^{3}=b^{3}
\]

равносильное (1), дает ту же И., то есть (1) и (3) - это различные формы задания о д н о й и т о й же И.

$\Pi$ р и м е p 3. Результаты произведенных с опибками независимых измерений к.-л. физич. величины дают И. о ее точном значении. Увеличение числа наблюдений увеличивает эту И. П р и м е р 3 а. Среднее арифметическое результатов наблюдений также содсржит нек-рую И. относительно рассматриваемой всличины. Как показывает математич. статистика, в случае нормального распределения вероятностей ошибок с известной дисперсией среднее арифоретическое содержит всю И.

П р и м е р 4. Пусть результатом нек-рого измерения является случайная величина $\xi$. При передаче по нек-рому каналу связи $\xi$ искажается, в результате чего на приемном конце получают величину

\[
\eta=\xi+\theta,
\]

где $\theta$ не зависит от $\xi$ (в смысле төории вероятностей). «Выход» $\eta$ дает И. о «входе» $\xi$, причем естественно ожидать, что эта И. тем меньше, чем больше "рассеяние» значений $\theta$.

В каждом из приведенных примеров данные сравнивались по большей или меньшей полноте содержащейся в них И. В примерах 1-3 смысл такого сравнения ясен и сводится $\boldsymbol{\kappa}$ анализу равносильности или неравносильности нек-рых соотношений. В примерах За и 4 этот смысл требует уточнения. Это уточнение дается, соответственно, математич. статистикой и теорией И. (для к-рых эти примеры являются типичными).

В основе инбормации теории лежит предложенный в $1948 \mathrm{~K}$. Ïенноном (C. Shannon) способ измерения количества И., содержащейся в одном случайном объекте (событии, величине, функции и т. п.) относительно другого случайного объекта. Этот способ приводит к выражению количества И. числом. Положение можно всего лучше объяснить в простейшей обстановке, когда рассматриваемые случайные объекты являются случайными величинами, принимающими липь конечное число значений. Пусть $\xi$ - случайная величина, принимающая значения $x_{1}, x_{2}, \ldots, x_{n}$ с вероятностями $p_{1}, p_{2}, \ldots, p_{n}$, а $\eta-$ случайная величина, принимающая значения $y_{1}, y_{2}, \ldots, y_{m}$ с вероятностями $q_{1}, q_{2}, \ldots, q_{m}$. Тогда И. $I(\xi, \eta)$ относительно $\eta$, содержащаяся в $\xi$, определяется формулой

\[
I(\xi, \eta)=\sum_{i, j} p_{i j} \log _{2}\left(p_{i j} / p_{i} q_{j}\right),
\]

где $p_{i j}$ - вероятность совмещения событий $\xi=x_{i}$ и $\eta=y_{j}$ и логарифмы берутся по основанию 2. И. $I(\xi, \eta)$ обладает рядом свойств, к-рые естественно требовать от меры количества И. Так, всегда $I(\xi, \eta) \geqslant 0$ и равенство $I(\xi, \eta)=0$ возможно тогда и только тогда, когда $p_{i j}=p_{i} q_{j}$ при всех $i$ и $j$, т. е. когда случайные величины $\xi$ и $\eta$ н е з а в и с и м ы. Далее, всегда $I(\xi, \eta) \leqslant I(\eta, \eta)$ и равенство возможно только в случае, когда $\eta$ есть функция от $\xi$ (напр., $\eta=\xi^{2}$ и т. д.). Неожиданным может казаться лишь равенство $I(\xi, \eta)=$ $=I(\eta, \xi)$.

Величина $H(\xi)=I(\xi, \xi)=\sum_{i} p_{i} \log _{2}\left(1 / p_{i}\right)$ носит название энтропии случайной величины $\xi$. Понятие энтропии относится к числу основных понятий теории И. Количество И. и энтропия связаны соотношением

\[
I(\xi, \eta)=H(\xi)+H(\eta)-H(\xi, \eta),
\]

где $H(\xi, \eta)$-- энтропия пары $(\xi, \eta)$, т. е.

\[
H(\xi, \eta)=\sum_{i, j} p_{i j} \log _{2}\left(1 / p_{i j}\right)
\]

Величина энтропии указывает среднее число двоичных знаков, необходимое для различения (или записи) возможных значений случайной величины. Әто обстоятельство позволяет понять роль количества И. (4) при «хранении»И. в запоминающих устройствах. Если случайные величины $\xi$ и $\eta$ независимы, то для записи значения $\xi$ требуется в среднем $H(\xi)$ двоичных знаков, для значения $\eta$ требуется $H(\eta)$ двоичных знаков, а для пары $(\xi, \eta)$ требуется $H(\xi)+H(\eta)$ двоичных знаков. Если же случайные величины $\xi$ и $\eta$ зависимы, то среднее число двоичных знаков, необходимое для записи пары $(\xi, \eta)$, оказывается меньшим суммы $H(\xi)+$ $+H(\eta)$, так как $H(\xi, \eta)=H(\xi)+H(\eta)-\dot{I}(\xi, \eta)$.

С помощью значительно более глубоких теорем выясняется роль количества И. (4) в вопросах передачи И. по каналам связи. Основная информационная характеристика каналов, так наз. е мк о с т ь, определяется через понятие «И.».

Если $\xi$ и $\eta$ могут принимать бесконечное число значений, то предельным переходом из (4) получается формула

\[
I(\xi, \eta)=\iint p(x, y) \log _{2} \frac{p(x, y)}{p(x) q(y)} d x d y,
\]

где буквами $p$ и $q$ обозначены соответствующие плотности вероятности. При этом энтропии $H(\xi)$ и $H(\eta)$ не существуют, но имеет место формула, аналогичная (5) где

\[
I(\xi, \eta)=h(\xi)+h(\eta)-h(\xi, \eta),
\]

\[
h(\xi)=\int p(x) \log _{2} \frac{1}{p(x)} d x
\]

- дифференциальная энтропия $\xi[h(\eta)$ и $h(\xi, \eta)$ определяются подобным же образом].

П р и м е $\mathrm{p}$ 5. Пусть в условиях примера 4 случайные величины $\xi$ и $\theta$ имеют нормальное распределение вероятностей с нулевыми средними значениями и дисперсиями, равными соответственно $\sigma_{\xi}^{2}$ и $\sigma_{\theta}^{2}$. Тогда, как можно подсчитать по формулам (6) или $(7): I(\eta, \xi)=$ $=I(\xi, \eta)=\frac{1}{2} \log _{2}\left[1+\sigma_{\xi}^{2} / \sigma_{\theta}^{2}\right]$. Таким образом, количество И. в «принятом сигнале» $\eta$ относительно «переданного сигнала» $\xi$ стремится к нулю при возраставии уровня «помех» $\theta$ (т. е. при $\sigma_{\theta}^{2} \rightarrow \infty$ ) и неограниченно возрастает при исчезающе малом влиянии «помех» (т. е. при $\sigma_{\theta}^{2} \rightarrow 0$ ).