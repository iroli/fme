$a$ заменено на $a+\varepsilon, \varepsilon>0$. Имеет место теорема кодирования (теорема Шеннона): пусть заданы информационно-устойчивая последовательность сообщений с распределением вероятностей $p^{t}(\cdot)$ и условиями точности $W^{t}$ и информационно-устойчивая последовательность каналов $\left(Q^{t}, V^{t}\right)$ такие, что функции $\pi^{t}(\cdot)$ и $\rho^{t}(\cdot, \cdot)$ равномерно ограничены по $t$; пусть $H_{W} t\left(p^{t}\right) \rightarrow \infty$ при $t \rightarrow \infty$ и

\[
\lim _{t \rightarrow \infty} \frac{C\left(Q^{t}, V^{t}\right)}{H_{W}{ }^{\left(p^{t}\right)}}>1
\]

тогда для любого $\varepsilon>0$ существует столь большое $t_{0}$, что при всех $t \geqslant t_{0}$ сообщение с распределением вероятностей $p^{t}(\cdot)$ может быть передано через канал $\left(Q^{t}, V_{\varepsilon}^{t}\right)$ с точностью воспрогзведения $W_{\varepsilon}^{t}$. Эта формулировка прямого утвержденія теоремы кодирования является одной из наиболее общих. Предположение об ограниченности функций $\pi^{t}(\cdot)$ и $\rho^{t}(\cdot, \cdot)$ может быть существенно ослаблено. Информационная устойчивость последовательности сообщений и каналов всегда имеет место в большөм числе практически интересных частных случаев. Наконец, при нек-рых условиях в сформулированной теореме можно заменить $W_{\varepsilon}^{t}$ на $W^{t}$ и $V_{\varepsilon}^{t}$ на $V^{t}$.

Для онисания реальных ситуаций наиболее интересен случай, когда последовательность каналов, рассматриваемая в теореме, есть последовательность отрезков данного фиксированного канала, а последовательность сообщений - это последовательность отрезков сообщений фиксированного источника с растущим числом компонент. Наглядно это соответствует функціонированию системы связи во времени. Ниже для әтой ситуации сформулированы нек-рый вариант теоремы кодирования и его обращение в несколько отличной от предыдущей форме. При этом рассматриваются дискретный стационарный источник и канал без памяти.

Пусть дискретный стационарный источник $U$, вырабатывающий сообение $\xi=\left\{\xi_{k}, k=\ldots,-1,0\right.$, $1, \ldots\}$, где отдельные компоненты (буквы сообщения $\xi_{k}$ ) принимают значения из нек-рого конечного множества алфавита $X$ объема $M$, производит буквы сообщения со скоростью одна буква в единицу времени. Компоненты сообщения $\tilde{\xi}=\left\{\bar{\xi}_{k}, k=\ldots,-1,0,1, \ldots\right\}$, получаемого адресатом, принимают значения из того же алф্фавита $X$ (т. е. $\tilde{X}=X)$. Пусть, далее, используется дискретный канал без памяти, передача по к-рому ведется со скоростью один символ в интервал времени $\tau$. Ограничения на распределение сигнала на входе канала отсутствуют. Пусть отрезок сообщения $\xi^{I .}=$ $=\left(\xi_{1}, \ldots, \xi_{L}\right)$ длины $L=L_{N}$ передается по отрезку канала длины $N=[L / \tau]$ (где $[x]-$ целая часть числа $x$ ) с помощью нек-рых методов кодирования и декодирования описанного выше типа. Если при этом $\tilde{\xi}^{L}=$ $=\left(\widetilde{\xi}_{1}, \ldots, \xi_{L}\right)-$ соответствующий отрезок сообщения, полученный адресатом, а $\hat{P}_{e}-$ средняя вероятность ошибки на букву источника, определяемая формулой

\[
\hat{P}_{e}=\frac{1}{L} \sum_{l=1}^{L} \mathrm{P}\left\{\tilde{\xi}_{l} \neq \xi_{l}\right\}
\]

то справедлива следующая теорема.

Обра ше н и те о ремы коди ров а и я. Пусть $H(U)$-скорость создания сообщений данным дискретным стационарным источником и $C$ - пропускная способность (на передаваемый символ) используемого канала без памяти. Тогда при всех $L$ справедливо неравенство

\[
\widehat{\boldsymbol{P}}_{e} \log (M-1)+h\left(\hat{P}_{e}\right) \geqslant H(U)-\frac{1}{\tau} C,
\]

где

\[
h(x)=-x \log x-(1-x) \log (1-x) .
\]

Таким образом, если скорость создания сообщений $H(U)$ больше, чем $\frac{1}{\tau} C$ (пропускная способность канала на букву источника), то средняя вероятность ошибки на букву источника при любом $L$ и для любых методов кодирования и декодирования ограничена снизу отличной от нуля константой и, значит, не стремится к нулю даже при $L \rightarrow \infty$. Чтобы сформулировать прямое утверждение теоремы кодирования, необходима величина

\[
R_{N}=\frac{\log _{2} M^{L_{N}}}{N} .
\]

В случае, когда $M=2$ и $L_{N} / \tau-$ целое число, $R_{N}=\tau$, т. е. $\boldsymbol{R}_{N}$ в битаx является числом двоичных символов, вырабатываемых источником за время передачи одного символа по каналу. Кроме того, $R_{N}$ совпадает со скоростью создания сообщений $H(U)$ источником $U$ с независимыми и равномерно распределенными компонентами. Вероятность ошибки и средняя вероятность ошибки на блок источника определяются соответственно формулами:

\[
\begin{gathered}
P_{e, x^{L}}=\mathrm{P}_{x^{L}}\left\{\tilde{\xi}^{L} \neq \xi^{L}\right\}, \\
\bar{P}_{e}=\sum_{x^{L}} \mathrm{P}\left\{\xi^{L}=x^{L}\right\} P_{e, x^{L}}
\end{gathered}
\]

здесь $P_{{ }_{x}}(\cdot)$ - условная вероятность при условии, что $\quad \xi^{L}=x^{L} \in X^{L}$. Справедлива следующая т е оре ма ко ди р о в а ния. Для всех $N$ и любого $R<C$ существуют методы кодирования и декодирования такие, что при $R_{N} \leqslant R$ для всех $x^{I} \in X^{L}$

\[
P_{e, x^{L}} \leqslant \exp \{-N E(R)\}
\]

(оценка (17) справедлива и для $\overrightarrow{P_{e}}$ ), причем для $0 \leqslant R<C$ функция $E(R)$ выпуклая, положительная и убывает с ростом $R$ (см. также Ошибочного декодирования вероятность). Таким образом, эта теорема показывает, что для всех $R<C$ вероятность ошибки с ростом $N$ стремится к нулю и притом экспоненциально быстро.

Имеются обобщения теорем Шеннона на случай так наз. составных каналов и сообщений с неизвестными параметрами. Интерес к подобным обобщениям вызван тем, что обычно на практике нельзя считать полностью известными статистич. параметры источника сообщений и канала связи, тем более что эти параметры могут иногда меняться в процессе передачи. Поэтому приходится предполагать, что источник сообщений и канал связи принадлежат нек-рому классу возможных источников сообщений и каналов. При этом вводится минимаксный критерий качества передачи, при к-ром качество данного метода передачи оценивается для наихудших возможных источников сообщеншй и каналов, принадлежащих рассматриваемому классу.

Имеются также обобщения теорем Шеннона на случай И. П. по каналу с обратной связью. Наличие полной обратной связи означает, что в момент времени $t$ на передающей стороне канала (т. е. на его входе) считаются известными точные значения сигналов на выходе канала для всех моментов времени $t^{\prime}<t$. В частности, для каналов без памяти с обратной связью основной результат состоит в том, что наличие обратной связи не увеличивает пропускную способность канала, хотя и может существенно уменьшить сложность кодирующих и декодирующих устройств.

Из других обобщений следует отметить теорию И. п. по каналам с опибками синхронизации, в к-рых воз-