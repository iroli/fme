функция) $\delta^{*}=\delta^{*}(x)$ определяется как функция, на к-рой достигаются минимальные полные потери

\[
\inf _{\delta} \rho(\pi, \delta)=\inf _{\delta} \int_{\Theta} \int_{X}|\theta-\delta(x)|^{2} P_{\theta}(d x) \pi(d \theta),
\]

или, что эквивалентно, минимальные условные потери

\[
\inf _{\delta}^{E}\left\{[\theta-\delta(x)]^{2} \mid x\right\}
\]

Отсюда следует, что в случае квадратичной функции потерь Б. о. $\delta^{*}(x)$ совпадает с апостериорным средним: $\delta^{*}(x)=\mathrm{E}(\theta \mid x)$, а б е й е с о в с ки й $\mathrm{p}$ и с к

\[
\rho\left(\pi, \delta^{*}\right)=E[D(\theta \mid x)],
\]

где $D(\theta \mid x)$ - дисперсия апостериорного распределения:

\[
\mathrm{D}(\theta \mid x)=\mathrm{E}\left\{[\theta-\mathrm{E}(\theta \mid x)]^{2} \mid x\right\} .
\]

П р име е. Пусть $x=\left(x_{1}, \ldots, x_{n}\right)$, где $x_{1}, \ldots$, $x_{n}$ - независимые одинаково распределенные случайные величины, имеющие нормальные распределения $N\left(\theta, \sigma^{2}\right), \sigma^{2}$ известно, а неизвестный параметр $\theta$ имеет нормальное распределение $N\left(\mu, \tau^{2}\right)$. Поскольку апостериорное распределение для $\theta$ (при заданном $x$ ) является нормальным $N\left(\mu_{n}, \tau_{n}^{2}\right)$ с

\[
\mu_{n}=\frac{n \bar{x} \sigma^{-2}+\mu \tau^{-2}}{n \sigma^{-2}+\tau^{-2}}, \quad \tau_{n}^{-2}=n \sigma^{-2}+\tau^{-2},
\]

где $\bar{x}=\left(\bar{x}_{1}+\ldots+\bar{x}_{n}\right) / n$, то в случае квадратичной функции потерь $|\dot{\theta}-d|^{2}$ бейесовская оценка $\delta^{*}(x)=\mu_{n}$, а бейесовский риск равен $\tau_{n}^{2}=\sigma^{2} \tau^{2} /\left(n \tau^{2}+\sigma^{2}\right)$. А. Н. нІирлев.

БЕЙСОВСКАЯ РЕШАЮШАЯ ФУНКЦИЯ - правило (функция) $\delta=\delta(x)$, которое сопоставляет каждому результату статистич. эксперимента $x$ решение $\delta(x)$ со значениями в заданном множестве решений и доставляет минимум полным потерям, определяемым в рамках бейесовского подхода к статистич. задачам. А. Н. Ширлев.

БЕЙЕСОВСКИЙ ПОДХОД К с к и м з а д ач а м - подход, основанный на предположении, что всякому параметру в статистич. проблеме принятия решения приписано нек-рое распределение вероятностей. Всякая общая статистич. проблема принятия решения определяется следующими элементами: пространством $(X, \mathscr{B} X)$ выборок $x$, пространством $\left(\Theta, \mathscr{B}_{\Theta}\right)$ значений неизвестного параметра $\theta$, семейством. распределений вероятностей $\left\{P_{0}, \theta \in \Theta\right\}$ на $\left(X, \mathscr{B}_{X}\right)$, пространством решений $\left(D, \mathscr{B}_{D}\right)$ и функцией $L(\theta, d)$, характеризующей потери от принятия решения $d$, когда истинное значение параметра есть $\theta$. Цель же проблемы принятия решения состоит в отыскании в определенном смысле наилучшего правила (решающей функции) $\delta=\delta(x)$, сопоставляющей каждому результату ваблюдения $x \in X$ решение $\delta(x) \in D$. При Б. П., когда считается, что неизвестный параметр $\theta$ является случайной величиной с заданным (априорным) распределением $\pi=\pi(d \theta)$ на $\left(\theta, \mathscr{B}_{\theta}\right)$, наилучшая решающая функция (бейесовская решающая бункция) $\delta^{*}=\delta^{*}(x)$ определяется как функция, на к-рой достигаются минимальные полные потери $\inf _{\delta} \rho(\pi, \delta)$, где

\[
\rho(\pi, \delta)=\int_{\Theta} \rho(\theta, \delta) \pi(d \theta),
\]

a

\[
\rho(\theta, \delta)=\int_{X} L(\theta, \delta(x)) P_{\theta}(d x)
\]

Таким образом,

\[
\rho\left(\pi, \delta^{*}\right)=\inf _{\delta} \int_{\Theta} \int_{X} L(\theta, \delta(x)) P_{\theta}(d x) \pi(d \theta) .
\]

При отыскании бейесовской решающей функции $\delta^{*}=$ $=\delta *(x)$ полезным оказывается следующее замечание. Пусть $P_{\theta}(d x)=p(x \mid \theta) d \mu(x), \pi(d \theta)=\pi(\theta) d v(\theta)$, где $\mu$ и $v$ - некоторые $\sigma$-конечные меры. Тогда, предполагая возможным смену порядков интегрирования, находим

\[
\begin{gathered}
\int_{\Theta} \int_{X} L(\theta, \delta(x)) P_{\theta}(d x) \pi(d \theta)= \\
=\int_{\Theta} \int_{X} L(\theta, \delta(x)) p(x \mid \theta) \pi(\theta) d \mu(x) d v(\theta)= \\
=\int_{X} d \mu(x)\left[\int_{\Theta} L(\theta, \delta(x)) p(x \mid \theta) \pi(\theta) d v(\theta)\right] .
\end{gathered}
\]

Отсюда видно, что для данного $x \in X \delta^{*}(x)$ есть то значение $d^{*}$, на к-ром достигается

\[
\inf _{d} \int_{\theta} L(\theta, d) p(x \mid \theta) \pi(\theta) d v(\theta)
\]

или, что эквивалентно,

\[
\inf _{d} \int_{\Theta} L(\theta, d) \frac{p(x \mid \theta) \pi(\theta)}{p(x)} d v(\theta)
\]

где

\[
p(x)=\int_{\Theta} p(x \mid \theta) \pi(\theta) d v(\theta) .
\]

Но по Бейеса бормуле

\[
\int_{\Theta} L(\theta, d) \frac{p(x \mid \theta) \pi(\theta)}{p(x)} d v(\theta)=\mathrm{E}[L(\theta, d) \mid x] .
\]

Тем самым для данного $x \delta^{*}(x)$ есть то значение $d^{*}$, на к-ром достигают минимума условные средние потери $\mathrm{E}[L(\theta, d) \mid x]$.

П р и м е р (Б. П. в задаче различения двух простых гипотез). Пусть $\Theta=\left\{\theta_{1}, \theta_{2}\right\}, D=\left\{d_{1}, d_{2}\right\}, L_{i j}=$ $=L\left(\theta_{i}, d_{j}\right), i, j=1,2 ; \pi\left(\theta_{1}\right)=\pi_{1}, \pi\left(\theta_{2}\right)=\pi_{2}, \pi_{1}+\pi_{2}=1$. Отождествляя решение $d_{i}$ с принятием гипотезы $H_{i}$ : $\theta=\theta_{i}$, естественно считать, что $L_{11}<L_{12}, L_{22}<L_{21}$. Тогда

\[
\begin{gathered}
\rho(\pi, \delta)=\int_{X}\left[\pi_{1} p\left(x \mid \theta_{1}\right) L\left(\theta_{1}, \delta(x)\right)+\right. \\
\left.+\pi_{2} p\left(x \mid \theta_{2}\right) L\left(\theta_{2}, \delta(x)\right)\right] d \mu(x),
\end{gathered}
\]

откуда следует, что $\inf _{\delta} \rho(\pi, \delta)$ достигается на функции

\[
\delta^{*}(x)=\left\{\begin{array}{l}
d_{1}, \text { если } \frac{p\left(x \mid \theta_{2}\right)}{p\left(x \mid \theta_{1}\right)} \leqslant \frac{\pi_{1}}{\pi_{2}} \frac{L_{12}-L_{11}}{L_{21}-L_{22}}, \\
d_{2}, \text { если } \frac{p\left(x \mid \theta_{2}\right)}{p\left(x \mid \theta_{1}\right)}>\frac{\pi_{1}}{\pi_{2}} \frac{L_{12}-L_{11}}{L_{21}-L_{22}} .
\end{array}\right.
\]

Преимущество Б. п. состоит в том, что полные потери $\rho(\pi, \delta)$ оказываются числом (в отличие от потерь $\rho(\theta, \delta)$, зависящих от неизвестного параметра $\theta$ ), и, следовательно, заведомо существуют, если и не оптимальные, то, по крайней мере, $\varepsilon$-оптимальные $(\varepsilon>0)$ решающие функции $\delta_{\varepsilon}^{*}$, для к-рых

\[
\rho\left(\pi, \delta_{\varepsilon}^{*}\right) \leqslant \inf _{\delta} \rho(\pi, \delta)+\varepsilon .
\]

Недостатком Б. п. является необходимость постулировать как существование априорного распределения для неизвестного параметра, так и знание его формы (в определенной степени последнее обстоятельство преодолевается в рамках бейесовского подхода әмпирического).

![](https://cdn.mathpix.com/cropped/2023_07_08_3daf383f6ffd4bcff57fg-1.jpg?height=50&width=803&top_left_y=1828&top_left_x=1020)

![](https://cdn.mathpix.com/cropped/2023_07_08_3daf383f6ffd4bcff57fg-1.jpg?height=70&width=836&top_left_y=1848&top_left_x=989)

БЕИЕСОВСКИЙ ПОДХОД ЭМПИРИЧЕСКИЙ статистич. интерпретация бейесовского подхода к построению выводов о ненаблюдаемых значения параметров при неизвестном их априорном распределении. Пусть $(Y, X)$ - случайный вектор, причем предполагается, что плотность $p(y \mid x)$ условного распределения $Y$ при любом заданном значении случайного параметра $X=x$ известна. Если в результате нек-рого эксперимента наблюдается лишь реализация $Y$, а соответствующая реализация $X$ неизвестна и требуется оденить значение заданной функции $\varphi(X)$ от ненаблюдаемой реализации, то согласно Б. П. э. в качестве приближенного значения $\psi(Y)$ для $\varphi(X)$ следует ис-