традиционных исследований по В. И. Интерес к ним вновь возрос в нач. 20 в. Были предложены новые способы редукции к задаче об отыскании экстремума функции многих переменных. Наиболее важным из них является Ритца метод, согласно к-рому решение задачи о минимуме (10) при условии (11) разыскивается на классе функций вида

\[
x=\varphi_{0}(t)+\sum_{i=1}^{N} a_{i} \varphi_{i}(t),
\]

где $\varphi_{i}(t), i=0,1, \ldots, N$ - элементы бесконечной полной системы линейно независимых функций, удовлетворяющих граничным условиям

$\varphi_{0}\left(t_{0}\right)=x_{0}, \varphi_{0}\left(t_{1}\right)=x_{1}, \varphi_{i}\left(t_{0}\right)=\varphi_{i}\left(t_{1}\right)=0, i=1,2, \ldots$ Задача сводится к отысканию минимума функции $N$ переменных

\[
J=J\left(a_{1}, \ldots, a_{N}\right) .
\]

Метод Ритца является достаточно общим. Он применяется для решения вариационных задач математич. физики, заключающихся в минимизации функционала, зависящего от функций многих переменных. Его дальнейшим обобщением для данного класса задач является метод (см. [2]), в к-ром коэффициенты считаются неизвестными функциями одного из независимых переменных (напр., если в задаче две независимые переменные $t$ и $\tau$, то $a_{i}$ могут задаваться в виде $\left.a_{i}(\tau)\right)$. Исходный функционал становится зависящим от $N$ функций $a_{i}(\tau)$, к-рые могут определяться с помощью необходимых условий, т. е., в конечном счете, из решения краевой задачи для системы $N$ уравнений Әйлера.

Потребности практики увеличили интерес к неклассич. задачам оптимального управления. Наличие в технич. задачах сложных ограничений на фазовые координаты и управляющие функции, разрывность правых частей дифференциальных уравнений, возможность существования особых и скользящих оптимальных режимов И т. Д. - все это потребовало разработки новых разновидностей прямых методов. Наибольшее распространение получили методы, использующие идеи спуска в пространстве управлений и идеи последовательного анализа вариантов (типа динамического програ.м.ирования).

Методы спуска в пространстве управлений основаны на получении последовательности управлений $u_{k} \in U$ вида

\[
u_{k+1}(t)=u_{k}(t)+\delta u_{k}(t),
\]

к-рой соответствует монотонно убывающая последовательность значений функционала. Пусть, напр., ищется минимум функционала

\[
J=F(x(T))
\]

при условиях (2), (3) п (5) ( $U$ - выпуклое и односвязное множество). Отыскание $\delta u_{k}(t)$ производится следующим образом. С помощью уравнений в вариациях для (2) и сопряженной системы (7) с условиями на правом конце

\[
\psi_{i}(T)=-\frac{\partial F(x(T))}{\partial x^{i}}, \quad i=1, \ldots, n,
\]

линейная часть приращения функционала (13) от вариации $\delta u$ представляется в виде

\[
\delta J=-\int_{t_{0}}^{T} \frac{\partial H}{\partial u} \delta u d t
\]

Для уменьшения функционала (13) следует на каждой птерации выбирать приращение

\[
\delta u_{k}(t)=x \frac{\partial H}{\partial u}, \quad x>0
\]

где величина $\partial H / \partial u$ вычисляется на управлении $u_{k}(t)$ и соответствующей ему траектории $x_{k}(t)$. Закон- ность линеаризации, а следовательно, и уменьшение функционала (13) обеспечиваются выбором достаточно малой величины $x$. Процесс спуска (12) начинается с нек-рого $u_{0}(t)$ и заканчивается, когда на нек-рой итерации $|\delta J|$ становится меныше нек-рого заданного $\varepsilon$. Для описанного случая свободного правого конца алгоритм получается наиболее простым (см. [5], [6], [7]). Весьма эффективным для решения задач со свободным концом является метод (см. [8]), к-рый не использует линеаризации исходной задачи. В случае, когда граничные условия заданы и на правом конце, все эти алгоритмы существенно усложняются. Для учета граничных условий в [5] привлекается процедура проектирования градиента, а в [6] вводится штраф за невыполнение граничных условий, т. е. вместо (13) рассматрив ается функционал

\[
J=F(x(T))+\sum_{i=1}^{n} \lambda_{i}\left(x^{i}(T)-x_{T}^{i}\right)^{2}, \quad \lambda_{i}>0 . \text { (14) }
\]

К градиентным методам примыкает метод [9], в к-ром приращение управления определяется из решения вспомогательной задачи линейного программирования.

Болышая группа прямых методов численного решения задач оптимального управления основана на идеях последовательного анализа вариантов (см. [10], [11], [12]). Важным достоинством этих методов является то, что с их помощью удается решать задачи с фазовыми ограничениями вида

\[
x \in G
\]

где $G$ - замкнутое множество $n$-мерного пространства. Их основной недостаток - существенное возрастание трудностей с увеличением размерности пространства. Эти методы используют редукцию исходной задачи к задаче нелинейного программирования специального вида. Распространены два способа такой редукции. Согласно первому способу в конечном итоге получается задача минимизации функции, зависящей только от управлений, заданных в точках дискретной сетки на оси (см. [13]). Во втором способе (см. [12]) управление исключается и задача сводится к минимизации функции вида

\[
J\left(x_{0}, \ldots, x_{N}\right)=\sum_{i=0}^{N-1} f_{i}\left(x_{i}, x_{i+1}\right),
\]

где $x_{i}$ - значение вектора $x$ в точке $t_{i}, i=0,1, \ldots, N$, при ограничения $\mathbf{x}$

\[
x_{i} \in G_{i}
\]

к-рые получаются из ограничений (3), (4), (15). Для пояснения схемы решения задачи минимизации (16) при условиях (17) удобно использовать следующую геометрич. интерпретацию. Каждой совокупности векторов $\left\{x_{0}, x_{1}, \ldots, x_{N}\right\}$ ставится в соответствие ломаная (см. рис.), к-рая проходит через точки $x_{0}, \ldots, x_{N}$, лежащие в гиперплоскостях $\Sigma_{i}$, задаваемых уравненияміи $t=t_{i}$. Длина этой ломаной $J\left(x_{0}, \ldots, x_{N}\right)$ складывается из длин $f_{i}\left(x_{i}, x_{i+1}\right)$ отдельных

![](https://cdn.mathpix.com/cropped/2023_07_08_bf066e4eec0e8313418cg-1.jpg?height=286&width=405&top_left_y=1683&top_left_x=1405)
звеньев. Область допустимых значений $x_{i}$ задается (17) и эта область отделяется от запретной нек-рой ломаной (на рис. запретная область заштрихована). Задача состоит в отыскании ломаной наименышей длины, лежащей в допустимой области и соединя ющей гиперплоскости $\Sigma_{0}$ и $\Sigma_{N}$. Алгоритм решения задачи представляет собой многошаговый процесс, на каждом шаге $i$ к-рого отметается нек-рое множество вариантов $\Omega_{i}$, заведомо не содержащее оптимальной ломаной. На нулевом шаге определяется функция

\[
l\left(x_{1}\right)=\min _{x_{0} \in G_{0}} f_{0}\left(x_{0}, x_{1}\right),
\]