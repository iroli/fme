приводит при нек-рых весьма общих условиях к результату, почти не зависящему от случая. Сближение частоты наступления случайного события с его вероятностью при возрастании числа испытаний (подмеченное сначала, по-відимому, на азартных играх) может служить первым примером действия этого принципа.

На рубеже 17 и 18 вв. Я. Бернулли [1] доказал теорему, утверждающую, что в последовательности независимых испытаниї, в каждом из к-рых вероятность наступления нек-рого события $A$ имеет одно и то же значение $p, 0<p<1$, верно соотношение:

\[
P\left\{\left|\frac{\mu_{n}}{n}-p\right|>\varepsilon\right\} \rightarrow 0
\]

при любом $\varepsilon>0$ и $n \rightarrow \infty$; здесь $\mu_{n}-$ число появлений события в первых $n$ испытаниях, $\mu_{n} / n$ - частота появлений. Эта Бернулли теорема была распространена C. Пуассоном [2] на случай последовательности независимых испытаний, где вероятность появления события $A$ может зависеть от номера испытания. Пусть эта вероятность для $k$-го испытания равна $p_{k}, k=1,2,3$, ..., п пусть

\[
\bar{p}=\frac{p_{1}+p_{2}+\ldots+p_{n}}{n} .
\]

Тогда Пуассона теорема утверждает, что

\[
\mathrm{P}\left\{\left|\frac{\mu_{n}}{n}-\bar{p}\right|>\varepsilon\right\} \rightarrow 0
\]

при любом $\varepsilon>0$ и $n \rightarrow \infty$. Первое строгое доказательство этой теоремы было дано П. Л. Чебышевым (1846), метод к-рого полностью отличен от метода Пуассона и основан на нек-рых экстремальных соображениях; С. Пуассон выводил (2) из приближенной формулы для указанной вероятности, основанной на использовании закона Гаусса і в то время еще строго не обоснованной. У C. Пуассона впервые встречается и термин «закон больших чисел», к-рым он назвал свое обобщение теоремы Бернулли.

Естественное дальнейшее обобщение теорем Бернулли и Пуассона возникает, если заметить, что случайные величины $\mu_{n}$ можно представить в виде суммы

\[
\mu_{n}=X_{1}+X_{2}+\ldots+X_{n}
\]

независимых случайных велічин, где $X_{k}=1$, если $A$ появляется в $k$-м испытании, и $X_{k}=0-$ в противном случае. Прі этом математич. ожидание $\mathrm{E}\left(\mu_{n} / n\right)$ (совпадающее со средним арифметическим математич. ожиданий $\mathrm{E} X_{k}$ ) равно $p$ для случая Бернулли и $\bar{p}$ для случая Пуассона. Другими словами, в обоих случаях рассматривается отклонение среднего арифметического величин $X_{k}$ от среднего арифметического их математич. ожиданий.

В работе П. Л. Чебышева «О средних величинах" (1867) было установлено, что для независимых случайных величин $X_{1}, X_{2}, \ldots, X_{n}, \ldots$ соотношение

\[
\mathrm{P}\left\{\left|\frac{X_{1}+\ldots+X_{n}}{n}-\frac{\mathrm{E} X_{1}+\ldots+\mathrm{EX} X_{n}}{n}\right|>\varepsilon\right\} \rightarrow 0
\]

(при любом $\varepsilon>0$ и $n \rightarrow \infty)$ верно при весьма общих предположениях. П. Л. Чебышев предполагал, что математич. ожидания $E X_{\hat{R}}^{2}$ все ограничены одной и той же постоянной, хотя из его доказательства видно, что достаточно требования ограниченности дисперсий $X_{k}, \mathrm{D} X_{k}=\mathrm{E} X_{k}^{2}-\left(\mathrm{E} X_{k}\right)^{2}$, или даже требования

\[
B_{n}^{2}=\mathrm{D} X_{1}+\ldots+\mathrm{D} X_{n}=o\left(n^{2}\right) \text { при } n \rightarrow \infty .
\]

Таким образом, П. Л. Чебышев показал возможность широкого обобщения теоремы Бернулли. А. А. Марков отметил возможность дальнейших обобщений и предложил применять название Б. ч. з. ко всей совокупности обобщений теоремы Бернулли [и в частности, к (3)]. Метод Чебышева основан на точном установлении общих свойств математич. ожиданий и на использовании так наз. Чебышева неравенства [для вероятности (3) оно дает оценку вида

\[
n^{-2} \varepsilon^{-2} \sum_{k=1}^{n} \mathrm{D} X_{k}
\]

эту границу можно заменить более точной, разумеется при более значительных ограничениях, см. Бернштейна неравенство]. Последующие доказательства различных форм Б. ч. з. в той или иной степени являются развитием метода Чебышева. Применяя надлежащее "урезание» случайных величин $X_{k}$ (замену их вспомогательными величинами $X_{n, k}^{\prime}$; именно: $X_{n, k}^{\prime}=X_{k}$, если $\left|X_{k}-\mathrm{E} X_{k}\right| \leqslant L_{n}$, и $X_{n, k}^{\prime}=0$, если $\left|X_{k}-\mathrm{E} X_{k}\right|>L_{n}$, где $L_{n}$ - нек-рые постоянные), А. А. Марков распространил Б. ч. з. на случаи, когда дисперсии слагаемых не существуют. Напр., он показал, что (3) имеет место, если при нек-рых постоянных $\delta>0$ и $L>0$ и всех $n$

\[
\mathrm{E}\left|X_{n}-\mathrm{E} X_{n}\right|^{1+\delta}<L .
\]

Аналогично доказывается т е о р е м а $\mathrm{X}$ и н ч и н а (1929): если $X_{n}$ имеют одинаковые законы распределения и $\mathrm{E} X_{n}$ существует, то Б. ч. 3. (3) выполняется.

Для сумм независимых случайных величин можно сформулировать более или менее окончательный вариант Б. ч. 3. Для этого целесообразно перейти на более общую точку зрения, связанную с понятием предельного постоянства последовательности случайных величин. Случайные величины последовательности $Y_{1}$, $Y_{2}, \ldots, Y_{n}, \ldots$ наз. $п$ р е д е л ь н о п о с т о я нн ы м и, если существует такая последовательность постоянных $C_{1}, C_{2}, \ldots, C_{n}, \ldots$, что при любом $\varepsilon>0$ и $n \rightarrow \infty$

\[
\mathrm{P}\left\{\left|Y_{n}-C_{n}\right|>\varepsilon\right\} \longrightarrow 0
\]

(т. е. $Y_{n}-C_{n}$ сходится к нулю «по вероятности»; если (4) выполняется с к.-л. $C_{n}$, то оно выполняется и с $C_{n}^{\prime}=$ $=m Y_{n}$, где $m Y$ - медиана случайной величины $Y$ ). Далее, вместо последовательности $X_{1}, X_{2}, \ldots, X_{n}$, ... независимых случайных величин можно взять так наз. схему серий (см. Сериц̆ схема):

\[
\begin{aligned}
& X_{1,1}, \ldots, X_{1, k_{1}}, \\
& X_{2,1}, \ldots, X_{2, k_{2}}, \\
& \dot{X}_{n, 1}, \ldots \cdot X_{n, k_{n}},
\end{aligned}
\]

случайных величин (первый индекс - номер серии, второй - номер величины внутри серии). Случайные величины каждой отдельной серии предполагаются взаимно независимыми. Схему последовательности легко свести к схеме серий, полагая $k_{1}=1, k_{2}=2, \ldots$, $X_{n, k}=X_{k} / n$.

Пусть

\[
Y_{n}=X_{n, 1}+\ldots+X_{n, k_{n}}
\]

Тогда общая форма вопроса 0 применимости Б.ч.з. для сумм независимых случайных величин такова: при каких условиях суммы $Y_{n}$ предельно постоянны?

Ответ на этот вопрос дал А. Н. Колмогоров (1928). Допустим, не ограничивая общности, что медианы величин $X_{n, k}$ равны нулю. Пусть $\tilde{X}_{n, k}=X_{n, k}$ при $\left|X_{n, k}\right| \leqslant 1$ и $\tilde{X}_{n, k}=0$ при $\left|X_{n, k}\right|>1$. Тогда одновременное выполнение двух условий:

$\boldsymbol{\Pi}$

\[
\sum_{k=1}^{k_{n}} \mathrm{P}\left\{\left|X_{n, k}\right|>1\right\} \longrightarrow 0 \text { при } n \longrightarrow \infty
\]

\[
\sum_{k=1}^{k} \mathrm{E} \tilde{X}_{n, k}^{2} \longrightarrow 0 \text { при } n \longrightarrow \infty,
\]