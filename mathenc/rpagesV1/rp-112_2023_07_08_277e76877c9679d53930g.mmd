с л о в а $x$. Интуитивно под этим понимается минимальное количество информации, к-рую надо иметь, чтобы по каждому числу $i \leqslant l(x)$ найти $i$-й знак слова $x$ (длину с пова $x$ при этом можно и не знать). Точнее, под сложностью разрешения слова $x=x_{1} x_{2} \ldots x_{l(x)}$ по частично рекурсивной функции $G(s, t)$ понимается

\[
K R_{G}(x)=\left\{\begin{array}{c}
\min l(p): \forall i \leqslant l(x) G(p, i)=x_{i}, \\
\infty, \text { если нет такого } p .
\end{array}\right.
\]

Имеет место теорема: существует частично рекурсивная функция $G_{0}$ (называемая о п т и м а л ь н о й) такая, что для любой другой частично рекурсивной функции $G$ выполняется неравенство $K R_{G_{0}}(x) \leqslant K R_{G}(x)+C_{G}$, где $C_{G}$ - константа, не зависящая от $x$. Сложностью разрешения $K R(x)$ слова $x$ наз. сложность $K R_{G_{0}}(x)$ по нек-рой раз и навсегда фиксированной оптимальной частично рекурсивной функции $G_{0}$. Очевидно, $K R(x) \leqslant$ $\leqslant K(x)+C \quad$ и $K(x) \leqslant K R(x)+2 l(x)+C$. Используя $K R(x)$, можно для любого множества натуральных чисел $M$ определить сложность $K(M, n)$ для $n$-куска множества $M: K(M, n)=K R\left(\omega_{n}\right)$, где $\omega=x_{1} x_{2} \ldots$ $x_{i} \ldots-$ х а р ак т е р истическ а я после дов а т ельность множе с т в а $M\left(x_{i}=1\right.$, если $i \in M$, и $x_{i}=0$, если $\left.i \notin M\right)$.

Алгоритмич. проблемы обычно могут быть представлены в виде проблемы вхождения в нек-рое рекурсивно перечислимое множество $M$. Если мы фиксируем некрое $n$ и ограничиваемся рассмотрением проблемы вхождения в $M$ только для первых $n$ натуральных чисел, то мы получаем ог р а ни ч е н у ю а лгори тми ч е с к ую п р обле м у. Величина $K(M, n)$ интуитивно выражает количество информации, к-рую надо иметь, чтобы можно было решить данную ограниченную проблему. В частности, множество $M$ рекурсивно тогда и только тогда, когда $K(M, n)$ ограничено сверху нек-рой константой.

Из теоремы Мартина-Лёфа следует существование множеств, для к-рых $K(M, n) \sim n$. При этом оказывается, что максимально сложные множества уже существуют среди множеств, задаваемых арифметич. шредикатами с двумя кванторами. Однако в случае рекурсивно перечислимых множеств имеет место теорема: a) для любого рекурсивно перечислимого множества $M$ и любого $n$ справедливо неравенство $K(M, n) \leqslant$ $\leqslant \log _{2} n+C$, где $C$ не зависит от $n$; б) существует рекурсивно перечислимое множество $M$ такое, что для любого $n$ имеет место неравенство $K(M, n)>\log _{2} n$. В то же время существуют такие рекурсивно перечислимые множества $M$, что при ограничении времени вычислений произвольной общерекурсивной функцией $t$ имеет место оценка $K^{t}(M, n) \geqslant n / c_{t}$, где $c_{t}$ не зависит OT $n$.

В указанных терминах можно дать также характеристику универсальных по нек-рому типу сводимости множеств (см. Универсальное жножество): множество $M$ является слабо таблично универсальным тогда и только тогда, когда существует неограниченная общерекурсивная функция $f(n)$ такая, что для любого $n$ имеет место неравенство $K(M, n) \geqslant f(n)$.

При изучении сложности ограниченных алгоритмич. проблем иногда используются и другие меры сложности, как, напр., минимальная длина изображения нормального алгорифма, решающего данную ограниченную проблему. Но оказывается, что между введенными выше сложностями и аналогами этих сложностей, выраженными через минимальную длину изображения нормального алгорифма (или через минимальное число внутренних состояний машины Тьюринга), существуют асимптотически точные соотношения (см. Алгорит.иа сложность). При построении А. т. и. вводится еще понятие у сло вно й с ло ж о с т и с ло в а $x$ при известном $y$ по частично рекурсивной функции $G(s, t)$ :

$K_{G}(x \mid y)=\left\{\begin{array}{l}\min l(p): G(p, y)=x \\ \infty, \text { если нет такого }\end{array}\right.$

Для этого понятия тание имеет место вовании оптимальной функции $G_{0}$ терема о сущестность $K(x \mid y)$ слова $x$ при известном $y$ определяется как сложность $K_{G_{0}}(x \mid y)$ по нек-рой раз и навсегда фиксированной оптимальной функции $G_{0} ; K(x \mid y)$ интуитивно обозначает минимальное количество информации, к-рое необходимо добавить к информации, содержащейся в слове $y$, чтобы можно было восстановить слово $x$. Очевидно, $\quad K(x \mid y) \leqslant K(x)+C$.

Следующим центральным понятием А. т. и. является понятие коли ч е ст в а инфо р м а ции в индивид у альн ом об ь е к т е $y$ относительно индивидуального объекта $x$ :

\[
I(y: x)=K(x)-K(x \mid y)
\]

Величину $I(y: x)$ наз. а л г о р и т м и ч е с к и м колич е с т во м ин фо р маци и в $y$ об $x$. Соответственно величины $K(x)$ и $K(x \mid y)$ наз. иногда а лгори тмиче ск о й энт ропией $x$ и а лгор и т мическ ой усл овно й әнт р о пи е й $x$ при заданном $y$. Формулы разложения энтропии $H(X$, $Y)=H(X)+H(Y \mid X)$ и коммутативности информации $I(X: Y)=I(Y: X)$ верны в алгоритмич. концепции лишь с точностью до членов порядка $O(\log H(X, Y))$ : $|K(x, y)-[K(x)+K(y \mid x)]| \leqslant O(\log K(x, y))$,\[
|I(x: y)-I(y: x)| \leqslant O(\log K(x, y)) .
\]

Между алгоритмич. и классич. определениями количества информации (точнее, между сложностью слова по Колмогорову и энтропией распределения частот по Шеннону) существует определенная связь (А. Н. Колмогоров): пусть задано число $r$ и пусть слово $x$ длины $i \cdot r$ состоит из $i$ слов длины $r$, причем $k$-е слово длины $r$ входит в $x$ с частотой $q_{k}\left(k=1,2, \ldots, 2^{r}\right)$, тогда

\[
\frac{K(x)}{i} \leq H+\alpha(i)
\]

где

\[
H=-\sum_{k=1}^{2^{r}} q_{k} \log _{2} q_{k}
\]

и $\alpha(i) \rightarrow 0$ при $i \rightarrow \infty$. В общем случае более тесную связь между энтропией и сложностью установить нельзя. Это объясняется тем, что энтропия приспособлена для изучения текстов, не имеющих других закономерностей, кроме частотных. Следовательно, для случайных последовательностей $\omega$ по мере, соответствующей независимым испытаниям, между рассматриваемыми величинами можно установить полную связь:

\[
\lim _{i \rightarrow \infty}\left(K\left(\omega_{i r}\right) / i\right)=H
\]

Аналогичный факт имеет место и для произвольного эргодического стационарного случайного процесса.

Juт.: [1] К о л м о г о р о в А. Н., «Проблемы Передачи информации», 1965, т. 1, вып. 1 , с. 3-11; [2] е г о ж е, "Проблемы передачи информации», 1969, т. 5, вып. 3, с. 3-7; [3] 1970, т. 25, вып. 6, с. 85-127; [4] Б а р з дин ь Я. М., "Докл, AH CCCP», 1968, т. 182, №, c. 1249-1252; [5] К А н о в и ч М. И., "Докл. АН СССР", 1970, т. 194, № 3, с. $500-503$.

АЛГОРИТМИЧЕСКАЯ ТЕОРИЯ МНОЖЕСТВ - См. Рекурсивная теория множеств.

АЛГОРИТМИЧЕСКИЙ ЯЗЫК, ф о р м а л ь ны й я зык П р ог р а м и и о в а ния, - формальный язык, предназначенный для описания вычислительных процессов, или, что то же, для записи алгоритмов, подлежащих выполнению на вычислительных машинах.