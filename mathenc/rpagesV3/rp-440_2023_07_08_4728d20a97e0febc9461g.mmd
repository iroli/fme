При обосновании Н. к. м. (по Гауссу) предполагается, что "убыток" от замены точного (неизвестного) значения нек-рой величины $\mu$ ее приближенным значением $X$, вычисленным по результатам наблюдений, пропорционален квадрату ошибки $(X-\mu)^{2} ; \quad$ оптимальной оценкой считается такая лишенная систематич. ошибки величина $X$, для к-рой среднее значение $\mathrm{E}(X-\mu)^{2}$ "убытка" минимально. Именно это требование и составляет основу Н. к. М. В общем случае отыскание оптимальной в смысле Н. к. м. оценки $X$ - задача весьма сложная, поэтому практически эту задачу сужают и в качестве $X$ выбирают линейную функцию от результатов наблюдений, лишенную систематич. ошибки, и такую, для к-рой среднее значение убытка минимально в классе всех линейных функций. Если случайные ошибки наблюдений подчиняются нормальному распределению и оцениваемая величина $\mu$ зависит от средних значений результатов наблюдений линейно (случай, весьма часто встречающийся в приложениях Н. к. м.), то решение этой задачи будет одновременно являться и решением общей задачи. При этом оптимальная оценка $X$ также подчиняется нормальному распределению со средним значением $\mu$ и, следовательно, плотность вероятности случайной величины $X$

\[
p(x ; \mu, \sigma)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left\{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right\}
\]

достигает максимума в точке $x=\mu$ (это свойство и выражает точное содержание распространенного в теории ошибок утверждения: «оценка $X$, вычисленная согласно Н. к. м., - наиболее вероятное значение неизвестного параметра $\mu »)$.

С луч а й одного неизвестного. Пусть для оценги значения неизвестной величины $\mu$ произведено $n$ независимых наблюдений, давших результаты $Y_{1}, Y_{2}, \ldots, Y_{n}$, т. e. $Y_{1}=\mu+\delta_{1}, Y_{2}=\mu+\delta_{2}, \ldots, Y_{n}=$ $=\mu+\delta_{n}$, где $\delta_{1}, \delta_{2}, \ldots, \delta_{n}-$ случайные ошибки (по
определению, принятому влассич. теории ошибок, с л уч а й ны е о ши б к и-независимые случайные величины с нулевым математич. ожиданием: $\mathrm{E} \delta_{i}=$ $=0$; если же $\mathrm{E} \delta_{i} \neq 0$, то $\mathrm{E} \delta_{i}$ наз. с и с т е м а т и ч ес к и м и о ши б к а и). Согласно Н. к. м. в качестве оценки величины $\mu$ принимают такое $X$, для к-рого будет наименьшей сумма квадратов (отсюда и само название метода):

\[
S(X)=\sum_{i=1}^{n} p_{i}\left(X-Y_{i}\right)^{2}
\]

где

\[
p_{i}=k / \sigma_{i}^{2} \text { и } \sigma_{i}^{2}=\mathrm{D}_{i}=\mathrm{E}_{i}^{2}
\]

(коәффициент $k>0$ можно выбирать произвольно). Величину $p_{i}$ наз. в е с о м, а $\sigma_{i}-$ к в а д р а т и ч ным о т к л о н н и е м измерения с номером $i$. В частности, если все измерения равноточны, то $\sigma_{1}=\sigma_{2}=\ldots=\sigma_{n}$, и в этом случае можно положить $p_{1}=p_{2}=\ldots=p_{n}=1$; если же каждое $Y_{i}$ - арифметич. среднее из $n_{i}$ равноточных измерений, то полагают $p_{i}=n_{i}$.

Сумма $S(X)$ будет наименьшей, если в качестве $X$ выбрать взвешенное среднее:

\[
\boldsymbol{X}=\overline{\boldsymbol{Y}}=\frac{1}{\boldsymbol{P}} \sum p_{i} Y_{i}, \text { где } \boldsymbol{P}=\sum p_{i}
\]

Оценка $\bar{Y}$ величины $\mu$ лишена систематич. ошибки, имеет вес $P$ и дисперсию $D \bar{Y}=k / P$. В частности, если все измерения равноточны, то $Y$ - арифметич. среднее результатов измерений:

\[
\bar{Y}=\frac{1}{n} \sum Y_{i} \text { и } \mathrm{D} \bar{Y}=\sigma^{2} / n .
\]

При нек-рых общих предположениях можно показать, что если количество наблюдений $n$ достаточно велико, то распределение оценки $\bar{Y}$ мало отличается от нормального с математич. ожиданием $\mu$ и дисперсией $k / P$. В этом случае абсолютная погрешность приближенного равенства $\mu \approx \bar{Y}$ меньше $t \sqrt{k / P}$ с вероятностью, близкой к значению интеграла

\[
I(t)=\frac{2}{\sqrt{2 \pi}} \int_{0}^{t} e^{-u^{2} / 2} d u
\]

(напр., $I(1,96)=0,950 ; I(2,58)=0,990 ; I(3,00)=0,997)$.

Если веса измерений $p_{i}$ заданы, а множитель $k$ до наблюдений остается неопределенным, то этот множитель и дисперсия оценки $\bar{Y}$ могут быть оценены по формулам:

\[
k \approx S(\bar{Y}) /(n-1)
\]

\[
\mathrm{D} \bar{Y}=k / P \approx s^{2}=S(\bar{Y}) /[(n-1) P]
\]

(обе оценки лишены систематич. ошибок).

В том практически важном случае, когда ошибки $\delta_{i}$ подчиняются нормальному распределению, можно найти точное значение вероятности, с к-рой абсолютная погрешность приближенного равенства $\mu \approx \bar{Y}$ окажется меньше $t s$ ( $t$ - произвольное положительное число):

\[
I_{n-1}(t)=C_{n-1} \int_{0}^{t}\left(1+\frac{v^{2}}{n-1}\right)^{-n / 2} d v,
\]

где постоянная $C_{n-1}$ выбрана таким образом, чтобы выполнялось условие $I_{n-1}(\infty)=1$ ( $($ тьюдента pacnpeделение с $n-1$ степенями свободы). При больших $n$ формулу (2) можно заменить формулой (1). Однако применение формулы (1) при небольших $n$ привело бы к грубым ошибкам. Так, напр., согласно (1) значению $I=$ $=0,99$ соответствует $t=2,58$; истинные значения $t$, определяемые при малых $n$ как решения соответствующих уравнений $I_{n-1}(t)=0,99$, приведены в таблице:

\begin{tabular}{c|c|c|c|c|c|c|c}
\hline$n \ldots$. & 2 & 3 & 4 & 5 & 10 & 20 & 30 \\
$t \ldots$ & 63,66 & 9,92 & 5,84 & 4,60 & 3,25 & 2,86 & 2,76 \\
\hline
\end{tabular}

П р и м е р. Для определения массы нек-рого тела произведено 10 независимых равноточных взвешиваний, давших результаты $Y_{i}$ (в 2$)$ :

\begin{tabular}{cc|c|c|c|c|c|c}
\hline $\boldsymbol{Y}_{i}$ & $\ldots$ & 18,41 & 18,42 & 18,43 & 18,44 & 18,45 & 18,46 \\
$n_{i}$ & $\ldots$ & 1 & 3 & 3 & 1 & 1 & 1 \\
\hline
\end{tabular}

(здесь $n_{i}$ - число случаев, в к-рых наблюдалась масса $\left.Y_{i} ; n=\Sigma n_{i}=10\right)$. Так как все взвешивания равноточные, то следует положить $p_{i}=n_{i}$ и в качестве оценки для неизвестного веса $\mu$ выбрать величину $Y=\Sigma n_{i} Y_{i} / \Sigma n_{i}=$ $=18,431$. Задавая, напр., $I_{9}=0,95$, по таблицам распределения Стьюдента с девятью степенями свободы можно найти, что $t=2,262$, и поэтому в качестве предельной абсолютной погрешности приближенного равенства $\mu \approx 18,431$ следует принять величину

\[
t s=t \sqrt{\sum n_{i}\left(Y_{i}-\bar{Y}\right)^{2} / 90}=2,262 \cdot 0,0048=0,011 .
\]

Таким образом, $18,420<\mu<18,422$.

С л у ч а й нескольких неиз ве с т ны (ли н е й ны е с в я зи). Пусть $n$ результатов измерений $Y_{1}, Y_{2}, \ldots, Y_{n}$ связаны с $m$ неизвестными величинами $x_{1}, x_{2}, \ldots, x_{m}(m<n)$ независимыми линейными соотношениями

\[
Y_{i}=\sum_{j=1}^{m} a_{i j} x_{j}+\delta_{i}, i=1,2, \ldots, n,
\]

где $a_{i j}$ - известные коәфффициенты, а $\delta_{i}$ - независимые случайные ошибки измерений. Требуется оценить неиз-