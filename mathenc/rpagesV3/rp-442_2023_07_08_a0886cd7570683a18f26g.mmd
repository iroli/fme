где

\[
\begin{gathered}
{\left[a_{1} a_{1}\right]=10,\left[a_{2} a_{2}\right]=\sum\left(t_{i}-\bar{t}\right)^{2}=1569,} \\
{\left[Y a_{1}\right]=\sum Y_{i}=-3,5,} \\
{\left[Y a_{2}\right]=\sum Y_{i}\left(t_{i}-\bar{t}\right)=-8,225 .}
\end{gathered}
\]

Дисперсии компонент решения әтой системы суть

\[
\mathrm{D} X_{1}=\frac{k}{\left[a_{1} a_{1}\right]}=\frac{k}{10} \quad \text { и } \quad \mathrm{D} X_{2}=\frac{k}{\left[a_{2} a_{2}\right]}=\frac{k}{1569},
\]

где $k$ - неизвестная дисперсия на единицу веса (в данном случае $k$ - дисперсия любой из величин $Y_{i}$ ). Так как в этом примере компоненты решения принимают значения $X_{1}=-0,35$, и $X_{2}=-0,00524$, то

\[
\begin{gathered}
k \approx S /(n-m)= \\
=\frac{1}{8} \sum_{i=1}^{10}\left[Y_{i}-X_{1}-X_{2}\left(t_{i}-\bar{t}\right)\right]^{2}=0,0427, \\
\mathrm{D} X_{1} \approx s_{1}^{2}=0,00427, \mathrm{D} X_{2} \approx s_{2}^{2}=0,000272, \\
s_{1}=0,065, \quad s_{2}=0,00522 .
\end{gathered}
\]

Если случайные ошибки наблюдений подчиняются нормальному распределению, то отношения $\left|X_{j}-x_{j}\right| / s_{j}$, $j=1,2$, распределены по закону Стьюдента. В частности, если результаты наблюдений лишены систематич. ошибок, то $x_{1}=x_{1}=0$, и, значит, закону Стьюдента должны подчиняться отношения $\left|X_{1}\right| ! s_{1}$ и $\left|X_{2}\right| / s_{2}$. С помощью таблиц распределения Стьюдента с $n-m=8$ степенями свободы можно убедиться, что если действительно $x_{1}=x_{2}=0$, то с вероятностью 0,999 каждое из этих отношений не должно превосхоцить 5,04 и с вероятностью 0,95 не должно превосходить 2,31 . В данном случае $\left|X_{1}\right| / s_{1}=5,38>5,04$, поэтому гипотезу отсутствия систематич. ошибок целесообразно отвергнуть; в то же время следует признать, что гипотеза об отсутствии методич. ошибки $\left(x_{2}=0\right)$ не противоречит результатам наблюдений, т. к. $\left|X_{2}\right| / s_{2}=1,004<2,31$. Таким образом, можно заключить, что для определения $t$ по результату наблюдения $T$ целесообразно пользоваться приближенной формулой $t=T+0,35$.

С луч а й не ско льких не извест ны х (н е ли н е й ны е с в я зи). Пусть $n$ результатов измерений $Y_{i}$ связаны с $m$ неизвестными $x_{j}(m<n)$ функциональной зависимостью $Y_{i}=f_{i}\left(x_{1}, x_{2}, \ldots, x_{m}\right)+\delta_{i}$; $i=1,2, \ldots, n$, где $\delta_{i}-$ независимые случайные ошибки, а функции $f_{i}$ (в общем случае нелинейные) дифференцируемы. Согласно Н. к. м. в качестве оценок для $x_{j}$ принимают такие величины $X_{j}$, для к-рых сумма квадратов

\[
S=\sum_{i=1}^{n} p_{i}\left[Y_{i}-f_{i}\left(X_{1}, X_{2}, \ldots, X_{m}\right)\right]^{2}
\]

будет наименьшей. Так как функции $f_{i}$ нелинейные, то решение нормальных уравнений $\partial S / \partial X_{j}=0$ в этом случае может представлять значительные трудности. Иногда нелинейные связи каким-либо преобразованием могут быть приведены к линейным.

Напр., при намагничивании железа напряженность магнитного поля $H$ связана с магнитной индукцией $B$ эмпирич. формулой $B=H /\left(x_{1}+H x_{2}\right.$ ) (коәффициенты $x_{1}$ и $x_{2}$ определяются по измеренным значениям $B_{i}$ при заданных $H_{i}$ ). Индукция $B$ - нелинейная функция от $x_{1}$ и $x_{2}$. Однако обратная величина индукции зависит от $x_{1}$ и $x_{2}$ линейно. Применение Н. к. м. к исходному и преобразованному равенствам дает, вообще говоря, различные оценки для неизвестных $x_{1}$ и $x_{2}$, но если дисперсия случайных ошибок измерения индукции значительно меньше измеряемых величин $B_{i}$, то $\mathrm{D}(1 / B) \approx$ $\approx B^{-4}$. Поэтому величинам $1 / B_{i}$ следует приписать веса $\left(1 / B_{i}\right)^{4}$; естественно ожидать, что при этих условиях различие оценок в нелинейном и линейном случаях будет практически несущественным. В тех случаях, когда не удается тождественными преобразованиями заменить нелинейные уравнения линейными, пользуются другим способом линеаризации. Из заданных $n$ уравнений отбирают какие-либо $m$ уравнений, решение к-рых $X_{1}^{0}, X_{2}^{0}, \ldots, X_{m}^{0}$ принимают за нулевое приближение для неизвестных $x_{j}$. Если положить $\xi_{j}=X_{j}-X_{j}^{0}$, то систему условных уравнений можно записать в виде:

$Y_{i}=f_{i}\left(X_{1}^{0}+\xi_{1}, X_{2}^{0}+\xi_{2}, \ldots, X_{m}^{0}+\xi_{m}\right), i=1,2, \ldots, n$. Разлагая правые части в ряд по степеням $\xi_{j}$ и ограничиваясь линейными членами, получают

\[
Y_{i}-\left(f_{i}\right)_{0}=\sum_{j=1}^{m}\left(\frac{\partial f_{i}}{\partial x_{j}}\right)_{0} \xi_{j}, i=1,2, \ldots, n,
\]

где $\left(f_{i}\right)_{0}$ и $\left(\frac{\partial f_{i}}{\partial x_{j}}\right)_{0}$ - значение функции $f_{i}$ и ее производных по $x_{j}$ при $x_{1}=X_{1}^{0}, x_{2}=X_{2}^{0}, \ldots, x_{m}=X_{m}^{0}$. Эта система уравнений линейна, и поэтому для оценки неизвестных $\xi_{j}$ легко может быть применен Н. к. м. Оценив $\xi_{j}$, получают первое приближение для неизвестных $X_{j}^{1}=X_{j}^{0}+\xi_{j}$. Величины $X_{i}^{1}$ берут за исходное приближение, и всю операцию повторяют, пока с заданной точностью не совпадут два последовательных приближения. Если дисперсии ошибок $\delta_{i}$ уменьшаются, то процесс сходится.

Очень часто при малых $D \delta_{i}$ оказывается вполне достаточным уже первое приближение: не имеет смысла требовать нахождения $X_{j}$ с точностью, значительно превышающей $\sqrt{\overline{\mathrm{D} X_{j}}}$.

Во многих практически важных случаях (и в частности, при огенке сложных нелинейных связей) количество неизвестных параметров бывает весьма большим, и поэтому реализация Н. к. м. оказывается әффективной лишь при использовании современной вычислительной техники.

Лит.: [1] М а р к о в А. А., Исчисление вероятностей, 4 изд., М., 1924; [2] К о л м о г о р о в А. Н., "Успехи матем. наукл, 1946 , т. 1 , в. 1 , с. $57-70$; [3] Л и н ни к Ю. В., Метод наименыших квадратов и основы математико-статистической теории обработии наблюдений, 2 изд., М., $1962 ;[4]$ Н а лілизе вещества, M., 1960 ; [5] He $1 \mathrm{~m}$ e r t F. R., Die Ausgleichungsrechnung nach der Methode der kleinsten Quadrate, 3 Aufl., НАЙЕНЫІІМХ РЕАКЦИИ ПРИНЦИІ - следствие из Гаусса принципа, получаемое из последнего с помощью уравнений, выражающих второй закон Ньютона для точек несвободной системы (см. [1]). Согласно H. р. п. для действительного движения системы велі पина

\[
\sum_{v} \frac{R_{v}^{2}}{2 m_{v}}
\]

есть минимум в классе мыслимых по Гауссу движениі. Здесь $R_{v}$ - реакции связё̈, $m_{v}-$ массы гочек системы. Лит.: [1] Ч е т а е в Н. Г., Устойчивость движения. Работы по аналитической механике, М., $1962, \underset{B}{\text { с. }}$ В.

НАИСКОРЕЙІЕГО СПУСКА МЕТОД̆ - частный случай метода спуска, когда направление $g^{k}$, указывающее спуск, выбирается противоположным $\operatorname{grad} f\left(x^{k}\right)$. Формулы Н. с. м. имеют вид

\[
x^{k+1}=x^{k}-\alpha_{k} f^{\prime}\left(x^{k}\right), \quad k=0,1, \ldots,
\]

где параметры $\left\{\alpha_{k}\right\}$ выбираются из условия максимального убывания на каждом шаге функции $f(x)$. Если функция $f$ дважды непрерывно дифференцируема и матрица $f^{\prime \prime}$ ее вторых производных удовлетворяет при любых $x, y$ неравенству

\[
m\|y\|^{2} \leqslant\left(f^{\prime \prime}(x) y, y\right) \leqslant M\|y\|^{2}
\]

с константами $M \geqslant m>0$, то (см. [2], [4]) последовательность $\left\{x^{k}\right\}$ сходится к решению $x^{*}$ задачи минимизации