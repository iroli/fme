ми дисперсиями $\sigma_{1}^{2}=\mathrm{D} X_{1}$ и $\sigma_{2}^{2}=\mathrm{D} X_{2}$ определяется равенством

\[
\rho\left(X_{1}, X_{2}\right)=\frac{E\left(X_{1}-a_{1}\right)\left(X_{2}-a_{2}\right)}{\sigma_{1} \sigma_{2}} .
\]

К. к. для $X_{1}$ и $X_{2}$ совпадает с ковариацией для нормированных величин $\left(X_{1}-a_{1}\right) / \sigma_{1}$ и $\left(X_{2}-a_{2}\right) / \sigma_{2}$. К. к. симметричен относительно $X_{1}$ и $X_{2}$ и инвариантен относительно изменения начала отсчета и масштаба. При этом $-1 \leqslant \rho \leqslant 1$. Значение К. к. как одной из возможных мер взаимосвязи определяется следующими его свойствами: 1) если величины $X_{1}$ и $X_{2}$ независимы, то $\rho\left(X_{1}, X_{2}\right)=0$ (обратное утверждение в общем случае неверно), о величинах, для к-рых $\rho=0$, говорят, что они некоррелированы; 2) $|\rho|=1$ тогда и только тогда, когда величины связаны линейной функциональной зависимостью:

\[
X_{2}=\rho \frac{\sigma_{2}}{\sigma_{1}}\left(X_{1}-a_{1}\right)+a_{2} .
\]

Трудность интерпретации $\rho$ как меры взаимозависимости заключается в том, что равенство $\rho=0$ может иметь место как для независимых, так и для зависимых случайных величин, в общем случае для независимости необходимо и достаточно равенство нулю их максимального коэффичиента коррелячии. Таким образом, К. к. не исчерпывает все виды связи между случайными величинами и является лишь мерой линейной зависимости. При әтом степень линейной зависимости характеризуется следующим образом: величина

\[
\hat{X}_{2}=\rho \frac{\sigma_{2}}{\sigma_{1}}\left(X_{1}-a_{1}\right)+a_{2}
\]

дает линейное представление $X_{2}$ по $X_{1}$, наилучшее в том смысле, что

\[
\mathrm{E}\left(X_{2}-\hat{X}_{2}\right)^{2}=\min _{c_{1}, c_{2}} \mathrm{E}\left(X_{2}-c_{1} X_{1}-c_{2}\right)^{2},
\]

см. также Регрессия. Характеристиками корреляции между несколькими случайными величинами служат частный коэфбичиент коррелячии и множественный коә бфициент корреляции. О способах проверки гипотез независимости и исследования корреляции с помощью К. К. см. Коррелячия. А. В. Прохоров.

КОРРЕЛЯЦИОННАЯ МАТРИЦА - матрица коэффициентов корреляции нескольких случайных величин. Если $X_{1}, \ldots, X_{n}-$ случайные величины с ненулевыми дисперсиями $\sigma_{1}^{2}, \ldots, \sigma_{n}^{2}$, то элементы $\rho_{i j}$ при $i \neq j$ равны корреляции коәфбичиентам $\rho\left(X_{i}, X_{j}\right)$, а при $i=j$ равны 1. Свойства К. М. $\mathbf{P}$ определяются свойствами ковариационной матрицы $\Sigma$ в силу соотношения: $\Sigma=$ $=B \mathbf{P} B$, где $B$ - диагональная матрица с диагональны-

![](https://cdn.mathpix.com/cropped/2023_07_08_85a60a9582db530ea80ag-1.jpg?height=60&width=828&top_left_y=1662&top_left_x=122)
те льь о о г о с л у ч а йн о г о п роц е с с а $\{X(t)$; $t \in T\}$ - функция аргументов $t, s \in T$, определяемая равенством

\[
B(t, s)=\mathrm{E}[\boldsymbol{X}(t)-\mathrm{E} X(t)][\boldsymbol{X}(s)-\mathrm{E} X(s)] .
\]

Для того чтобы К. ф. была определена, следует предположить, что процесс $X(t)$ при всех $t \in T$ имеет конечный второй момент $Е X(t)^{2}$. Параметр $t$ пробегает здесь некоторое подмножество $\boldsymbol{T}$ действительной прямой и обычно интерпретируется как "время", однако совершенно аналогично определяется $\kappa$. ф. случайной функции, заданной на множестве произвольной природы, в частности К. ф. случайного поля, когда $T$ - подмножество конечномерного пространства. Если $\boldsymbol{X}(t)=\left[X_{1}(t), \ldots\right.$, $\left.X_{n}(t)\right]$ - многомерный случайный процесс (случайная функция), то его К. ф. наз. матричнозначная функция

\[
B(t, s)=\left\|B_{i j}(t, s)\right\|_{i, j=1}^{n},
\]

где

\[
B_{i j}(t, s)=\mathrm{E}\left[X_{i}(t)-\mathrm{E} X_{i}(t)\right]\left[X_{j}(s)-\mathrm{E} X_{j}(s)\right]
\]

—взаимная корреляционная функц и я процессов $X_{i}(t), X_{j}(t)$.

К. Ф. является важной характеристикой случайного процесса. Если $\boldsymbol{X}(t)$ - аауссовский процесс, то его К. ф. $B(t, s)$ и среднее значение $\operatorname{EX}(t)$ (т. е. первые и вторые моменты) однозначно определяют конечномерные распределения, а значит и процесс в целом. В общем случае первых двух моментов заведомо недостаточно для полного описания случайного процесса. Напр., одинаковую $K$. ф. $B(t, s)=e^{-a|t-s|}$ имеют гауссовский марковский стационарный процесс, траектории к-рого непрерывны, и так наз. т е л е г р а ф н ы й с и г н а лточечный марковский стационарный процесс, принимающий два значения \pm 1 . Однако К. ф. определяет ряд важных свойств процесса - так наз. с в о й с т в а Б т о р о г п п р я д к а (т. е. выражающиеся в терминах вторых моментов). В силу этого, а также благодаря своей относительной простоте, корреляционные методы широко используются как в теории случайных процессов, так и в ее статистич. приложениях (см. Коррелограмма).

Скорость и характер убывания корреляций при $|t-s| \rightarrow \infty$ дают представление об эргодических свойствах процесса. Условия на скорость убывания корреляций в той или иной форме присутствуют в предельных теоремах для случайных процессов. Локальные свойства 2-го порядка, такие как среднеквадратичныө непрерывность, дифференцируемость, дают полезную, хотя и весьма грубую характеристику локального поведения процесса. Исследование свойств траекторий в терминах К. ф. с большой полнотой проведено в гауссовском случае (см. Выборочная функция). Одним из наиболее завершенных разделов теории случайных процессов является теория линейной әкстраполяции и фильтрации, позволяющая находить оптимальные линейные алгоритмы прогноза и аппроксимации случайных процессов, основываясь на знании К. ф.

Характеристическим свойством К. ф. является положительная определенность:

\[
\sum_{i, j=1}^{n} c_{i} \bar{c}_{j} B\left(t_{i}, t_{j}\right) \geqslant 0
\]

для любого $n$, любых комплексных $c_{1}, \ldots, c_{n}$ и любых $t_{1}, \ldots, t_{n} \in T$. В наиболее важном случае стационарного в широком смысле процесса $B(t, s)$ зависит от разности аргументов: $B(t, s)=R(t-s)$. Условие положительной определенности принимает тогда вид

\[
\sum_{i, j=1}^{n} c_{i} \bar{c}_{j} R\left(t_{i}-t_{j}\right) \geqslant 0 .
\]

Если $R(t)$ дополнительно непрерывна при $t=0$ (что соответствует среднеквадратичной непрерывности процесса $X(t))$, то

\[
R(t)=\int e^{i t \lambda} F(d \lambda),
\]

где $F(d \lambda)$ - положительная конечная мера; здесь $\lambda$ пробегает всю действительную прямую, если $T=$ $=(-\infty, \infty)$ (случай «непрерывного времени»), или отрезок $[-\pi, \pi]$, если $T=\{\ldots,-1,0,1, \ldots\}$ (случай «дискретного времени»). Мера $F(d \lambda)$ наз. с п е к тр а ғ ь н о й м е р о й случайного процесса. Таким образом, корреляционные и спектральные свойства стационарного случайного процесса оказываются тесно связанными; напр., скорость убывания корреляций при $t \rightarrow \infty$ соответствует степени гладкости спектральной плотности $f(\lambda)=F(d \lambda) / d \lambda$ и т. п.

В статистической механике $К$. ф. наз. также совместная плотность вероятности $\rho\left(x_{1}, \ldots, x_{m}\right)$ нахождения $m$ различных частиц рассматриваемой системы в точках $x_{1}, \ldots, x_{m} ;$ совокупность этих функций однозначно определяет соответствующее точечное случайное поле.