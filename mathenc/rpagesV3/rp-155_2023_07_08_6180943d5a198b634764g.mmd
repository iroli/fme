задачи (3), (4) с различными $h$ и последующего выделения главной части погрешности вида $h v[z]_{h}$, где $\gamma-$ известный порядок малости погрешности. Так, если справедливо асимптотич. соотношение

To

\[
\left\|[u]_{h}-u_{h}-h^{\nu}[z]_{h}\right\|_{U_{h}}=o\left(h^{\nu}\right)
\]

\[
[z]_{h} \sim\left(u_{h}-u_{H}\right) /\left(H^{\nu}-h^{\nu}\right)
\]

Иногда удается получить уравнения для функции $z$, содержащие производные от точного решения $u$. Тогда можно решать их численно на более грубой сетке после решения исходной задачи и по получении главной части погрешности добавить ее к приближенному решению $u_{h}$ и тем самым уточнить его.

В некоторых случаях при специальном выборе координатных функций вариационного или проекционного метода решения задачи (1), (2) получаются уравнения вида (3), (4), обеспечивающие сходимость не только к классическому, но и к обобщенному решению. Этот способ построения аппроксимаций, наз. иногда м ет о д о м о не чны х ә ле мен то в, предоставляет большую свободу в выборе сетки. Возможность целесообразно располагать узлы позволяет достигать требуемой точности при меньшем числе узлов сетки.

Лum.: [1] Б Б б у ушк а И., В и т а с е к Э., П р аr e p M., Численные пропессы решения дифференциальных уравнений, пер. с англ., М., 1969; [2] Б а х в а Ј о в Н. С., Кіонспекты по курсу “Основы вычислительной математики», ч. 4, М., 1968; [3] е г о же е, Численные методы, 2 изд., М., 1975; [4] В а p r a Р., Функциональный анализ и теория аппрокси-
мации в численном анализе, пер. с англ., М., 1974; [5] Г а в у-

![](https://cdn.mathpix.com/cropped/2023_07_08_6180943d5a198b634764g-1.jpg?height=40&width=828&top_left_y=1099&top_left_x=122)
р и н М. К., Ј. Кции по методам вычислений, М., 1971; [6] 2 изд., М., 1977; [7]' Д ь я к о н о в Е. Г., Разностные методы решения краевых задач, в. $1-2$, М., $1971-72$; [8] К а н т о р oв и ч Л. В., А К и ло о в $\Gamma$. П., ной математики, 2 изд., М., 1980 ; [i0] М и х ли н С. $\Gamma$. л и ц к и й X. Л., Приближенные методы решения диффференциальных и интегральных уравнений, М., 1965; [11] Р и х тма йе p Р. Д., М М р т о н K., Разностные методы решения краевых задач, [пер. с англ.], М., 1972; [12] Р я б е н ь к и й В. С., Фи ли пп о в А. Ф., Об устойчивости разностных уравнений, М., 1956; [13] С а м а р с к и й А. А., Теория разносте в Е. С., Методы решения сеточных иравнений, М.

А. Ф. Шапкин.

ЛИНЕИНАЯ НЕЗАВИСИМОСТЬ - одно иЗ основных понятий линейной алгебры. Пусть $V-$ векторное пространство над полем $k$; векторы $a_{1}, \ldots, a_{n}$ наз. ли не й но не

\[
k_{1} a_{1}+\ldots+k_{n} a_{n} \neq 0
\]

для любого набора $k_{i} \in K$, кроме $k_{1}=\ldots=k_{n}=0$. В противном случае векторы $a_{1}, \ldots, a_{n}$ наз. Јі и н е йн о з а в и с и м ы м и. Векторы $a_{1}, \ldots, a_{n}$ линейно зависимы в том и только в том случае, когда по крайней мере один из них является линейной комбинацией остальных. Бесконечное подмножество векторов из $V$ наз. линейно зависимым, если линейно зависимо его нек-рое конечное подмножество, и линейно независимым, если любое его конечное подмножество линейно независимо. Число элементов (мощность) максимального линейно независимого подмножества пространства не зависит от выбора этого подмножества и наз. р а нгом, или ра $о$ ме р ностью, пространства, а само это подмножество - базисом (базой).

В частном случае, когда векторы $a_{1}, \ldots, a_{n}-$ элементы нек-рого числового поля $K$, a $k$ - подполе в $K$, возникает понятие л и н е й н о й н е з а в и с и м ос т и ч и с е л. Л. н. чисел над полем рациональных чисел $Q$ можно рассматривать также, как обобщение понятия иррациональности. Так, числа $\alpha$ и 1 линейно независимы тогда и только тогда, когда $\alpha$ иррационально. Понятие линейной зависимости и независимости элементов вводится также в абелевых группах и модулях.

Линейная зависимость - частный случай более широкого понятия - абстрактного отношения зависи$\begin{array}{ll}\text { мости на множестве. } & \text { O. А. Иванова. }\end{array}$

ЛИНЕИНАЯ ОБОЛОЧКА - пересечение $M$ всех подпространств, содержащих множество $A$ векторного пространства $E$. При этом $M$ наз. также п о д П р ос т р а н с т в о м, порожденным $A$. M. И. Войцеховский.

ЛИНЕИНАЯ ОЦЕНКА - линейная функция от наблюдаемых случайных величин, используемая (при подстановке в нее конкретных значений наблюденных величин) в качестве приближенного значения (оценки) неизвестного параметра анализируемой стохастич. схемы (см. Оуенка статистическая). Специальное выделение класса Л. о. оправдано следующими обстоятельствами. Л. о. легче поддаются статистич. анализу, в частности исследованию на состоятельность, несмещенность, эффективность, построению соответствующих доверительных интервалов и т. П. В то же время в достаточно широком диапазоне случаев поиск «наилучших» (в определенном смысле) оценок не выводит за пределы класса Л. о. Так, напр., статистич. анализ линейной регрессионной модели (см. Линейная регрессия) вида

\[
\boldsymbol{Y}=\boldsymbol{X} \boldsymbol{\Theta}+\boldsymbol{\varepsilon}
\]

дает в качестве наилучшей (в смысле метода наименьших квадратов) оценки параметров $\boldsymbol{Q}$ оценку

\[
\widehat{\boldsymbol{\Theta}}=\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{Y},
\]

к-рая является линейной относительно наблюденных значений исследуемой случайной величины $\boldsymbol{Y}$. Здесь $Y$ есть $n$-мерный вектор-столбец наблюденных значений $y_{i}, i=1, \ldots ., n$ исследуемого результирующего признака (случайной величины), $\boldsymbol{X}$ - матрица размера $n \times p$ (ранга $p$ ) наблюденных значений $x_{i}^{(k)}, i=1$, ..., $n, k=1, \ldots, p, p$ неслучайных факторов-аргументов, от к-рых зависит результирующий признак $\boldsymbol{Y}$, $\odot$ есть $p$-мерный вектор-столбец неизвестных параметров $Q_{k}, k=1, \ldots, p$, и $\varepsilon$ есть $n$-мерный случайный вектор-столбец остаточных компонент, удовлетворяющий условия $\mathrm{E} \varepsilon=0, \mathrm{E}\left(\varepsilon \varepsilon^{\prime}\right)=\sigma^{2} I(I-$ единичная матрица).

Лum.: [1] К р а м е p Г., Математические методы статистики, пер. с англ., 2 изд., М.,, 1975; [2] Р а о С. Р., Јинейные статистические методы и их применения, пер. с англ., М., 1968; [3] 3 a к с ШI., Теория статистических выводов, пер. с англ., англ., м. 1963 . е ф ф е Г., Дисперсионный анализ, пер. с

ЛӤЕИНАЯ РЕГРЕССИЯ 0 д н о й с л у ч а Йн о й п е р е м е н н о й $\boldsymbol{Y}=\left(Y^{(1)}, \ldots, \quad \boldsymbol{Y}^{(m)}\right)^{\prime}$ по другой $\boldsymbol{X}=\left(X^{(1)}, \ldots, X^{(p)}\right)^{\prime}-$ линейная по $\boldsymbol{x}$ $m$-мерная векторная форма, описывающая зависимость условного математич. ожидания (при условии $\boldsymbol{X}=\boldsymbol{x}$ ) случайного вектора $\boldsymbol{Y}$ от значений $\boldsymbol{x}=\left(x^{(1)}, \ldots\right.$, $\left.x^{(p)}\right)^{\prime}$. Соответствующие уравнения

\[
\begin{gathered}
y^{(k)}(x, b)=\mathrm{E}\left(Y^{(k)} \mid \boldsymbol{X}=\boldsymbol{x}\right)=\sum_{j=0}^{p} b_{k j} x^{(j)}, \\
x^{(0)} \equiv 1, k=1,2, \ldots, m,
\end{gathered}
\]

наз. ур а в ени я ми лине й но й ре г р е сс и и $\boldsymbol{Y}$ по $\boldsymbol{X}$, а параметры $b_{k j}-$ к о ә фф и ц и е нта м р е гре с си и (см. также Регрессия).

В приложениях допускается интерпретация переменной $\boldsymbol{X}$ как наблюдаемого параметра (не обязательно случайного), от к-рого зависит математич. ожидание исследуемого результируюшего показателя $\boldsymbol{Y}(\boldsymbol{X})$. Кроме того, часто под J. p. $\boldsymbol{Y}^{(k)}$ по $\boldsymbol{X}$ понимают «наилучшую" (в определенном смысле) линейную аппроксимацию $\boldsymbol{Y}^{(k)}$ посредством величин $\boldsymbol{X}$ или результат наилучшего