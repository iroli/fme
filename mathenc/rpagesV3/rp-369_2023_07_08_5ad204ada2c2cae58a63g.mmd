областей и т. д.). Так, пусть исследуемый многомерный прıзнак $\boldsymbol{x}$ интерпретируется как векторная случайная величина, подчиненная $p$-мерному нормальному распределению $N_{p}(\boldsymbol{\mu}, \boldsymbol{V})$, и расчленен на два подвекторастолбца $x^{(1)}$ и $x^{(2)}$ размерности $q$ и $p-q$ соответственно. Әто определяет и соответствующее расчленение вектора математич. ожиданії̈ $\boldsymbol{\mu}$, теоретической и выборочной ковариационных матриц $\boldsymbol{V}$ и $\hat{\boldsymbol{V}}$, а именно:

\[
\boldsymbol{\mu}=\left(\begin{array}{l}
\mu^{(1)} \\
\mu^{(2)}
\end{array}\right), \quad \boldsymbol{V}=\left(\begin{array}{ll}
V_{11} & V_{12} \\
V_{21} & V_{22}
\end{array}\right) \quad \text { и } \quad \hat{\boldsymbol{V}}=\left(\begin{array}{ll}
\widehat{V}_{11} & \hat{V}_{12} \\
\widehat{\boldsymbol{V}}_{21} & \hat{\boldsymbol{V}}_{22}
\end{array}\right)
\]

Тогда (см. [1], [2]) условное распределение подвектора $\boldsymbol{x}^{(1)}$ (при условии, что второй подвектор принял фиксированное значение $\left.x^{(2)}\right)$ будет также нормальным $N_{q}\left(\boldsymbol{\mu}^{(1)}+\boldsymbol{B}\left(\boldsymbol{x}^{(2)}-\boldsymbol{\mu}^{(2)}\right), \Sigma\right)$. При этом оценками максимального правдоподобия $\hat{\boldsymbol{B}}$ и $\hat{\Sigma}$ для матриц регрессионных коэфффициентов $\boldsymbol{B}$ и ковариаций $\Sigma$ этой классической многомерной модели множественной регрессии

\[
E\left(x^{(1)} \mid x^{(2)}\right)=\mu^{(1)}+B\left(x^{(2)}-\mu^{(2)}\right)
\]

будут взаимно независимые статистики соответственно

\[
\hat{B}=\hat{\boldsymbol{V}}_{12} \hat{\boldsymbol{V}}_{22}^{-1} \text { и } \hat{\Sigma}=\hat{\boldsymbol{V}}_{11}-\hat{\boldsymbol{V}}_{12} \hat{\boldsymbol{V}}_{22}^{-1} \hat{\boldsymbol{V}}_{21} ;
\]

здесь распределение оценки $\hat{\boldsymbol{B}}$ подчинено нормальному закону $N_{q(p-q)}\left(\boldsymbol{B}, \boldsymbol{V}_{\boldsymbol{B}}\right)$, а оценки $n \hat{\Sigma}$-закону Уиларта с параметрами $\Sigma$ и $n-(p-q)$ (әлементы ковариационной матрицы $V_{B}$ выражаются в терминах элементов матрицы $\boldsymbol{V})$.

Основные результаты по построению оценок параметров и исследованию их свойств в моделях факторного анализа, главных компонент и канонич. корреляций относятся к анализу вероятностно-статистич. свойств собственных (характеристических) значений и векторов различных выборочных ковариационных матриц.

В схемах, не укладывающихся в рамки классич. нормальной моделіи и тем более в рамки какой-либо вероятностной модели, основные результаты относятся к построению алгоритмов (и исследованию піх свойств) вычисления оценок параметров, наилучших с точки зрения нек-рого экзогенно заданного функционала качества (или адекватности) модели.

2) Построение статистич. критериев для проверки различных гипотез о структуре исследуемых взаимосвязей. В рамках многомерной нормальной модели (последовательности наблюдений вида (1) интерпретируются как случайные выборки из соответствующих многомерных нормальных генеральных совокупностей) построены, напр., статистич. критериі для проверкі следуюшцх гипотез.

I. Гипотезы $\boldsymbol{\mu}=\mu^{*}$ о равенстве вектора математич. ожиданий исследуемых показателей заданному конкретному вектору $\mu * ;$ проверяется с помощью $T^{2}$-статистики Хотеллинга с подстановкой в формулу (6) $\boldsymbol{\mu}=\mu^{*}$.

II. Гипотезы $\boldsymbol{\mu}^{(1)}=\boldsymbol{\mu}^{(2)}$ о равенстве векторов математІч. ожиданий в двух генеральных совокупностях (с одинаковыми, но неизвестными ковариационными матрицами), представленных двумя выборками; проверяется с помощью статистики $\tilde{T}^{2}$ (см. [7]).

III. Гипотезы $\boldsymbol{\mu}^{(1)}=\boldsymbol{\mu}^{(2)}=\ldots=\mu^{(k)}=\mu \quad$ o $\quad$ равенстве векторов математич. ожиданий в нескольких генеральных совокупностях (с одинаковыми, но неизвестными ковариационными матрицами), представленных своими выборками; проверяется с помощью статистики

\[
U_{p, k-1, n-k}=\frac{\left|\sum_{j=1}^{k} \sum_{i=1}^{n}\left(x_{\cdot i}^{(j)}-\hat{\mu}^{(j)}\right)\left(x_{\cdot i}^{(j)}-\hat{\mu}^{(j)}\right)^{\prime}\right|}{\left|\sum_{j=1}^{k} \sum_{i=1}^{n_{j}^{j}}\left(x_{\cdot i}^{(j)}-\hat{\mu}\right)\left(x_{\cdot i}^{(j)}-\hat{\mu}\right)^{\prime}\right|},
\]

в к-рой $x_{. i}^{(j)}$ есть $i$-е $p$-мерное наблюдение в выборке объема $n_{j}$, представляющей $j$-ю генеральную совокупность, а $\hat{\boldsymbol{\mu}}^{(j)}$ и $\hat{\boldsymbol{\mu}}$ - оценки вида (3), построенные соответственно отдельно по каждой из выборок и по объединенной выборке объема $n=n_{1}+\ldots+n_{k}$.

IV. Гипотезы $\boldsymbol{\mu}^{(1)}=\boldsymbol{\mu}^{(2)}=\ldots=\boldsymbol{\mu}^{(k)}=\boldsymbol{\mu}$ и $\boldsymbol{V}_{1}=\ldots=V_{k}=$ $=V$ об әквівалентности нескольких нормальных генеральных совокупностеї, представленных своими выборками $\left\{x_{\cdot i}^{(j)}\right\}_{i=1}^{n_{j}}, j=1,2, \ldots, k$; проверяется с помощью статистики

\[
\lambda=\frac{\Pi_{j=1}^{k}\left|n_{j} \hat{\boldsymbol{V}}_{j}\right|^{\left(n_{j}-1\right) / 2}}{\left|\sum_{j=1}^{k} \sum_{i=1}^{n_{j}}\left(x_{\cdot i}^{(j)}-\hat{\mu}\right)\left(x_{\cdot i}^{(j)}-\mu\right)^{\prime}\right|^{(n-k) / 2}}
\]

в к-рой $\hat{\boldsymbol{V}}_{j}$ - оценка вида (4), построенная отдельно по наблюдениям $j$-й выборки, $j=1,2, \ldots, k$.

V. Гипотезы о взаимної независимості подвекторовстолбцов $x^{(1)}, x^{(2)}, \ldots, x^{(m)}$ размерностей соответственно $p_{1}, p_{2}, \ldots, p_{m}$, на к-рыс расчленен исходный $p$-мерный вектор исследуемых показателей $\boldsymbol{x}, p_{1}+p_{2}+\ldots+$ $+p_{m}=p ;$ проверяется с помощью статистики

\[
\psi=\frac{|n \hat{\boldsymbol{V}}|}{\Pi_{i=1}^{m}\left|n_{i} \hat{\boldsymbol{V}}_{i}\right|},
\]

в к-рой $\hat{\boldsymbol{V}}$ и $\hat{\boldsymbol{V}}_{i}-$ выборочные ковариационные матрицы вида (4) для всего вектора $x$ и для сго подвектора $\boldsymbol{x}^{(i)}$ соответственно.

Многомерный статистический анализ геометрической структуры исследуемой совокупности многомерных наблюдений объединяет в себе понятия и результаты таких моделей и схем, как дискриминантный анализ, смеси вероятностных распределений, кластер-анализ и таксономия, многомерное шкалирование. Узловым во всех этих схемах является понятие расстояния (меры близости, меры сходства) между анализируемыми элементами. При этом анализируемыми могут быть как реальные объекты, на каждом из к-рых фиксируются значения показателей $\boldsymbol{x}$, - тогда геометрич. образом $i$-го обследованного обтекта будет точка $x_{. i}=\left(x_{1 i}, \ldots\right.$, , $\left.x_{p i}\right)^{\prime}$ в соответствующем $p$-мерном пространстве, так и сами показатели $x_{l .}, l=1,2, \ldots, p,-$ тогда геометрич. образом $l$-го показателя будет точка $x_{l}=\left(x_{l 1}, x_{l 2}, \ldots\right.$, $\left.x_{l n}\right)$ в соответствующем $n$-мерном пространстве.

Методы и результаты дискриминантного анализа (см. [1], [2], [7]) направлены на решение следующей задачи. Известно о существовании определенного числа $k \geqslant 2$ генеральных совокупностей и у исследователя имеется по одной выборке из каждой совокупности («обучающие выборки»). Требуется построить основанное на имеющихся обучающих выборках наилучшее в определенном смысле классифицирующее правило, позволяюгее приписать нек-рый новый әлемент (наблюдение $x$ ) к своей генеральной совокупности в ситуации, когда исследователю заранее не известно, к какой из совокупностей этот элемент принадлежит. Обычно под классифицирующим правилом понимается последовательность действий: по вычислению скалярной функции от исследуемых показателей, по значениям к-рой принимается решение об отнесении элемента к одному из классов (построеніе дискриминантной функции); по упорядочениг самих показателей по степени их информативности с точки зрения правильного отнесения элементов к классам; по вычислению соотвстствующих вероятностей ошибочной классификации.

Задача анализа смесей распределений вероятностей (см. [7]) чаще всего (но не всегда) возникает также в связи с исследовангем «геометрической структуры"