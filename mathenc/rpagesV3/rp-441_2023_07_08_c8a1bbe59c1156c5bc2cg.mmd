вестные величины $x_{j}$ (эту задачу можно рассматривать как обобщение предыдущей, в к-рой

\[
\mu=x_{1} \text { и } m=a_{i 1}=1 ; i=1,2, \ldots, n .
\]

Так как $\mathrm{E} \delta_{i}=0$, то средние значения результатов измерений $y_{i}=$ Е $Y_{i}$ связаны с неизвестными величинами $x_{1}, x_{2}, \ldots, x_{m}$ линейными уравнениями (линейные свя3и):

\[
y_{i}=\sum_{j=1}^{m} a_{i j} x_{j}, \quad i=1,2, \ldots, n .
\]

Следовательно, искомые величины $x_{i}$ представляют собой решение системы (4), уравнения к-рой предполагаются совместными. Точные значения измеряемых величин $y_{i}$ и случайные ошибки $\delta_{i}$ обычно неизвестны, поэтому вместо систем (3) и (4) принято записывать так наз. условные ур а вне ния

\[
Y_{i}=\sum_{j=1}^{m} a_{i j} x_{j}, \quad i=1,2, \ldots, n .
\]

Согласно Н. к. м. в качестве оценок для неизвестных $x_{j}$ применяют такие величины $X_{j}$, для к-рых сумма квадратов отклонений

\[
S=\sum_{i=1}^{n} p_{i}\left(Y_{i}-\sum_{j=1}^{m} a_{i j} X_{j}\right)^{2}
\]

будет наименьшей (как и в предыдущем случае, $p_{i}-$ вес измерения, $Y_{i}$ - величина, обратно пропорциональная дисперсии случайной отибки $\delta_{i}$ ). Условные уравнения, как правило, несовместны, т. е. при любых значениях $X_{j}$ разности

\[
\Delta_{i}=Y_{i}-\sum_{j=1}^{m} a_{i j} X_{j}, \quad i=1,2, \ldots, n,
\]

не могут, вообце говоря, все обратиться в нуль. Н. к. м. предписывает в качестве оценок выбрать такие значения $X_{j}$, к-рые минимизируют сумму $S$. В тех исключительных случаях, когда условные уравнения совместны и, значит, обладают решением, әто решение совпадает с оценками, полученными согласно Н. к. м.

Сумма квадратов $S$ представляет собой квадратичный многочлен относительно переменных $X_{f}$; этот многочлен достигает минимума при таких значениях $X_{1}, X_{2}, \ldots$, $X_{m}$, при к-рых обращаются в нуль все первые частные производные:

\[
\frac{\partial S}{\partial X_{j}}=-2 \sum_{i=1}^{n} p_{i} \Delta_{i}=0, \quad j=1,2, \ldots, m .
\]

Отсюда следует, что оценки $X_{j}$, полученные согласно Н. к. м., должны удовлетворять системе т. н. н о рма л ьны х ур а в н е и й, к-рая в обозначениях, предложенных К. Гауссом, имеет вид

$\sum_{j=1}^{m}\left[p a_{j} a_{l}\right] X_{j}=\left[p Y a_{l}\right], \quad l=1,2, \ldots, m$,

где

\[
\left[p a_{j} a_{l}\right]=\sum_{i=1}^{n} p_{i} a_{i j} a_{i l}
\]

и

\[
\left[p Y a_{l}\right]=\sum_{i=1}^{n} p_{i} Y_{i} a_{i l} .
\]

Оценки, получающиеся в результате решения системы нормальных уравнений, лишены систематич. ошибок $\left(\mathrm{E} X_{j}=x_{j}\right)$; дисперсии $\mathrm{D} \boldsymbol{X}_{j}$ величин $X_{j}$ равны $k d_{j j} / d$, где $d$ - определитель системы (5), а $d_{j j}$ - минор, соответствующий диагональному әлементу [pa $\left.a_{j} a_{j}\right]$ (иными словами, $d_{j j} / d$ - вес оценки $\left.X_{j}\right)$. Если множитель пропорциональности $k$ ( $k$ наз. д и с п е p с и е й н a е д ин и ц у в е с а) заранее неизвестен, то для его оценки, a также для оценки дисперсии $\mathrm{D} X_{j}$ служат формулы

\[
k \approx S /(n-m) \text { и } \mathrm{D} X_{j} \approx s_{j}^{2}=S d_{j j} / d(n-m)
\]

( $S$ - минимальное значение исходной суммы квадра- тов). При нек-рых общих предположениях можно показать, что если количество наблюдений $n$ достаточно велико, то абсолютная погрешность приближенного равенства $x_{j} \approx X_{j}$ меньше $t s_{j}$ с вероятностью, близкой к значению интеграла (1). Если случайные ошибки наблюдений $\delta_{i}$ подчиняются нормальному распределению, то все отношения $\left(X_{j}-x_{j}\right) / s_{j}$ распределены по закону Стьюдента с $n-m$ степенями свободы (точная оценка абсолютной погрешности приближенного равенства производится здесь с помощью интеграла (2) так же, как в случае одного неизвестного). Кроме того, минимальное значение суммы $S$ в вероятностном смысле не зависит от $X_{1}, X_{2}, \ldots, X_{m}$, и потому приближенные значения дисперсий оценок $\mathrm{D} X_{j} \approx s_{j}^{2}$ не зависят от самих оценок $\boldsymbol{X}_{\boldsymbol{j}}$

Один из наиболее типичных случаев применения Н. к. м. - «выравнивание» таких результатов наблюдений $\boldsymbol{Y}_{j}$, для к-рых в уравнениях (3) $a_{i j}=a_{j}\left(t_{i}\right)$, где $\boldsymbol{a}_{j}(t)$ - известные функции нек-рого параметра $t$ (если $t$ - время, то $t_{1}, t_{2}, \ldots$ - те моменты времени, в к-рые производились наблюдения). Особенно часто встречается в приложениях случай т. н. п а р а б о л и ч е с к о й и н т е р по л я ц и и, когда $a_{j}(t)-$ многочлены (напр., $\left.a_{1}(t)-1, \quad a_{2}(t)=t, \quad a_{3}(t)=t^{2}, \ldots\right) ; \quad$ если $t_{2}-t_{1}=t_{3}-t_{2}=$ $=\ldots=t_{n}-t_{n-1}$, а наблюдения равноточные, то для вычисления оценок $X_{j}$ можно воспользоваться таблицами ортогональных многочленов. Другой важный для приложений случай - т. н. г а р м о и и е с к а я и нте р п о л ц и я, когда в качестве $a_{j}(t)$ выбирают тригонометрич. функции (напр., $a_{j}(t)=\cos (j-1) t, j=1$, $2, \ldots, m)$.

П р и ме р. Для оценки точности одного из методов химич. анализа әтим методом определялась концентрация $\mathrm{CaO}$ в десяти әталонных пробах заранее известного состава. Результаты наблюдений указаны в таблице ( $i$ - номер әксперимента, $t_{i}-$ истинная концентрация $\mathrm{CaO}, T_{i}$ - концентрация $\mathrm{CaO}$, определенная в результате химич. анализа, $Y_{i}=T_{i}-t_{i}-$ ошибка химич. анализа):

\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
\hline 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
\hline 4 & 8 & 12,5 & 16 & 20 & 25 & 31 & 36 & 40 & 40 \\
\hline$-0,3$ & $-0,2$ & $-0,4$ & $-0,4$ & $-0,2$ & $-0,5$ & $+0,1$ & $-0,5$ & $-0,6$ & $-0,5$ \\
\hline
\end{tabular}

Если результаты химич. анализа не имеют систематич. ошибок, то $\mathrm{E} Y_{i}=0$. Если же такие ошибки имеются, то в первом приближении их можно представить в виде: $\mathrm{E} Y_{i}=\alpha+\beta t_{i}(\alpha$ наз. п о с т о я н н о й $о$ ши б-

![](https://cdn.mathpix.com/cropped/2023_07_08_c8a1bbe59c1156c5bc2cg-1.jpg?height=44&width=834&top_left_y=1727&top_left_x=987)
или, что то же самое,

\[
\mathrm{E} Y_{i}=(\alpha+\beta \bar{t})+\beta\left(t_{i}-\bar{t}\right)
\]

где

\[
\bar{t}=\frac{1}{10} \sum_{i=1}^{10} t_{i}=23,25 .
\]

Для отыскания оценок $\alpha$ и $\beta$ достаточно оценить величины $x_{1}=\alpha+\bar{\beta} \bar{t}$ и $x_{2}=\beta$. Условные уравнения в данном случае имеют вид

\[
Y_{i}=x_{1}+x_{2}\left(t_{i}-\bar{t}\right), \quad i=1,2, \ldots, 10,
\]

поэтому $a_{i 1}=1, a_{i 2}=t_{i}-\bar{t}$ (согласно предположению о равноточности наблюдений все $\left.p_{i}=1\right)$. Так как $\left[a_{1} a_{2}\right]=$ $=\left[a_{2} a_{1}\right]=\Sigma\left(t_{i}-\bar{t}\right)=0$, то система нормальных уравнений записывается особенно просто:

\[
\left[a_{1} a_{1}\right] X_{1}=\left[Y a_{1}\right] ;\left[a_{2} a_{2}\right] X_{2}=\left[Y a_{2}\right] \text {, }
\]