рассматриваемой совокупности. При этом понятие $r$-го однородного класса формализуется с помощью генеральной совокупности, описываемой нек-рым (как правило, унимодальным) законом распределения $\mathrm{P}\left(\boldsymbol{x} \mid \theta_{r}\right)$, так что распределение общей генеральной совокупности, ІІз к-рой извлечена выборка (1), описывается смесью распределений вида

\[
\mathrm{P}(\boldsymbol{x})=\sum_{r=1}^{k} \pi_{\boldsymbol{r}} \mathbf{P}\left(\boldsymbol{x} \mid \boldsymbol{\theta}_{r}\right),
\]

где $\pi_{r}-$ априорная вероятность (удельный вес әлементов) $r$-го гласса в общей генеральной совокупности. Задача состоит в «хорошем» статистич. огенивании (по выборге $\left.\left\{x_{i}\right\}_{1}^{n}\right)$ неизвестных параметров $\theta_{r}, \pi_{r}$, а иногда и $k$. Это, в частности, позволяет свесті задачу классиф্ккации элементов к схеме дискриминантного анализа, хотя в данном случае отсутствовали обучающие выборки.

Методы іІ результаты кластер-анализа (классификациі, таксономии, распознавании образов "без учителя", см. [2], [6], [7]) направлены на решение следующей задачіг. Геометрич. структура анализируемой совокупности элементов задана либо координатамі соответствуюици точек (т. е. матрицей $\left\|x_{i j}\right\|, i=1, \ldots, p ; j=1$, $\ldots, n)$, либо набором геометрич. характеристик их взаимного расположения, напр. матрицей попарных расстояний $\left\|\rho_{i j}\right\|_{i, j=1}^{n}$. Требуется разбить исследуемую совокупность әлементов на сравнительно небольшое (заранее известное или нет) число классов так, чтобы элементы одного класса находились на небольшом расстоянии друг от друга, в то время как разные классы былі бы по возможности достаточно взаимоудалены одін -от другого и не разбивались бы на столь же удаленные друг от друга части.

Задача многомерного шкалирования (см. [6]) относится к ситуации, когда исследуемая совокупность элементов задана с помощью матрицы попарных расстоянйі $\left\|\rho_{i}\right\|_{i, j=1}^{n}$ І заключается в приписывании каждому из элементов заданного числа $(p)$ координат таким образом, чтобы структура попарных взаимных расстояниї между әлементами, измеренных с помощью этих вспомогательных координат, в среднем наименее отличалісь бы от заданної. Следует заметить, что основные результаты І методы кластер-анализа іІ многомерного Ішкалирования развиваются обычно без каких-лібо допущений $\cap$ вероятностной природе исходных данных.

Прикладное назначение многомерного статистического анализа состопт в основном в обслуживаниіг следующих трех проблем.

Проблема статистического исследования зависимостей между анализируемыми показателями. П редполагая, что исследусмый набор статистически регистрируемых показателеӥі $\boldsymbol{x}$ разбит, исходя из содержательного смысла этих показателей и окончательных целеї исследования, на $q$-мерный подвектор $x^{(1)}$ предсказываемых (зависимых) переменных и $(p-q)$-мерный подвектор $\boldsymbol{x}^{(2)}$ предсказываюццх (независимых) переменных, можно сказать, что проблема состоит в определении на основаниіг выборки (1) такой $q$-мерной векторной функции $f\left(x^{(2)}\right)$ из класса допустимых решений $F$, к-рая давала бы наилучшую, в определенном смысле, аппроксимацию поведения подвектора показателей $x^{(1)}$. В зависимости от конкретного вида функционала качества аппроксимации и природы анализируемых показателей приходят к тем или иным схемам множественной регрессии, дисперсионного, ковариационного или конфлюентного аналпза.

Проблема классификации элементов (объектов или показателей) в о б щ е й (нестрогой) п о с т а н о в к заключается в том, чтобы всю анализируемую совокупность әлементов, статистически представленную в виде матрицы $\left\|x_{i j}\right\|, i=1, \ldots, p ; j=1, \ldots, n$, илІи матрицы $\left\|\rho_{i j}\right\|, i, j=1, \ldots, n$, разбить на сравнительно небольшое число однородных, в определенном смысле, групп [7]. В зависимости от природы априорной информации и конкретного вида функционала, задающего критерий качества классификации, приходят к тем или иным схемам дискриминантного анализа, кластер-анализа (таксономии, распознавания образов «без учителя»), расщепления смесей распределений.

Проблема снижения размерности исследуемого факторного пространства и отбора наиболее информативных показателей заключается в определении такого набора сравнительно небольшого числа $m \leqslant p$ показателей $\boldsymbol{z}=\left(z_{1}, z_{2}, \ldots, z_{m}\right)^{\prime}$, найденного в классе допустимых преобразований $Z(x)$ исходных показателей $\boldsymbol{x}=\left(x_{1}\right.$, $\left.x_{2}, \ldots, x_{p}\right)$, на к-ром достигается верхняя грань нек-рой экзогенно заданной меры информативности $m$-мерЕой системы признаков (см. [7]). Конкретизация функционала, задающего ме р у а в т о и н ф о р м а т и вн о с т и (т. е. нацеленное на максимальное сохранение информации, содержащейся в статистич. массиве (1) относительно самих исходных признаков), приводит, в частности, к различным схемам факторного аналиєа и главных.компонент, к методам экстремальной группг ровки признаков. Функционалы, задающие ме р у в н ш н е й и нфо р м а т и в н о с т и, т. е. нацеленные на извлечение из (1) максимальной информации относительно нек-рых других, не содержащихся непосредственно в $x$, показателей или явлений, приводят к различным методам отбора наиболее информативных показателей в схемах статистич. исследования зависимостей и дискриминантного анализа.

Основной математический инструментарий М. с. а. составляют специальные методы теории систем линейных уравнений и теории матриц (методы решения простой и обобщенной задачи о собственных значениях и векторах; простое обращение и псевдообращение матриц; процедуры диагонализации матриц и т. Д.) и некрые оптимизационные алгоритмы (методы покоординатного спуска, сопряженных градиентов, ветвей и границ, различные версии случайного поиска и стохастич. аппроксимации и т. Д.).

Лит.: [1] А н д е р с о н т., Введение в многомерный статистический анализ, пер. с англ., М., $1963 ;[2] \mathrm{K}$ е н д а л л М. Д ж., С т ь ю а p т A., Многомерный статистический анализ и вре"Bull. Int. Stat. Inst.", 1969, N $\mathrm{Nk}$ 3, p. 425-41; [ [ ] W i s h a r t J., "Biometrika", 1928 , v. $20 \mathrm{~A}$, p. 32-52; [5] H o t e 1 i i n g H., "Ann. Math.'Stat.", 1931 , v. 2, p. $360-78$; [6] K r u s k a l J. B., "Psychometrika", 1964, v. 29, p. 1-27; [7] A й в а з я н C. A., Бе ж а е в а З. И., С т а p о в е p о в О. В., Классификация

![](https://cdn.mathpix.com/cropped/2023_07_08_7d407fec31b112a4b8e8g-1.jpg?height=52&width=828&top_left_y=1593&top_left_x=992)
жений сферы в сферу. Более точно, $n$-мерным узлом коразмерности $q$ наз. ІІара $K=\left(S^{n+q}, k^{n}\right)$, состоянгая из ориентированной сферы $S^{n+q}$ и ее ориентированного локально плоского подмногообразия $k^{n}$, гомеоморфного сфере $S^{n}$. Два узла $K_{1}=\left(S^{n+q}, k_{1}^{n}\right)$ и $K_{2}=\left(S^{n+q}, k_{2}^{n}\right)$ наз. эквивалентными, если существует изоmопия сферы $S^{n+q}$, переводящая $k_{1}^{n}$ на $k_{2}^{n}$ с coxpaнением ориентации. В зависимости от того, в какой категории (Diff, PL или Top) понимаются термины «подмногообразие» и «изотопия» в предыдущих определениях, говорится о гладких, пусочно линейных или топологич. М. у. соответственно. В гладком случае подмногообразие $k^{n}$ может иметь и нестандартную дифференцируемую структуру. $n$-мерный узел коразмерности $q$, изотопный стандартному вложению, наз. т р и в иа льны м, или не з а узленны м, узлом.

Изучение М. у. коразмерности 1 связано с IÏ̈нфлиса әипотезой. Всякий топологич. узел коразмерности 1 тривиален. Это же верно и для кусочно линейных и гладких узлов, если $n \neq 3,4$.

Кусочно линейные и топологич. М. у. коразмерности $q \geqslant 3$ тривиальны. В гладком случае это не так. Множе-