последовательность $\left\{x_{k}\right\}$, построенная алгоритмом (5), сходится к точке минимума функции $x^{*}$ по закону геометрич. прогрессии (см. [3])

\[
\left\|x_{k}-x^{*}\right\| \leqslant C q^{k}
\]

где $C=\mathrm{const}$,

\[
q=\left[k\left(J^{\prime \prime}\left(x^{*}\right)\right)-1\right] /\left[k\left(J^{\prime \prime}\left(x^{*}\right)\right)+1\right]
\]

Так как для овражной функции $k\left(J^{\prime \prime}\left(x^{*}\right)\right) \gg 1$, то $q \simeq 1$ и сходимость практически отсутствует.

Аналогичная картина наблюдается и для простой градиентной схемы (см. [4])

\[
x_{k+1}=x_{k}-h J_{k}^{\prime \prime}, J_{k+1}=J\left(x_{k+1}\right), h=\text { const. }
\]

Ускорение ее сходимости основано на использовании результатов предыдущих итераций для уточнения дна оврага. Может быть использован (см. [4] [5]) градиентный метод (7) с вычислением на каждой итерации отношения $q=\left\|J_{k}^{\prime}\right\| /\left\|J_{k-1}^{\prime}\right\|$. Когда оно устанавливается около нек-рого постоянного значения $q=1$, делается большой ускоряющий шаг согласно выражению

\[
x_{k+1}=x_{k}-\frac{h}{1-q} J_{k}^{\prime} \text {. }
\]

Далее из точки $x_{k+1}$ продолжается спуск градиентным методом до следующего ускоряющего шага.

Различные версии метода параллельных касательных (см. [4] - [6]) основаны на выполнении ускоряющего шага вдоль направления $x_{k+2}-x_{k}$, задаваемого точками $x_{k}, x_{k+2}$ в градиентном методе. В методе «тяжелого шарика» (см. [4]) очередное приближение имеет вид

\[
x_{k+1}=x_{k}-\alpha J_{k}^{\prime}+\beta\left(x_{k}-x_{k-1}\right) .
\]

В м е т о д е о в р а г о в (см. [7]) предлагается провести локальные спуски градиентным методом (7) из двух случайно выбранных исходных точек, а затем выполнить ускоряющий шаг по направлению, задаваемому двумя полученными на дне оврага точками.

Все эти методы немногим сложнее градиентного метода (7) и построены на его основе. Ускорение сходимости получается для одномерных оврагов. В более общих случаях многомерных оврагов, где сходимость этих схем резко замедляется, приходится обращаться к более мощным методам квадратичной аппроксимации, в основе к-рых лежит метод Ньютона

\[
x_{k+1}=x_{k}-\left(J_{k}^{\prime \prime}\right)^{-1} J_{k}^{\prime}, J_{k}^{\prime \prime}=J^{\prime \prime}\left(x_{k}\right) \text {. }
\]

Точка минимума функции (6) удовлетворяет системе линейных уравнений

\[
D \boldsymbol{x}=\boldsymbol{b},
\]

и при условии абсолютной точности всех вычислений для квадратичной функции метод Ньютона независимо от степени овражности (2) и размерности оврагов приводит к минимуму за один шаг. На самом деле, при больших числах обусловленности $k(D)$ при ограниченной разрядности вычислений задача получения решения (9) может быть некорректной, и небольшие деформации элементов матрицы $D$ и вектора $b$ могут приводить к большим вариациям $x^{*}$.

При умеренных степенях овражности в выпуклой ситуации метод Ньютона часто оказывается более предпочтительным по скорости сходимости, чем другие, напр. градиентные, методы.

Большой класс квадратичных (квазиньютоновских) методов основан на использовании сопряженных направлений (см. [2], [3], [8]). Эти алгоритмы для случая минимизации выпуклой функции оказываются весьма эффективными, ибо, имея квадратичное окончание, они не требуют вычисления матрицы двух производных. Иногда (см. [8]) итерации строятся по схеме

\[
x_{k+1}=x_{k}-\left(\beta_{k} E+J_{k}^{\prime \prime}\right)^{-1} J_{k}^{\prime},
\]

где $E$ - единичная матрица. Скаляр $\beta_{k}$ подбирается так, чтобы матрица $J_{k}^{\prime \prime}+\beta_{k} E$ была положительно определенной и чтобы

\[
\left\|x_{k+1}-x_{k}\right\| \leqslant \varepsilon_{k} .
\]

Существует ряд аналогичных подходов (см. [8]), основанных на получении строго положительно определенных аппроксимаций матрицы Гессе. При минимизации овражных функций такие алгоритмы оказываются малоэффективными из-за трудностей в подборе параметров $\beta_{k}, \varepsilon_{k}$ и т. Д. Выбор әтих параметров основан на информации о величине наименьших по модулю собственных значений матрицы Гессе, а при реальных вычислениях и большой степени овражности эта информация сильно искажена.

Более целесообразно обобщение метода Ньютона на случай минимизации овражных функций проводится на базе непрерывного принципа оптимизации. Функции $\boldsymbol{J}(x)$ ставится в соответствие дифференциальная система (3), интегрируемая системным методом (см. Жесткая дифференциальная система). Алгоритм минимизации принимает вид

\[
\begin{gathered}
x_{k+1}=x_{k}-\Phi\left(2^{N} h_{k}^{0}\right) J_{k}^{\prime} \\
\Phi\left(2^{N} h_{k}^{0}\right)=\int_{0}^{2 N h_{k}^{0}} \exp \left(-J_{k}^{\prime \prime} \tau\right) d \tau \\
J_{k+1}=\min _{N} J\left(x_{k}-\Phi\left(2^{N} h_{k}^{0}\right) J_{k}^{\prime}\right) \\
\left.\Phi\left(h_{k}^{0}\right)=h_{k}^{0}\left[E-\frac{h_{k}^{0}}{2} J_{k}^{\prime \prime}+\frac{\left(h_{k}^{0}\right)^{2}}{6}\left(J_{k}^{\prime \prime}\right)^{2}-\ldots\right], J_{k}^{\prime \prime}\left(2^{s} h_{k}^{0}\right)\right], \\
\Phi\left(2^{s+1} h_{k}^{0}\right)=\Phi\left(2^{s} h_{k}^{0}\right)\left[2 E-J_{k}^{\prime \prime}\right) \\
s=0,1, \ldots, N-1
\end{gathered}
\]

Предложен [1] алгоритм минимизации овражной функции, основанный на использовании свойств жестких систем. Пусть функция $J(x)$ в окрестности $x_{0}$ аппроксимируется квадратичной функцией (6). Матрица $D$ и вектор $\boldsymbol{b}$ вычисляются, напр., с помощью конечноразностной аппроксимации. Из представления әлементов матрицы

\[
d_{i j}=\sum_{s=1}^{m} u_{i s} u_{j s} \lambda_{s}
\]

где $u_{s}=\left(u_{1 s}, \ldots, u_{m s}\right)^{T}, s=1, \ldots, m$,-ортонормированный базис собственных векторов $D$, следует, что неточное измерение этих әлементов искажает информацию о малых собственных значениях плохо обусловленной матрицы, а следовательно, приводит к некорректности задачи минимизации функции (6).

Вместе с тем система дифференциальных уравнений спуска для овражной функции (6)

\[
\frac{d x}{d t}=-D x+b, x(0)=x_{0}
\]

имеет решение, в к-ром в силу условия (1) слагаемые с сомножителями $\exp \left(-\lambda_{1} t\right)$ оказывают влияние лишь на малом начальном отрезке длиной $\tau_{п с}=O(\lambda-1)$.

Другими словами, компоненты вектора $x(t)$ удовлетворяют равенству

\[
x^{T}(t) u_{1}-\lambda_{1}^{-1} b^{T} u_{1}=\left(x_{0}^{T} u_{1}-\lambda_{1}^{-1} b^{T} u_{1}\right) \exp \left(-\lambda_{1} t\right),
\]

быстро переходящему в стационарную связь

\[
\sum_{i=1}^{m} u_{i 1} \bar{x}_{i}-\lambda_{1}^{-1} \sum_{i=1}^{m} b_{i} u_{i 1}=0
\]