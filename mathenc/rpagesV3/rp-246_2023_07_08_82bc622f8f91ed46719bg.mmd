Если имеется несколько членов ряда, по модулю равных $\mu(r)$, то за центральный индекс принимается наибольший из индексов этих членов. Функция

\[
y=\ln \mu\left(e^{x}\right), \quad-\infty<x<\infty,
\]

- неубывающая и выпуклая; функция $v(r)$ - ступенчатая, возрастает в точках разрыва на натуральное число и всюду непрерывна справа.

Лuт.: [1] В а л и р о н Ж., Аналитические функции, пер. с франц., М., 1957; [2] В и т т и х Г. В., Новейшие исследования по однозначным аналитическим функциям, пер. с нем., М.,

МАКСИМИЗАЦИЯ И МИНИИЗАЦИЯ ФУНКЦИЙ кон е ч о г о ч и с л а пе р е ме ны ы х-задача поиска экстремума функции $f(x), x=\left(x^{1}, \ldots, x^{n}\right) \in X \sqsubseteq$ $\subseteq \mathbb{R} n$; под этой задачей понимается:

1) нахождение $\bar{f}=\sup _{x \in X} f(x)$ или $f=\inf _{x \in X} f(x)$;

2) отыскание точек максимума или минимума, если $\bar{f}$ или $f$ достигаются на допустимом множестве (см. Максил̈ум и минимум функции);

3) построение м а к с и м и зи и у щ е ї п о л л ед о в а т е ль но с т и $\left\{x_{i}\right\}$ или м и н и м ї з и р у юще й пос ле довате льности $\left\{x_{i}\right\}$ таких, что

\[
\lim _{i \rightarrow \infty} f\left(x_{i}\right)=\bar{f}, \quad \lim _{j \rightarrow \infty} f\left(x_{j}\right)=\underline{f},
\]

если $\bar{f}$ или $\underline{f}$ недостижимы на $X$.

Исследованием әкстремумов функций дискретных аргументов занимается дискретное програмжирование и челочисленное программирование. Ниже освещены только методы М. и м. Ф. непрерывных аргументов.

Классические (непрямые) методы М. и м. ф. применимы только для гладких функций. Они используют необходимое условие әкстремума для поиска стационарных точек. Нули производных $\frac{\partial f}{\partial x^{\alpha}}, \alpha=1, \ldots, n$, вычисляются на практике чаще всего одним из многочисленных методов последовательных приближений (cм. [3]). С другой стороны, каждую задачу решения конечных функциональных уравнений вида

\[
\varphi_{m}\left(x^{1}, \ldots, x^{n}\right)=0, \quad m \leqslant n,
\]

можно интерпретировать как задачу М. и м. Ф., напр. функции

\[
f(x)=\varphi_{1}^{2}(x)+\ldots+\varphi_{m}^{2}(x) \longrightarrow \min ,
\]

и шрименить для решенія последней один из специфич. методов М. и м. ф.

Прямые методы М. и м. ф. основываются на непосредственном сравнении значений $f(x)$ в двух или нескольких точках.

Для практич. отыскания әкстремумов применяются итеративные алгоритмы вида:

\[
x_{i+1}=\hat{X}\left(i, x_{i}, x_{i-1}, \ldots, x_{i-j}\right),
\]

где $i$ - номер итерации, а $\widehat{X}(\cdot)$ - нек-рый оператор. При әтом обычно предполагается:

1) сходимость алгоритма в том или ином смысле, чаще всего в смысле

\[
x_{\infty}=\bar{x}\left(x_{\infty}=\underline{x}\right) \text { или } f\left(x_{\infty}\right)=\bar{f}\left(f\left(x_{\infty}\right)=\underline{f}\right) ;
\]

2) локальность итерационной процедуры, т. е. $j \ll i(j=o(i)$ при $i \rightarrow \infty)$; алгоритм «помнит» значения $x$ только для итераций в нек-рой окрестности текущего положения $x_{i}$. При $j \equiv 0$ получается простой марковский вычислительный процесс без памяти.

Оператор $\hat{X}(\cdot)$ может быть детерминированным в детерминированных методах или содержать стохастич. параметры. В вычислительной практике часто сочетают стохастич. методы с детерминированными, напр. в покоординатного спуска методе направление спуска может определяться случайным образом. Вероятностные характеристики стохастических параметров, в свою очередь, могут меняться от итерации к итерации (поиск с адаптацией и «самообучением», случайный поиск).

Широко применяют и комбинирование различных детерминированных методов, к к-рому относится последовательное и параллельное вычисление экстремума несколькими методами, композиции алгоритмов вида $\widehat{X}=\hat{X}_{2}\left(\hat{X}_{1}(\cdot)\right)$ и т. п. Напр., м е т о д Л е в е н б е рr a - M a p к в a p д т a

\[
x_{i+1}=x_{i}-\left(\alpha_{i} \nabla \nabla f\left(x_{j}\right)+\beta_{i} I\right)^{-1} \nabla f\left(x_{i}\right) \text {, }
\]

к-рый при $\alpha_{i}=0$ совпадает с градиентным методом, а при $\beta_{i}=0$ с методом Ньютона.

Од номе рная о п тими зация, то есть М. и м. Ф. $f(x), x \in \mathbb{R} \mathbf{1}$, помимо самостоятельного интереса, является необходимым этапом большинства применяемых методов. $\mathbf{K}$ специфически одномерным относятся, напр., Фибоначчи метод, половинного деления метод (дихотомии метод), парабол метод. Методами М. и м. Ф. многих переменных являются градиентный метод, наискорейшего спуска метод, покоординатного спуска метод, симплексный поиск, сканирования метод, сопряженных градиентов метод, тяжелого шарика метод, установления метод и др.

Алгоритмы большинства из перечисленных методов укладываются в схему ме т о д а с п у с к а (по д ъe $\mathbf{M} \mathbf{a})$ :

\[
x_{i+1}=x_{i} \mp x_{i} y_{i},
\]

причем $f\left(x_{i+1}\right) \leqslant f\left(x_{i}\right)$ или $f\left(x_{i+1}\right) \geqslant f\left(x_{i}\right)$ для всех $i$ (условие релаксации). Они различаются между собой либо выбором вектора направления $y_{i}$ спуска, либо выбором способов движения вдоль вектора спуска, определяемым ші а г о в ы м м о жи т е ле м æ.

Овражные методы разработаны для функций, рельеф к-рых имеет вид «оврагов с крутыми склонами» (см. Овражных бункчий методы минимизации). Ординарные (не овражные) методы, будучи примененными здесь, дают извилистый р е ла к с а ц и о н ны й П у т ь, требующий чрезмерно больших затрат машинного времени для вычисления экстремума.

Сравнительная эффективность методов оценивается по многим и противоречивым критериям. Сюда входят: точность решения, скорость решения, надежность метода, время подготовки задачи к счету, сходимость алгоритма и др. Область применения каждого из апробированных методов весьма ограничена.

Для испытания методов разработаны наборы стандартных т е с т - ф у н ц и й, характерных для различных функциональных классов (см. [1]). Усіленно исследуется сходимость методов М. и м. ф. (см. [6], [8]). Однако сходимость - это качество, к-рое не является ни необходимым, ни достаточным для әффективного окончания вычислений.

Bсе перечисленные выше методы приводят к одному из локальных экстремумов, если начальное приближение принадлежит области притяжения точки әтого әкстремума. Нахождение глобального экстремума гарантируется лишь для выпуклых і родственных им унимодальных функций. Теория отыскания глобального экстремума находится (1982) в начальной стадиі развития (см. Многоэкстремальная задача). Другім развивающимся направлением М. и м. ф. является оптимизация негладких функций (см. [4], [13], [16]). В частности, к негладкой функции, как правило, приводит задача минимизации функции максимума (см. Максимин; численные методы). По-видимому, все из общеупотребительных методов оптимизации имеют содержательный физический, экономический или биологический